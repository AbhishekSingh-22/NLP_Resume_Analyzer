{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c132fee",
   "metadata": {},
   "source": [
    "### Adding path of the project folder in system variable to find modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fa69f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\\\python311.zip', 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\\\DLLs', 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\\\Lib', 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0', 'd:\\\\Work\\\\Capstone_Project\\\\resume-nlp\\\\venv', '', 'd:\\\\Work\\\\Capstone_Project\\\\resume-nlp\\\\venv\\\\Lib\\\\site-packages', 'd:\\\\Work\\\\Capstone_Project\\\\resume-nlp']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# get project root path (parent of 'notebooks' directory)\n",
    "root_path = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "if root_path not in sys.path:\n",
    "    sys.path.append(root_path)\n",
    "\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943c2e20",
   "metadata": {},
   "source": [
    "### Step 1: Extract text from pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f049d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abhishek Singh\n",
      "8010852459 | abhisheksingh.vizag@gmail.com | LinkedIn | GitHub | LeetCode\n",
      "Education\n",
      "•\n",
      "VIT Bhopal University | CGPA 9.01\n",
      "Oct 2022 – Present\n",
      "Bachelor of Technology in Computer Science and Engineering\n",
      "Bhopal, Madhya Pradesh\n",
      "•\n",
      "Higher Secondary Education | Grade: 95.4%\n",
      "July 2021\n",
      "Navy Children School, Goa\n",
      "Vasco Da Gama, Goa\n",
      "Technical Skills\n",
      "• Languages/Databases: C++, Python, SQL\n",
      "• Framew\n",
      "email found: True\n",
      "phone found: True\n",
      "education header: True\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from src.ingest import extract_text_pymupdf\n",
    "\n",
    "\n",
    "text = extract_text_pymupdf('../data/Abhishek_Singh_Resume.pdf')\n",
    "print(text[:400])\n",
    "print('email found:', bool(re.search(r\"[\\w\\.-]+@[\\w\\.-]+\", text)))\n",
    "print('phone found:', bool(re.search(r\"\\+?\\d[\\d\\s\\-()]{6,}\\d\", text)))\n",
    "print('education header:', 'Education' in text or 'EDUCATION' in text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84be2729",
   "metadata": {},
   "source": [
    "- setting up some configuration for the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e04e074e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG\n",
    "from pathlib import Path\n",
    "DATA_ROOT = Path('../data/resumes_raw_pdf')\n",
    "PDF_DIR = DATA_ROOT / 'pdfs'\n",
    "TXT_DIR = DATA_ROOT / 'txt'\n",
    "FAIL_DIR = DATA_ROOT / 'failures'\n",
    "REPORT_JSON = DATA_ROOT / 'extraction_report.json'\n",
    "REPORT_CSV = DATA_ROOT / 'extraction_report.csv'\n",
    "for d in [PDF_DIR, TXT_DIR, FAIL_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# How many resumes to download for testing (start small: 50-100)\n",
    "N_SAMPLES = 100\n",
    "\n",
    "\n",
    "# Toggle OCR fallback (requires system Tesseract and pytesseract)\n",
    "ENABLE_OCR = False\n",
    "OCR_LANGUAGE = 'eng'\n",
    "\n",
    "\n",
    "# thresholds for checks\n",
    "MIN_TEXT_CHARS = 200\n",
    "MIN_ALPHA_RATIO = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854c9dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Work\\Capstone_Project\\resume-nlp\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authenticated as: AbhishekProgrammer22\n"
     ]
    }
   ],
   "source": [
    "# Option 1: set token for this notebook session and re-run the download cell\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # take environment variables from .env file\n",
    "\n",
    "token = os.getenv(\"HUGGINGFACE_HUB_TOKEN\")   # <-- paste your token here\n",
    "os.environ[\"HUGGINGFACE_HUB_TOKEN\"] = token\n",
    "\n",
    "# optional check who you are\n",
    "from huggingface_hub import whoami\n",
    "print(\"Authenticated as:\", whoami(token=token).get(\"name\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060ed5af",
   "metadata": {},
   "source": [
    "### Step 2: Selecting a dataset and proceeding with NER Topic modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d94f7769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1940 pdf files in the repo. Will copy/process first 100.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "files:  29%|██▉       | 29/100 [00:27<01:48,  1.53s/it]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "files:  31%|███       | 31/100 [00:32<02:08,  1.87s/it]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "files:  32%|███▏      | 32/100 [00:35<02:26,  2.15s/it]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "files:  36%|███▌      | 36/100 [00:41<01:42,  1.60s/it]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "files:  40%|████      | 40/100 [00:51<02:17,  2.29s/it]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "files:  43%|████▎     | 43/100 [01:00<02:20,  2.46s/it]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "files:  64%|██████▍   | 64/100 [01:52<01:36,  2.68s/it]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "files:  67%|██████▋   | 67/100 [02:01<01:23,  2.53s/it]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "files:  68%|██████▊   | 68/100 [02:05<01:38,  3.07s/it]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "files:  73%|███████▎  | 73/100 [02:17<01:05,  2.43s/it]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "files:  82%|████████▏ | 82/100 [02:31<00:30,  1.67s/it]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "files:  89%|████████▉ | 89/100 [02:44<00:19,  1.80s/it]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "files:  91%|█████████ | 91/100 [02:50<00:21,  2.34s/it]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "files:  97%|█████████▋| 97/100 [03:06<00:08,  2.91s/it]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "files: 100%|██████████| 100/100 [03:16<00:00,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n=== SUMMARY ===\n",
      "Total files processed: 100\n",
      "Success (no detected failure reasons): 1 (1.0%)\n",
      "Files with failures: 99\n",
      "Failure reasons counts: {'missing_contact': 99, 'no_key_section': 67, 'short_text': 24, 'low_alpha_ratio': 24}\n",
      "Report saved -> ..\\data\\resumes_raw_pdf_direct\\extraction_report.json\n",
      "PDFs saved  -> ..\\data\\resumes_raw_pdf_direct\\pdfs\n",
      "Text saved  -> ..\\data\\resumes_raw_pdf_direct\\txt\n",
      "Failures    -> ..\\data\\resumes_raw_pdf_direct\\failures\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Use cached HF snapshot to copy PDFs -> run extraction and produce report\n",
    "from huggingface_hub import list_repo_files, hf_hub_download\n",
    "from pathlib import Path\n",
    "import os, shutil, json, csv, traceback\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# your extractors\n",
    "from src.ingest import extract_text_pymupdf, extract_text_pdfplumber\n",
    "\n",
    "REPO_ID = \"d4rk3r/resumes-raw-pdf\"\n",
    "OUT_ROOT = Path(\"../data/resumes_raw_pdf_direct\")\n",
    "PDF_DIR = OUT_ROOT / \"pdfs\"\n",
    "TXT_DIR = OUT_ROOT / \"txt\"\n",
    "FAIL_DIR = OUT_ROOT / \"failures\"\n",
    "REPORT_JSON = OUT_ROOT / \"extraction_report.json\"\n",
    "REPORT_CSV  = OUT_ROOT / \"extraction_report.csv\"\n",
    "\n",
    "for d in (OUT_ROOT, PDF_DIR, TXT_DIR, FAIL_DIR):\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "N_SAMPLES = 100          # change if you want fewer\n",
    "MIN_TEXT_CHARS = 200\n",
    "MIN_ALPHA_RATIO = 0.2\n",
    "\n",
    "token = os.getenv(\"HUGGINGFACE_HUB_TOKEN\")\n",
    "if not token:\n",
    "    raise RuntimeError(\"HUGGINGFACE_HUB_TOKEN missing - set it before running.\")\n",
    "\n",
    "# list files (we already saw this works)\n",
    "all_files = list_repo_files(REPO_ID, repo_type=\"dataset\", token=token)\n",
    "pdf_files = [f for f in all_files if f.lower().endswith(\".pdf\")]\n",
    "print(f\"Found {len(pdf_files)} pdf files in the repo. Will copy/process first {min(N_SAMPLES, len(pdf_files))}.\")\n",
    "\n",
    "# helper: hf_hub_download returns a local cached path when available\n",
    "def get_cached_path(fname):\n",
    "    try:\n",
    "        local = hf_hub_download(repo_id=REPO_ID, filename=fname, repo_type=\"dataset\", token=token)\n",
    "        return Path(local)\n",
    "    except Exception as e:\n",
    "        print(\"hf_hub_download failed for\", fname, \"->\", type(e).__name__, str(e)[:200])\n",
    "        return None\n",
    "\n",
    "report = []\n",
    "to_process = pdf_files[:min(N_SAMPLES, len(pdf_files))]\n",
    "\n",
    "for idx, fname in enumerate(tqdm(to_process, desc=\"files\")):\n",
    "    rec = {\"filename\": Path(fname).name, \"repo_path\": fname, \"downloaded\": False,\n",
    "           \"extraction_method\": None, \"ocr_used\": False, \"error\": None, \"checks\": None,\n",
    "           \"failure_reasons\": [], \"failure_file\": None}\n",
    "    try:\n",
    "        cached = get_cached_path(fname)\n",
    "        if cached is None:\n",
    "            rec[\"error\"] = \"cache_lookup_failed\"\n",
    "            report.append(rec)\n",
    "            continue\n",
    "\n",
    "        # copy cached file to our PDF_DIR with normalized name\n",
    "        tgt = PDF_DIR / f\"resume_{idx:05d}.pdf\"\n",
    "        shutil.copyfile(cached, tgt)\n",
    "        rec[\"downloaded\"] = True\n",
    "\n",
    "        # extract text: pymupdf primary, pdfplumber fallback\n",
    "        txt = \"\"\n",
    "        try:\n",
    "            txt = extract_text_pymupdf(str(tgt))\n",
    "            rec[\"extraction_method\"] = \"pymupdf\"\n",
    "            if len(txt) < MIN_TEXT_CHARS // 2:\n",
    "                txt2 = extract_text_pdfplumber(str(tgt))\n",
    "                if len(txt2) > len(txt):\n",
    "                    txt = txt2\n",
    "                    rec[\"extraction_method\"] += \"+pdfplumber\"\n",
    "        except Exception as e1:\n",
    "            try:\n",
    "                txt = extract_text_pdfplumber(str(tgt))\n",
    "                rec[\"extraction_method\"] = \"pdfplumber\"\n",
    "            except Exception as e2:\n",
    "                rec[\"error\"] = f\"both_extractors_failed: {e1} | {e2}\"\n",
    "                badf = FAIL_DIR / f\"downloaded_but_extractfail_{idx:05d}.txt\"\n",
    "                with open(badf, \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(f\"Cached path: {cached}\\\\nErrors:\\\\n{e1}\\\\n{e2}\")\n",
    "                rec[\"failure_file\"] = str(badf)\n",
    "                report.append(rec)\n",
    "                continue\n",
    "\n",
    "        txt = (txt or \"\").replace(\"\\r\", \"\\n\").strip()\n",
    "        # save text\n",
    "        with open(TXT_DIR / (tgt.stem + \".txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(txt)\n",
    "\n",
    "        # checks\n",
    "        import re\n",
    "        EMAIL_RE = re.compile(r\"[\\w\\.-]+@[\\w\\.-]+\\.\\w+\")\n",
    "        PHONE_RE = re.compile(r\"\\+?\\d[\\d\\s\\-\\(\\)]{6,}\\d\")\n",
    "        SECTION_KEYWORDS = ['education','experience','skills','projects','certifications','publications','summary','objective']\n",
    "\n",
    "        email = bool(EMAIL_RE.search(txt))\n",
    "        phone = bool(PHONE_RE.search(txt))\n",
    "        txt_low = txt.lower()\n",
    "        sections = {kw: (kw in txt_low) for kw in SECTION_KEYWORDS}\n",
    "        any_section = any(sections.values())\n",
    "        letters = sum(c.isalpha() for c in txt)\n",
    "        alpha_ratio = letters / max(1, len(txt))\n",
    "\n",
    "        checks = {'email': email, 'phone': phone, 'sections': sections, 'any_section': any_section,\n",
    "                  'len_chars': len(txt), 'alpha_ratio': alpha_ratio, 'preview': txt[:800].replace(\"\\n\",\"\\\\n\")}\n",
    "        rec[\"checks\"] = checks\n",
    "\n",
    "        failures = []\n",
    "        if rec[\"error\"]:\n",
    "            failures.append(\"extractor_error\")\n",
    "        if checks['len_chars'] < MIN_TEXT_CHARS:\n",
    "            failures.append(\"short_text\")\n",
    "        if checks['alpha_ratio'] < MIN_ALPHA_RATIO:\n",
    "            failures.append(\"low_alpha_ratio\")\n",
    "        if not checks['any_section']:\n",
    "            failures.append(\"no_key_section\")\n",
    "        if not (checks['email'] and checks['phone']):\n",
    "            failures.append(\"missing_contact\")\n",
    "\n",
    "        rec[\"failure_reasons\"] = failures\n",
    "        if failures:\n",
    "            fname_fail = FAIL_DIR / (tgt.stem + \"_failure.txt\")\n",
    "            with open(fname_fail, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(f\"FILENAME: {tgt.name}\\\\nREPO_PATH: {fname}\\\\nEXTRACTION_METHOD: {rec['extraction_method']}\\\\nFAILURE_REASONS: {failures}\\\\n\\\\n---PREVIEW---\\\\n\\\\n\")\n",
    "                f.write(txt)\n",
    "            rec[\"failure_file\"] = str(fname_fail)\n",
    "\n",
    "    except Exception as e:\n",
    "        rec[\"error\"] = f\"fatal:{type(e).__name__}:{e}\"\n",
    "        rec[\"failure_file\"] = None\n",
    "    report.append(rec)\n",
    "\n",
    "# save reports\n",
    "with open(REPORT_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "with open(REPORT_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as csvf:\n",
    "    writer = csv.writer(csvf)\n",
    "    writer.writerow(['filename','repo_path','downloaded','extraction_method','len_chars','alpha_ratio','email','phone','any_key_section','failure_reasons','failure_file','error'])\n",
    "    for r in report:\n",
    "        ch = r.get('checks') or {}\n",
    "        writer.writerow([r.get('filename'), r.get('repo_path'), r.get('downloaded'), r.get('extraction_method'),\n",
    "                         ch.get('len_chars'), round(ch.get('alpha_ratio',0),3), ch.get('email'), ch.get('phone'), ch.get('any_section'),\n",
    "                         ';'.join(r.get('failure_reasons',[])), r.get('failure_file',''), r.get('error','')])\n",
    "\n",
    "# pretty summary\n",
    "from collections import Counter\n",
    "total = len(report)\n",
    "failures = [r for r in report if r.get('failure_reasons')]\n",
    "success = total - len(failures)\n",
    "fail_reasons = Counter()\n",
    "for r in failures:\n",
    "    fail_reasons.update(r.get('failure_reasons',[]))\n",
    "\n",
    "print(\"\\\\n=== SUMMARY ===\")\n",
    "print(\"Total files processed:\", total)\n",
    "print(\"Success (no detected failure reasons):\", success, f\"({round(100*success/total if total else 0,2)}%)\")\n",
    "print(\"Files with failures:\", len(failures))\n",
    "print(\"Failure reasons counts:\", dict(fail_reasons))\n",
    "print(\"Report saved ->\", REPORT_JSON)\n",
    "print(\"PDFs saved  ->\", PDF_DIR)\n",
    "print(\"Text saved  ->\", TXT_DIR)\n",
    "print(\"Failures    ->\", FAIL_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9042f05a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset lhoestq/resumes-raw-pdf-for-ocr split=train (this may take a minute)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Work\\Capstone_Project\\resume-nlp\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\abhis\\.cache\\huggingface\\hub\\datasets--lhoestq--resumes-raw-pdf-for-ocr. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Generating train split: 100%|██████████| 1585/1585 [00:00<00:00, 3149.88 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded. Columns: ['label', 'images', 'text'] Num examples: 1585\n",
      "Using text field: text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "writing text:  32%|███▏      | 500/1585 [00:03<00:07, 139.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 500 text samples to ..\\data\\resumes_preocr\\txt\n",
      "Report: ..\\data\\resumes_preocr\\report_preocr.json\n",
      "Summary CSV: ..\\data\\resumes_preocr\\summary_preocr.csv\n"
     ]
    }
   ],
   "source": [
    "# Load HF dataset properly and write out text files per example\n",
    "# Paste & run in the same notebook environment\n",
    "\n",
    "from datasets import load_dataset\n",
    "from pathlib import Path\n",
    "import json, csv, shutil\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "\n",
    "OUT_ROOT = Path(\"../data/resumes_preocr\")\n",
    "TXT_DIR = OUT_ROOT / \"txt\"\n",
    "PDF_DIR = OUT_ROOT / \"pdfs\"   # may remain empty for this dataset\n",
    "REPORT = OUT_ROOT / \"report_preocr.json\"\n",
    "SUMMARY_CSV = OUT_ROOT / \"summary_preocr.csv\"\n",
    "\n",
    "OUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "TXT_DIR.mkdir(exist_ok=True)\n",
    "PDF_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "REPO_ID = \"lhoestq/resumes-raw-pdf-for-ocr\"\n",
    "SPLIT = \"train\"   # dataset split\n",
    "N_SAMPLES = 500   # adjust to how many you want to extract (max ~ full dataset size ~ 1585)\n",
    "\n",
    "print(f\"Loading dataset {REPO_ID} split={SPLIT} (this may take a minute)...\")\n",
    "ds = load_dataset(REPO_ID, split=SPLIT)\n",
    "\n",
    "print(\"Dataset loaded. Columns:\", ds.column_names, \"Num examples:\", len(ds))\n",
    "\n",
    "# Which field contains text? Common names: 'text'\n",
    "text_field = None\n",
    "for candidate in (\"text\", \"ocr\", \"page_text\", \"raw_text\"):\n",
    "    if candidate in ds.column_names:\n",
    "        text_field = candidate\n",
    "        break\n",
    "# fallback: look for any string column\n",
    "if text_field is None:\n",
    "    for c in ds.column_names:\n",
    "        # sample few rows to check if column is string-like and non-empty\n",
    "        try:\n",
    "            sample = ds[0].get(c)\n",
    "            if isinstance(sample, str):\n",
    "                text_field = c\n",
    "                break\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "if text_field is None:\n",
    "    raise RuntimeError(f\"Couldn't find a text column in dataset. Columns: {ds.column_names}\")\n",
    "\n",
    "print(\"Using text field:\", text_field)\n",
    "\n",
    "report = []\n",
    "count = 0\n",
    "for i, ex in enumerate(tqdm(ds, desc=\"writing text\")):\n",
    "    if count >= N_SAMPLES:\n",
    "        break\n",
    "    txt = ex.get(text_field) or \"\"\n",
    "    # sometimes the text may be empty (filter as desired)\n",
    "    if not txt or not txt.strip():\n",
    "        # skip empty text entries (optionally you can save them)\n",
    "        continue\n",
    "    fname = f\"sample_{count:05d}.txt\"\n",
    "    tgt = TXT_DIR / fname\n",
    "    with open(tgt, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(txt)\n",
    "    report.append({\"index\": i, \"filename\": fname, \"text_len\": len(txt), \"preview\": txt[:500].replace(\"\\n\",\"\\\\n\")})\n",
    "    count += 1\n",
    "\n",
    "# save the report json and a CSV summary with basic checks\n",
    "with open(REPORT, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "\n",
    "import re\n",
    "EMAIL_RE = re.compile(r\"[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+\")\n",
    "PHONE_RE = re.compile(r\"(?:\\+?\\d{1,3}[-.\\s]?)?(?:\\(?\\d{2,4}\\)?[-.\\s]?)?\\d{6,12}\")\n",
    "SECTIONS = ['education','experience','skills','projects','certifications','publications','summary','objective','work experience']\n",
    "\n",
    "rows = []\n",
    "for r in report:\n",
    "    txt = (TXT_DIR / r['filename']).read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    email = bool(EMAIL_RE.search(txt))\n",
    "    phone = bool(PHONE_RE.search(txt))\n",
    "    any_section = any(k in txt.lower() for k in SECTIONS)\n",
    "    rows.append({\n",
    "        \"file\": r['filename'],\n",
    "        \"chars\": len(txt),\n",
    "        \"email\": email,\n",
    "        \"phone\": phone,\n",
    "        \"any_section\": any_section,\n",
    "        \"preview\": txt[:400].replace(\"\\n\",\"\\\\n\")\n",
    "    })\n",
    "\n",
    "# write CSV\n",
    "import csv\n",
    "with open(SUMMARY_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=list(rows[0].keys()) if rows else [\"file\"])\n",
    "    writer.writeheader()\n",
    "    for row in rows:\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(f\"Saved {len(report)} text samples to {TXT_DIR}\")\n",
    "print(\"Report:\", REPORT)\n",
    "print(\"Summary CSV:\", SUMMARY_CSV)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23644b73",
   "metadata": {},
   "source": [
    "##### Multilingual NER using a different model than SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f3e5e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Work\\Capstone_Project\\resume-nlp\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "d:\\Work\\Capstone_Project\\resume-nlp\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\abhis\\.cache\\huggingface\\hub\\models--Davlan--xlm-roberta-base-ner-hrl. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Device set to use cpu\n",
      "100%|██████████| 500/500 [02:33<00:00,  3.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved fixed names to: ..\\data\\resumes_preocr\\ner_fixed_names.json\n",
      "Sample: Nguyen Dang Binh -> Nguyen Dang Binh\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# === Multilingual NER for PERSON names (fixes name extraction) ===\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "ROOT = Path(\"../data/resumes_preocr\")\n",
    "TXT_DIR = ROOT / \"txt\"\n",
    "NER_ENH = ROOT / \"ner_enhanced.json\"\n",
    "OUT = ROOT / \"ner_fixed_names.json\"\n",
    "\n",
    "# load previous enhanced NER\n",
    "with open(NER_ENH, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "MODEL = \"Davlan/xlm-roberta-base-ner-hrl\"   # multilingual NER (very good on PERSON)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "model = AutoModelForTokenClassification.from_pretrained(MODEL)\n",
    "ner_pipe = pipeline(\"token-classification\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
    "\n",
    "def load_text(fname):\n",
    "    p = TXT_DIR / fname\n",
    "    return p.read_text(encoding=\"utf-8\", errors=\"ignore\") if p.exists() else \"\"\n",
    "\n",
    "def extract_person_name(text):\n",
    "    \"\"\"Return first PERSON detected by multilingual NER.\"\"\"\n",
    "    try:\n",
    "        ents = ner_pipe(text[:800])  # only beginning of CV for speed\n",
    "    except:\n",
    "        return None\n",
    "    persons = [e[\"word\"] for e in ents if e[\"entity_group\"] == \"PER\"]\n",
    "    return persons[0] if persons else None\n",
    "\n",
    "fixed = []\n",
    "for entry in tqdm(data):\n",
    "    txt = load_text(entry[\"file\"])\n",
    "    person = extract_person_name(txt)\n",
    "    entry[\"primary_name_fixed\"] = person if person else entry[\"primary_name\"]\n",
    "    fixed.append(entry)\n",
    "\n",
    "with open(OUT, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(fixed, f, indent=2)\n",
    "\n",
    "print(\"Saved fixed names to:\", OUT)\n",
    "print(\"Sample:\", fixed[0][\"primary_name\"], \"->\", fixed[0][\"primary_name_fixed\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6e21444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cleaned NER with 'primary_name_clean' -> ..\\data\\resumes_preocr\\ner_fixed_names_clean.json  (fixed 83 names)\n",
      "sample_00000.txt -> Nguyen Dang Binh -> Nguyen Dang Binh -> Nguyen Dang Binh\n",
      "sample_00001.txt -> Phường Lái Thiêu -> Phường Lái Thiêu -> Phường Lái Thiêu\n",
      "sample_00002.txt -> Kumar Tiwari -> Akhilesh Kumar Tiwari -> Akhilesh Kumar Tiwari\n",
      "sample_00003.txt -> Thiện Chí Trần\n",
      "IOS -> Thiện Chí Trần -> Thiện Chí Trần\n",
      "sample_00004.txt -> Địa -> ĐINH PHƯƠNG HUYỀN -> ĐINH PHƯƠNG HUYỀN\n",
      "sample_00005.txt -> nghiệp chuyên -> Lê Trung Giang -> Lê Trung Giang\n"
     ]
    }
   ],
   "source": [
    "# 1) Name-cleaning heuristics\n",
    "import json, re\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path(\"../data/resumes_preocr\")\n",
    "TXT_DIR = ROOT / \"txt\"\n",
    "NER_FIXED = ROOT / \"ner_fixed_names.json\"   # or ner_fixed_names_spacy.json if you used spaCy fallback\n",
    "OUT_CLEAN = ROOT / \"ner_fixed_names_clean.json\"\n",
    "\n",
    "BAD_NAME_PATTERNS = [\n",
    "    re.compile(r\"^\\s*$\"),\n",
    "    re.compile(r\"^(?:ph|pv|hr|cv)$\", re.I),\n",
    "    re.compile(r\"^\\d{4}[\\s\\-:]\\d{4}$\"),    # year ranges\n",
    "    re.compile(r\"^(?:jun|jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec)\\b\", re.I),\n",
    "    re.compile(r\"^(?:\\d{1,2}[:/.-])\"),     # starts with numbers like dates\n",
    "    re.compile(r\"^[\\W_]+$\")                # punctuation-only\n",
    "]\n",
    "\n",
    "def looks_bad_name(s):\n",
    "    if s is None: return True\n",
    "    s = s.strip()\n",
    "    if len(s) < 2 or len(s.split()) > 6 and len(s) > 80:\n",
    "        # too short or suspiciously long\n",
    "        return True if len(s) < 2 or len(s) > 80 else False\n",
    "    low = s.lower()\n",
    "    if any(p.search(s) for p in BAD_NAME_PATTERNS):\n",
    "        return True\n",
    "    # contains words that are headings\n",
    "    if any(k in low for k in (\"experience\",\"objective\",\"summary\",\"profile\",\"address\",\"phone\",\"email\",\"marital\",\"date\",\"present\",\"current\",\"managed\")):\n",
    "        return True\n",
    "    # contains many digits -> bad\n",
    "    if sum(c.isdigit() for c in s) / max(1,len(s)) > 0.15:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def fallback_name_from_text(text):\n",
    "    # return first plausible short line near top that isn't a heading\n",
    "    for ln in text.splitlines()[:12]:\n",
    "        ln = ln.strip()\n",
    "        if not ln: continue\n",
    "        if len(ln) > 100: continue\n",
    "        low = ln.lower()\n",
    "        if any(k in low for k in (\"objective\",\"cv\",\"curriculum\",\"resume\",\"skills\",\"experience\",\"education\",\"address\",\"phone\",\"email\",\"profile\",\"summary\",\"contact\")):\n",
    "            continue\n",
    "        # filter lines that are mostly dates/locations or bullets\n",
    "        if re.match(r\"^[\\-\\u2022\\•\\*]\\s*\", ln): \n",
    "            # remove bullet char and keep going\n",
    "            ln = re.sub(r\"^[\\-\\u2022\\•\\*]\\s*\", \"\", ln).strip()\n",
    "        if len(ln) < 3 or len(ln) > 80:\n",
    "            continue\n",
    "        # good candidate\n",
    "        return ln\n",
    "    return None\n",
    "\n",
    "# load\n",
    "with open(NER_FIXED, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "cleaned = []\n",
    "fix_count = 0\n",
    "for entry in data:\n",
    "    orig = entry.get(\"primary_name_fixed\") or entry.get(\"primary_name\") or \"\"\n",
    "    if looks_bad_name(orig):\n",
    "        txt_path = TXT_DIR / entry[\"file\"]\n",
    "        txt = txt_path.read_text(encoding=\"utf-8\", errors=\"ignore\") if txt_path.exists() else \"\"\n",
    "        fallback = fallback_name_from_text(txt)\n",
    "        if fallback:\n",
    "            entry[\"primary_name_clean\"] = fallback\n",
    "            fix_count += 1\n",
    "        else:\n",
    "            entry[\"primary_name_clean\"] = None\n",
    "    else:\n",
    "        entry[\"primary_name_clean\"] = orig\n",
    "    cleaned.append(entry)\n",
    "\n",
    "with open(OUT_CLEAN, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(cleaned, f, indent=2)\n",
    "\n",
    "print(f\"Saved cleaned NER with 'primary_name_clean' -> {OUT_CLEAN}  (fixed {fix_count} names)\")\n",
    "# show a small sample mapping\n",
    "for e in cleaned[:6]:\n",
    "    print(e[\"file\"], \"->\", e.get(\"primary_name\"), \"->\", e.get(\"primary_name_fixed\"), \"->\", e.get(\"primary_name_clean\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf75898",
   "metadata": {},
   "source": [
    "### Step 3: Generating BERT sentence embedding for matching cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b964f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: all-mpnet-base-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding (clean names): 100%|██████████| 500/500 [04:36<00:00,  1.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regenerated embeddings for 500 resumes; emb_index updated at ..\\data\\resumes_preocr\\emb_index.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 2) Regenerate embeddings using primary_name_clean\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np, json\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "ROOT = Path(\"../data/resumes_preocr\")\n",
    "TXT_DIR = ROOT / \"txt\"\n",
    "CLEAN = ROOT / \"ner_fixed_names_clean.json\"\n",
    "EMB_DIR = ROOT / \"embeddings\"\n",
    "EMB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODEL_NAME = \"all-mpnet-base-v2\"   # good BERT-like sentence embedder\n",
    "\n",
    "# load cleaned metadata\n",
    "with open(CLEAN, \"r\", encoding=\"utf-8\") as f:\n",
    "    meta = json.load(f)\n",
    "\n",
    "model = SentenceTransformer(MODEL_NAME)\n",
    "print(\"Model loaded:\", MODEL_NAME)\n",
    "\n",
    "def load_text(fname, n_chars=1600):\n",
    "    p = TXT_DIR / fname\n",
    "    return p.read_text(encoding=\"utf-8\", errors=\"ignore\")[:n_chars] if p.exists() else \"\"\n",
    "\n",
    "count = 0\n",
    "for entry in tqdm(meta, desc=\"Embedding (clean names)\"):\n",
    "    fname = entry[\"file\"]\n",
    "    name = entry.get(\"primary_name_clean\") or \"\"\n",
    "    snippet = load_text(fname)\n",
    "    skills = \", \".join(entry.get(\"skills\",[])[:12])\n",
    "    orgs = \", \".join(entry.get(\"orgs\",[])[:6])\n",
    "    combined = (name + \"\\n\\n\" + snippet + \"\\n\\nSkills: \" + skills + \"\\nOrgs: \" + orgs).strip()\n",
    "    if not combined:\n",
    "        combined = snippet or \" \"\n",
    "    emb = model.encode(combined, show_progress_bar=False)\n",
    "    npy_path = EMB_DIR / (Path(fname).stem + \".npy\")\n",
    "    np.save(npy_path, emb)\n",
    "    count += 1\n",
    "\n",
    "# write emb_index.json\n",
    "emb_index = [{\"file\": e[\"file\"], \"npy\": str(EMB_DIR / (Path(e[\"file\"]).stem + \".npy\"))} for e in meta]\n",
    "with open(ROOT / \"emb_index.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(emb_index, f, indent=2)\n",
    "\n",
    "print(\"Regenerated embeddings for\", count, \"resumes; emb_index updated at\", ROOT / \"emb_index.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4495a100",
   "metadata": {},
   "source": [
    "### Step 4: Scoring resumes using custom scoring heurisitcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3650eb9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaned Refined Top 10 resumes:\n",
      "101.219 | sim=0.476 (0.738) | nameQ=1.00 | exp=25 | contact=found   | skills= 9 | LE HOANG                                 | sample_00419.txt\n",
      " 99.868 | sim=0.416 (0.708) | nameQ=1.00 | exp=17 | contact=found   | skills=10 | Hoàng Quang Hưng                         | sample_00218.txt\n",
      " 98.123 | sim=0.339 (0.669) | nameQ=1.00 | exp=40 | contact=found   | skills= 8 | Doan Minh Hoang                          | sample_00220.txt\n",
      " 97.977 | sim=0.332 (0.666) | nameQ=1.00 | exp=17 | contact=found   | skills= 9 | Nguyen Ngoc Dang                         | sample_00159.txt\n",
      " 96.126 | sim=0.517 (0.758) | nameQ=1.00 | exp=10 | contact=found   | skills= 9 | NGUYEN VAN HUONG\n",
      "Day                     | sample_00261.txt\n",
      " 92.210 | sim=0.432 (0.716) | nameQ=1.00 | exp=16 | contact=missing | skills=11 | Chung Vi Huy                             | sample_00267.txt\n",
      " 91.011 | sim=0.378 (0.689) | nameQ=1.00 | exp=15 | contact=missing | skills= 6 | DINH NGUYEN DANG KHOA                    | sample_00236.txt\n",
      " 90.233 | sim=0.433 (0.716) | nameQ=1.00 | exp=16 | contact=found   | skills= 4 | Van Luong                                | sample_00465.txt\n",
      " 89.771 | sim=0.377 (0.688) | nameQ=1.00 | exp=14 | contact=missing | skills= 6 | PHAN MINH THỌ                            | sample_00180.txt\n",
      " 86.899 | sim=0.480 (0.740) | nameQ=1.00 | exp= 3 | contact=found   | skills=11 | Trần Dưỡng                               | sample_00444.txt\n",
      "\n",
      "Saved: ..\\data\\resumes_preocr\\resume_scores_refined_clean.json and ..\\data\\resumes_preocr\\leaderboard_refined_clean.csv\n"
     ]
    }
   ],
   "source": [
    "# 3) Recompute refined scoring using cleaned names\n",
    "import json, re, csv\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "ROOT = Path(\"../data/resumes_preocr\")\n",
    "TXT_DIR = ROOT / \"txt\"\n",
    "CLEAN = ROOT / \"ner_fixed_names_clean.json\"\n",
    "EMB_INDEX = ROOT / \"emb_index.json\"\n",
    "OUT_JSON = ROOT / \"resume_scores_refined_clean.json\"\n",
    "OUT_CSV = ROOT / \"leaderboard_refined_clean.csv\"\n",
    "\n",
    "# loads\n",
    "with open(CLEAN, \"r\", encoding=\"utf-8\") as f: meta = json.load(f)\n",
    "with open(EMB_INDEX, \"r\", encoding=\"utf-8\") as f: emb_index = json.load(f)\n",
    "\n",
    "file_to_entry = {e[\"file\"]: e for e in meta}\n",
    "file_to_npy = {e[\"file\"]: e[\"npy\"] for e in emb_index}\n",
    "\n",
    "# Job description embedding (same JD or modify)\n",
    "JOB_DESC = \"\"\"\n",
    "We are hiring an NLP Engineer with strong Python skills, proven experience with PyTorch and Transformers,\n",
    "solid foundations in NLP methods (tokenization, attention, sequence models), and experience deploying models to production.\n",
    "Experience with ML pipelines, REST APIs, and cloud deployment (AWS/GCP) is a plus.\n",
    "\"\"\"\n",
    "MODEL_NAME = \"all-mpnet-base-v2\"\n",
    "model = SentenceTransformer(MODEL_NAME)\n",
    "job_emb = model.encode(JOB_DESC)\n",
    "\n",
    "# small helper to estimate experience (reuse previous helper)\n",
    "YEARS_PHRASE_RE = re.compile(r\"(\\d+)\\s+(?:years|yrs)\\b\", re.I)\n",
    "def estimate_experience(text):\n",
    "    m = YEARS_PHRASE_RE.search(text)\n",
    "    if m:\n",
    "        try: return min(int(m.group(1)), 40)\n",
    "        except: pass\n",
    "    # fallback: look for 2010-2014 style\n",
    "    m2 = re.search(r\"((19|20)\\d{2})\\s*[\\-–—]\\s*((19|20)\\d{2})\", text)\n",
    "    if m2:\n",
    "        try:\n",
    "            return min(int(m2.group(3)) - int(m2.group(1)), 40)\n",
    "        except: pass\n",
    "    return 0\n",
    "\n",
    "# scoring weights (tweak as needed)\n",
    "WEIGHT_CONTACT = 8\n",
    "WEIGHT_SKILL = 5\n",
    "WEIGHT_SIM = 45\n",
    "WEIGHT_NAME_QUALITY = 6\n",
    "WEIGHT_LANG_MATCH = 6\n",
    "WEIGHT_EXP_PER_YEAR = 1.2\n",
    "CAP_SKILLS = 6\n",
    "CAP_EXP = 15\n",
    "\n",
    "results = []\n",
    "for rec in emb_index:\n",
    "    fname = rec[\"file\"]\n",
    "    npy = rec[\"npy\"]\n",
    "    if fname not in file_to_entry: continue\n",
    "    entry = file_to_entry[fname]\n",
    "    try:\n",
    "        emb = np.load(npy)\n",
    "    except Exception:\n",
    "        continue\n",
    "    raw_sim = float(cosine_similarity([emb], [job_emb])[0,0])\n",
    "    sim_norm = (raw_sim + 1)/2.0\n",
    "    contact_bonus = WEIGHT_CONTACT if entry.get(\"contact_status\") == \"found\" else 0\n",
    "    n_skills = len(entry.get(\"skills\",[]))\n",
    "    skill_score = min(CAP_SKILLS, n_skills) * WEIGHT_SKILL\n",
    "    # name_quality heuristic: prefer names with at least 2 alphabetic words\n",
    "    name = entry.get(\"primary_name_clean\") or \"\"\n",
    "    name_quality = 1.0 if (len(name.split())>=2 and re.search(r\"[A-Za-z\\u00C0-\\u017F]\", name)) else 0.6 if len(name.split())==1 and re.search(r\"[A-Za-z\\u00C0-\\u017F]\", name) else 0\n",
    "    name_score = name_quality * WEIGHT_NAME_QUALITY\n",
    "    # language bonus\n",
    "    lang_bonus = WEIGHT_LANG_MATCH if entry.get(\"language\",\"\") == \"en\" else 0\n",
    "    # experience\n",
    "    text = (TXT_DIR / fname).read_text(encoding=\"utf-8\", errors=\"ignore\") if (TXT_DIR/ fname).exists() else \"\"\n",
    "    exp = estimate_experience(text)\n",
    "    exp_score = min(exp, CAP_EXP) * WEIGHT_EXP_PER_YEAR\n",
    "    sim_score = sim_norm * WEIGHT_SIM\n",
    "    total = contact_bonus + skill_score + sim_score + name_score + lang_bonus + exp_score\n",
    "    results.append({\n",
    "        \"file\": fname,\n",
    "        \"primary_name\": name,\n",
    "        \"contact_status\": entry.get(\"contact_status\",\"missing\"),\n",
    "        \"n_skills\": n_skills,\n",
    "        \"sim_raw\": round(raw_sim,4),\n",
    "        \"sim_norm\": round(sim_norm,4),\n",
    "        \"name_quality\": round(name_quality,3),\n",
    "        \"exp_years\": exp,\n",
    "        \"score\": round(total,3),\n",
    "        \"top_skills\": entry.get(\"skills\",[])[:8],\n",
    "        \"preview\": entry.get(\"original_preview\",\"\")[:300].replace(\"\\n\",\"\\\\n\")\n",
    "    })\n",
    "\n",
    "results_sorted = sorted(results, key=lambda x: x[\"score\"], reverse=True)\n",
    "\n",
    "# save\n",
    "with open(OUT_JSON, \"w\", encoding=\"utf-8\") as f: json.dump(results_sorted, f, indent=2)\n",
    "with open(OUT_CSV, \"w\", newline='', encoding=\"utf-8\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\"score\",\"sim_raw\",\"sim_norm\",\"name_quality\",\"exp_years\",\"contact_status\",\"n_skills\",\"primary_name\",\"file\",\"top_skills\",\"preview\"])\n",
    "    writer.writeheader()\n",
    "    for r in results_sorted: writer.writerow({k:r.get(k,\"\") for k in writer.fieldnames})\n",
    "\n",
    "# print top 10\n",
    "print(\"\\nCleaned Refined Top 10 resumes:\")\n",
    "for s in results_sorted[:10]:\n",
    "    print(f\"{s['score']:7.3f} | sim={s['sim_raw']:.3f} ({s['sim_norm']:.3f}) | nameQ={s['name_quality']:.2f} | exp={s['exp_years']:2} | contact={s['contact_status']:<7} | skills={s['n_skills']:2} | {s['primary_name'][:40]:40} | {s['file']}\")\n",
    "print(\"\\nSaved:\", OUT_JSON, \"and\", OUT_CSV)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b171658",
   "metadata": {},
   "source": [
    "##### OLD NER Technique using SpaCy -> Drawback: Only english compatible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d9a90f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 500 text files with spaCy NER...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NER: 100%|██████████| 500/500 [03:54<00:00,  2.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== NER SUMMARY ===\n",
      "Files processed: 500\n",
      "Emails found: 1\n",
      "Phones found: 43\n",
      "Files with detected skills: 499\n",
      "Average text length: 3762 chars\n",
      "Saved ner extract -> ..\\data\\resumes_preocr\\ner_extract.json\n",
      "\n",
      "Sample entry (first file):\n",
      "{'chars': 2812,\n",
      " 'emails': [],\n",
      " 'file': 'sample_00000.txt',\n",
      " 'locations': ['Hanoi', 'Robot', 'Tkinter', 'Taiwan'],\n",
      " 'names': ['Nguyen Dang Binh', 'Luster LightTech', 'Debug'],\n",
      " 'orgs': ['AI/Computer Vision Engineer\\nAddress', 'Vision Software Senior', 'BacNinh', 'AOI', 'Luxshare-ICT',\n",
      "          'Medical Image Segmentation'],\n",
      " 'phones': [],\n",
      " 'preview': 'Nguyen Dang Binh – AI/Computer Vision Engineer\\\\nAddress: Bac Ninh City\\\\nE '\n",
      "            'mail:\\\\nMobile/Zalo:\\\\nBirthday: Jan 8th 1986\\\\nK EY STRENGTHS\\\\nMachine vision \\uf020Team '\n",
      "            'Leader\\\\n\\uf0a7\\uf020\\\\n\\uf0a7\\uf020AI engineer Automation software\\\\nP ROFESSIONAL EXPERIENCE\\\\nVision '\n",
      "            'Software Senior (Aug 2023 – Now): Luster LightTech, BacNinh, VN:\\\\n\\uf0d8 Develop the low-code software '\n",
      "            'from internal platforms for vision projects.\\\\nDebug the software at AOI machine at Luxshare-ICT, '\n",
      "            'Goerteck factory.\\\\n\\uf0d8\\\\nSoftware Engineer Senior Leader (Mar 2020 – May 2023): Electronic ACE '\n",
      "            'Antenna\\\\nCompany | Hanoi, VN:\\\\n\\uf0d8 Measurement and detection defect software project: Develop t',\n",
      " 'skills': ['c', 'c++', 'deep learning', 'machine learning', 'python', 'pytorch', 'tensorflow']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 3 (run): SpaCy NER + rule-based extraction\n",
    "import json, re, sys\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ensure spaCy model installed\n",
    "try:\n",
    "    import spacy\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except Exception as e:\n",
    "    print(\"spaCy model en_core_web_sm not found. Install it and re-run:\")\n",
    "    print(\"    python -m spacy download en_core_web_sm\")\n",
    "    raise\n",
    "\n",
    "ROOT = Path(\"../data/resumes_preocr\")\n",
    "TXT_DIR = ROOT / \"txt\"\n",
    "OUT_EXTRACT = ROOT / \"ner_extract.json\"\n",
    "\n",
    "EMAIL_RE = re.compile(r\"[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+\")\n",
    "PHONE_RE = re.compile(r\"(?:\\+?\\d{1,3}[-.\\s]?)?(?:\\(?\\d{2,4}\\)?[-.\\s]?)?\\d{6,12}\")\n",
    "\n",
    "# small initial skills vocabulary — expand with your domain\n",
    "SKILLS = {\n",
    "    \"python\",\"java\",\"c++\",\"c\",\"sql\",\"nlp\",\"natural language processing\",\"deep learning\",\n",
    "    \"machine learning\",\"tensorflow\",\"pytorch\",\"react\",\"docker\",\"kubernetes\",\"git\",\n",
    "    \"aws\",\"gcp\",\"azure\",\"linux\",\"pandas\",\"numpy\",\"scikit-learn\",\"keras\"\n",
    "}\n",
    "\n",
    "def extract_skills(text):\n",
    "    tl = text.lower()\n",
    "    found = []\n",
    "    for s in SKILLS:\n",
    "        if s in tl:\n",
    "            found.append(s)\n",
    "    return sorted(found)\n",
    "\n",
    "results = []\n",
    "files = sorted(TXT_DIR.glob(\"*.txt\"))\n",
    "print(\"Processing\", len(files), \"text files with spaCy NER...\")\n",
    "\n",
    "for p in tqdm(files, desc=\"NER\"):\n",
    "    txt = p.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    doc = nlp(txt)\n",
    "    names = []\n",
    "    orgs = []\n",
    "    gpes = []\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PERSON\":\n",
    "            names.append(ent.text)\n",
    "        elif ent.label_ in (\"ORG\",\"NORP\"):\n",
    "            orgs.append(ent.text)\n",
    "        elif ent.label_ == \"GPE\":\n",
    "            gpes.append(ent.text)\n",
    "    emails = EMAIL_RE.findall(txt)\n",
    "    phones = PHONE_RE.findall(txt)\n",
    "    skills = extract_skills(txt)\n",
    "    results.append({\n",
    "        \"file\": p.name,\n",
    "        \"names\": list(dict.fromkeys(names))[:3],\n",
    "        \"orgs\": list(dict.fromkeys(orgs))[:6],\n",
    "        \"locations\": list(dict.fromkeys(gpes))[:4],\n",
    "        \"emails\": emails,\n",
    "        \"phones\": phones,\n",
    "        \"skills\": skills,\n",
    "        \"chars\": len(txt),\n",
    "        \"preview\": txt[:600].replace(\"\\n\",\"\\\\n\")\n",
    "    })\n",
    "\n",
    "# save results\n",
    "with open(OUT_EXTRACT, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "# print quick stats\n",
    "total = len(results)\n",
    "emails_found = sum(1 for r in results if r[\"emails\"])\n",
    "phones_found = sum(1 for r in results if r[\"phones\"])\n",
    "skills_found = sum(1 for r in results if r[\"skills\"])\n",
    "avg_len = sum(r[\"chars\"] for r in results) / total if total else 0\n",
    "\n",
    "print(\"\\n=== NER SUMMARY ===\")\n",
    "print(f\"Files processed: {total}\")\n",
    "print(f\"Emails found: {emails_found}\")\n",
    "print(f\"Phones found: {phones_found}\")\n",
    "print(f\"Files with detected skills: {skills_found}\")\n",
    "print(f\"Average text length: {avg_len:.0f} chars\")\n",
    "print(\"Saved ner extract ->\", OUT_EXTRACT)\n",
    "print(\"\\nSample entry (first file):\")\n",
    "if results:\n",
    "    import pprint\n",
    "    pprint.pprint(results[0], compact=True, width=120)\n",
    "else:\n",
    "    print(\"No text files found in\", TXT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e745214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote enhanced NER -> ..\\data\\resumes_preocr\\ner_enhanced.json\n",
      "Total entries: 500\n",
      "Contact found: 152\n",
      "States: {'lang:en': 183, 'lang:vi': 315, 'has_phone': 152, 'lang:nl': 1, 'lang:zh-cn': 1, 'has_email': 1}\n",
      "\n",
      "Sample enhanced entry (first):\n",
      "{'chars': 2812,\n",
      " 'contact_status': 'missing',\n",
      " 'emails': [],\n",
      " 'file': 'sample_00000.txt',\n",
      " 'github': [],\n",
      " 'language': 'en',\n",
      " 'linkedin': [],\n",
      " 'orgs': ['BacNinh', 'AOI', 'Luxshare-ICT', 'Medical Image Segmentation'],\n",
      " 'original_preview': 'Nguyen Dang Binh – AI/Computer Vision '\n",
      "                     'Engineer\\\\nAddress: Bac Ninh City\\\\nE '\n",
      "                     'mail:\\\\nMobile/Zalo:\\\\nBirthday: Jan 8th 1986\\\\nK EY '\n",
      "                     'STRENGTHS\\\\nMachine vision \\uf020Team '\n",
      "                     'Leader\\\\n\\uf0a7\\uf020\\\\n\\uf0a7\\uf020AI engineer '\n",
      "                     'Automation software\\\\nP ROFESSIONAL EXPERIENCE\\\\nVision '\n",
      "                     'Software Senior (Aug 2023 – Now): Luster LightTech, '\n",
      "                     'BacNinh, VN:\\\\n\\uf0d8 Develop the low-code software from '\n",
      "                     'internal platforms for vision projects.\\\\nDebug the '\n",
      "                     'software at AOI machine at Luxshare-ICT, Goerteck '\n",
      "                     'factory.\\\\n\\uf0d8\\\\nSoftware Engineer Senior Leader (Mar '\n",
      "                     '2020 – May 2023): Electronic ACE Antenna\\\\nCompany | '\n",
      "                     'Hanoi, VN:\\\\n\\uf0d8 Measurement and detection defect '\n",
      "                     'software project: Develop t',\n",
      " 'phones': [],\n",
      " 'primary_name': 'Nguyen Dang Binh',\n",
      " 'skills': ['c',\n",
      "            'c#',\n",
      "            'c++',\n",
      "            'communication',\n",
      "            'computer vision',\n",
      "            'deep learning',\n",
      "            'machine learning',\n",
      "            'mlops',\n",
      "            'opencv',\n",
      "            'python',\n",
      "            'pytorch',\n",
      "            'tensorflow']}\n"
     ]
    }
   ],
   "source": [
    "# Post-process NER outputs: normalize names, improve phone/email detection, expand skills, detect language\n",
    "import json, re, os\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "\n",
    "ROOT = Path(\"../data/resumes_preocr\")\n",
    "NER_IN = ROOT / \"ner_extract.json\"\n",
    "NER_OUT = ROOT / \"ner_enhanced.json\"\n",
    "TXT_DIR = ROOT / \"txt\"\n",
    "\n",
    "# load NER results\n",
    "with open(NER_IN, \"r\", encoding=\"utf-8\") as f:\n",
    "    ner = json.load(f)\n",
    "\n",
    "# stronger regexes\n",
    "EMAIL_RE = re.compile(r\"[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+\")\n",
    "# phone: look for sequences of digits with common separators, allow country code, require 7-15 digits total\n",
    "PHONE_RE = re.compile(r\"(?:\\+?\\d{1,3}[-.\\s]?)?(?:\\(?\\d{1,4}\\)?[-.\\s]?)?(?:\\d[-.\\s]?){6,14}\\d\")\n",
    "LINKEDIN_RE = re.compile(r\"(linkedin\\.com/[A-Za-z0-9_\\-./]+|linkedin:[A-Za-z0-9_\\-/]+)\", re.I)\n",
    "GITHUB_RE = re.compile(r\"(github\\.com/[A-Za-z0-9_.\\-]+|github:[A-Za-z0-9_.\\-]+)\", re.I)\n",
    "\n",
    "# expanded skills vocabulary (extendable)\n",
    "MORE_SKILLS = [\n",
    "    \"python\",\"java\",\"c++\",\"c\",\"c#\",\"sql\",\"nlp\",\"natural language processing\",\"deep learning\",\n",
    "    \"machine learning\",\"tensorflow\",\"pytorch\",\"keras\",\"scikit-learn\",\"spark\",\"hadoop\",\n",
    "    \"pandas\",\"numpy\",\"matplotlib\",\"seaborn\",\"docker\",\"kubernetes\",\"aws\",\"gcp\",\"azure\",\n",
    "    \"react\",\"node.js\",\"express\",\"flask\",\"django\",\"rest api\",\"graphql\",\"git\",\"linux\",\n",
    "    \"bash\",\"mlops\",\"computer vision\",\"opencv\",\"pandas\",\"communication\",\"leadership\",\n",
    "    \"excel\",\"tableau\",\"power bi\",\"spark\",\"hive\",\"bigquery\",\"seo\",\"marketing\",\"sales\"\n",
    "]\n",
    "# normalize to lowercase for substring matching\n",
    "MORE_SKILLS = list(dict.fromkeys([s.lower() for s in MORE_SKILLS]))\n",
    "\n",
    "# optional fuzzy matching: use rapidfuzz if installed\n",
    "try:\n",
    "    from rapidfuzz import process, fuzz\n",
    "    FUZZY_AVAILABLE = True\n",
    "except Exception:\n",
    "    FUZZY_AVAILABLE = False\n",
    "\n",
    "# helper: load raw text for a sample file if present\n",
    "def load_text_for_file(filename):\n",
    "    txt_path = TXT_DIR / filename\n",
    "    if txt_path.exists():\n",
    "        return txt_path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    return \"\"\n",
    "\n",
    "# helper: clean orgs (remove entries that look like addresses or job titles)\n",
    "JOB_TITLE_KEYWORDS = set([\"engineer\",\"developer\",\"manager\",\"senior\",\"lead\",\"intern\",\"assistant\",\"consultant\",\n",
    "                          \"officer\",\"analyst\",\"specialist\",\"architect\",\"director\",\"president\",\"coordinator\",\n",
    "                          \"supervisor\"])\n",
    "def clean_org_list(orgs):\n",
    "    cleaned = []\n",
    "    for o in orgs:\n",
    "        s = o.strip()\n",
    "        # skip if empty or obviously a sentence fragment or contains newline markers\n",
    "        if not s or len(s) < 2:\n",
    "            continue\n",
    "        # skip if it contains 'address' or 'mobile' or 'email' (likely not org)\n",
    "        low = s.lower()\n",
    "        if any(x in low for x in (\"address\",\"mobile\",\"email\",\"phone\",\"birthday\",\"birth\",\"cv\",\"c.v\",\"objective\",\"profile\")):\n",
    "            continue\n",
    "        # skip if it's too long garbage with many punctuation characters\n",
    "        punct_ratio = sum(1 for ch in s if not ch.isalnum() and not ch.isspace()) / max(1,len(s))\n",
    "        if punct_ratio > 0.25 and len(s) < 50:\n",
    "            continue\n",
    "        # optionally skip entries that are likely job titles (we want organizations)\n",
    "        if any(jk in low for jk in JOB_TITLE_KEYWORDS):\n",
    "            # allow if it contains a comma and an apparent company name later\n",
    "            if \",\" not in s and len(s.split()) < 6:\n",
    "                # likely a job title, skip\n",
    "                continue\n",
    "        cleaned.append(s)\n",
    "    # dedupe keeping order\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for o in cleaned:\n",
    "        if o not in seen:\n",
    "            out.append(o); seen.add(o)\n",
    "    return out\n",
    "\n",
    "# helper: expand skills by substring (and fuzzy if available)\n",
    "def extract_skills_from_text(text):\n",
    "    tl = text.lower()\n",
    "    found = set()\n",
    "    for s in MORE_SKILLS:\n",
    "        if s in tl:\n",
    "            found.add(s)\n",
    "    # fuzzy: if available, match tokens\n",
    "    if FUZZY_AVAILABLE and not found:\n",
    "        # take top fuzzy matches for single-word tokens\n",
    "        tokens = set(re.findall(r\"[A-Za-z0-9+#\\.\\-]+\", tl))\n",
    "        for tok in tokens:\n",
    "            best = process.extractOne(tok, MORE_SKILLS, scorer=fuzz.partial_ratio)\n",
    "            if best and best[1] >= 90:\n",
    "                found.add(best[0])\n",
    "    return sorted(found)\n",
    "\n",
    "# try langdetect if available\n",
    "try:\n",
    "    from langdetect import detect, DetectorFactory\n",
    "    DetectorFactory.seed = 0\n",
    "    LANGDET_AVAILABLE = True\n",
    "except Exception:\n",
    "    LANGDET_AVAILABLE = False\n",
    "\n",
    "# process and enhance\n",
    "enhanced = []\n",
    "counters = Counter()\n",
    "for entry in ner:\n",
    "    fname = entry.get(\"file\")\n",
    "    raw_text = load_text_for_file(fname)\n",
    "    # combine original fields and raw text for better detection\n",
    "    emails = entry.get(\"emails\", []) or EMAIL_RE.findall(raw_text)\n",
    "    phones = entry.get(\"phones\", []) or PHONE_RE.findall(raw_text)\n",
    "    linkedin = LINKEDIN_RE.findall(raw_text) or LINKEDIN_RE.findall(\" \".join(entry.get(\"orgs\",[])))\n",
    "    github = GITHUB_RE.findall(raw_text) or GITHUB_RE.findall(\" \".join(entry.get(\"orgs\",[])))\n",
    "    # choose primary name: prefer first PERSON entity; if no person, try first line of text\n",
    "    primary_name = None\n",
    "    if entry.get(\"names\"):\n",
    "        # pick first name-like token but avoid when it's obviously organization/job (heuristic)\n",
    "        for nm in entry[\"names\"]:\n",
    "            if nm and len(nm) > 1 and not any(t.lower() in nm.lower() for t in (\"engineer\",\"developer\",\"company\",\"address\",\"mobile\")):\n",
    "                primary_name = nm.strip()\n",
    "                break\n",
    "        if primary_name is None:\n",
    "            primary_name = entry[\"names\"][0].strip()\n",
    "    if not primary_name and raw_text:\n",
    "        # take the first non-empty line up to 80 chars as a fallback\n",
    "        for ln in raw_text.splitlines():\n",
    "            ln = ln.strip()\n",
    "            if ln and len(ln) < 80:\n",
    "                primary_name = ln\n",
    "                break\n",
    "    # clean orgs\n",
    "    orgs_clean = clean_org_list(entry.get(\"orgs\",[]))\n",
    "    # expand skills\n",
    "    text_for_skills = \" \".join([raw_text, entry.get(\"preview\",\"\")])\n",
    "    skills = sorted(set(entry.get(\"skills\",[])) | set(extract_skills_from_text(text_for_skills)))\n",
    "    # detect language\n",
    "    lang = \"unknown\"\n",
    "    if LANGDET_AVAILABLE and raw_text.strip():\n",
    "        try:\n",
    "            lang = detect(raw_text[:2000])\n",
    "        except Exception:\n",
    "            lang = \"unknown\"\n",
    "    # contact status\n",
    "    if emails or phones:\n",
    "        contact_status = \"found\"\n",
    "    else:\n",
    "        # can't reliably detect redaction here (we didn't run redaction detector on this dataset)\n",
    "        contact_status = \"missing\"\n",
    "    # assemble enhanced entry\n",
    "    e = {\n",
    "        \"file\": fname,\n",
    "        \"primary_name\": primary_name or \"\",\n",
    "        \"orgs\": orgs_clean,\n",
    "        \"emails\": emails,\n",
    "        \"phones\": phones,\n",
    "        \"linkedin\": linkedin,\n",
    "        \"github\": github,\n",
    "        \"skills\": skills,\n",
    "        \"chars\": entry.get(\"chars\", 0),\n",
    "        \"language\": lang,\n",
    "        \"contact_status\": contact_status,\n",
    "        \"original_preview\": entry.get(\"preview\",\"\")\n",
    "    }\n",
    "    enhanced.append(e)\n",
    "    counters.update([contact_status])\n",
    "    if emails: counters.update([\"has_email\"])\n",
    "    if phones: counters.update([\"has_phone\"])\n",
    "    counters.update([\"lang:\"+lang])\n",
    "\n",
    "# save\n",
    "with open(NER_OUT, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(enhanced, f, indent=2)\n",
    "\n",
    "# print a concise summary\n",
    "total = len(enhanced)\n",
    "print(\"Wrote enhanced NER ->\", NER_OUT)\n",
    "print(\"Total entries:\", total)\n",
    "print(\"Contact found:\", counters[\"found\"] if \"found\" in counters else sum(1 for e in enhanced if e[\"contact_status\"]==\"found\"))\n",
    "print(\"States:\", {k:v for k,v in counters.items() if k.startswith(\"lang:\") or k in ('has_email','has_phone')})\n",
    "print(\"\\nSample enhanced entry (first):\")\n",
    "pprint(enhanced[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f9e40da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Work\\Capstone_Project\\resume-nlp\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: all-mpnet-base-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Work\\Capstone_Project\\resume-nlp\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\abhis\\.cache\\huggingface\\hub\\models--sentence-transformers--all-mpnet-base-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Embedding resumes: 100%|██████████| 500/500 [05:25<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved embeddings: 500 -> ..\\data\\resumes_preocr\\embeddings\n",
      "\n",
      "Top 10 resumes by baseline score:\n",
      " 72.619 | sim=0.557 | contact=found   | skills= 9 | Python                                   | sample_00396.txt\n",
      " 70.418 | sim=0.517 | contact=found   | skills= 9 | NGUYEN VAN HUONG\n",
      "Day                     | sample_00261.txt\n",
      " 68.397 | sim=0.480 | contact=found   | skills=11 | Trần Dưỡng                               | sample_00444.txt\n",
      " 67.514 | sim=0.464 | contact=found   | skills= 7 | Yen Hoa                                  | sample_00075.txt\n",
      " 67.091 | sim=0.456 | contact=found   | skills= 9 | Duy Duc Thien                            | sample_00289.txt\n",
      " 66.588 | sim=0.447 | contact=found   | skills= 9 | Marital Status                           | sample_00419.txt\n",
      " 65.818 | sim=0.433 | contact=found   | skills= 6 | Ho Chi Minh                              | sample_00066.txt\n",
      " 65.018 | sim=0.418 | contact=found   | skills= 8 | Shipworks                                | sample_00174.txt\n",
      " 64.756 | sim=0.414 | contact=found   | skills=10 | Quang Hưng                               | sample_00218.txt\n",
      " 64.559 | sim=0.519 | contact=found   | skills= 4 | Ho Chi Minh                              | sample_00216.txt\n",
      "Saved resume_scores.json and leaderboard.csv in ..\\data\\resumes_preocr\n"
     ]
    }
   ],
   "source": [
    "# === Create BERT-based embeddings (all-mpnet-base-v2) and compute baseline scores ===\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np, json, os\n",
    "from pathlib import Path\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "ROOT = Path(\"../data/resumes_preocr\")\n",
    "TXT_DIR = ROOT / \"txt\"\n",
    "NER_ENH = ROOT / \"ner_enhanced.json\"\n",
    "EMB_DIR = ROOT / \"embeddings\"\n",
    "EMB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 1) load enhanced NER entries\n",
    "with open(NER_ENH, \"r\", encoding=\"utf-8\") as f:\n",
    "    ner = json.load(f)\n",
    "\n",
    "# helper to read resume text\n",
    "def load_text(fname):\n",
    "    p = TXT_DIR / fname\n",
    "    return p.read_text(encoding=\"utf-8\", errors=\"ignore\") if p.exists() else \"\"\n",
    "\n",
    "# 2) select BERT-style sentence-transformer model (MPNet = BERT-family)\n",
    "MODEL_NAME = \"all-mpnet-base-v2\"   # strong BERT-like sentence embedding model\n",
    "print(\"Loading model:\", MODEL_NAME)\n",
    "model = SentenceTransformer(MODEL_NAME)\n",
    "\n",
    "# 3) build & save embeddings for each resume (uses name + chunk + skills + orgs)\n",
    "emb_index = []\n",
    "for entry in tqdm(ner, desc=\"Embedding resumes\"):\n",
    "    fname = entry[\"file\"]\n",
    "    txt = load_text(fname) or \"\"\n",
    "    snippet = txt[:1600]  # representative chunk; increase if you'd like\n",
    "    name = entry.get(\"primary_name\",\"\") or \"\"\n",
    "    skills_str = \", \".join(entry.get(\"skills\",[]))\n",
    "    orgs_str = \", \".join(entry.get(\"orgs\",[]))\n",
    "    combined = (name + \"\\n\\n\" + snippet + \"\\n\\nSkills: \" + skills_str + \"\\nOrgs: \" + orgs_str).strip()\n",
    "    if not combined:\n",
    "        combined = \" \"\n",
    "    emb = model.encode(combined, show_progress_bar=False)\n",
    "    npy_path = EMB_DIR / (Path(fname).stem + \".npy\")\n",
    "    np.save(npy_path, emb)\n",
    "    emb_index.append({\"file\": fname, \"npy\": str(npy_path)})\n",
    "\n",
    "# save emb index\n",
    "with open(ROOT / \"emb_index.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(emb_index, f, indent=2)\n",
    "\n",
    "print(\"Saved embeddings:\", len(emb_index), \"->\", EMB_DIR)\n",
    "\n",
    "# 4) Job description (edit this to match the role you want)\n",
    "JOB_DESC = \"\"\"\n",
    "We are hiring an NLP Engineer with strong Python skills, proven experience with PyTorch and Transformers,\n",
    "solid foundations in NLP methods (tokenization, attention, sequence models), and experience deploying models to production.\n",
    "Experience with ML pipelines, REST APIs, and cloud deployment (AWS/GCP) is a plus.\n",
    "\"\"\"\n",
    "\n",
    "job_emb = model.encode(JOB_DESC)\n",
    "\n",
    "# 5) scoring parameters (adjustable later)\n",
    "WEIGHT_CONTACT = 12        # bonus if contact exists\n",
    "WEIGHT_SKILL = 6           # per matched skill (cap)\n",
    "WEIGHT_SIM = 55            # multiplier for similarity score\n",
    "CAP_SKILLS = 5\n",
    "\n",
    "# 6) compute similarity + baseline score\n",
    "file_to_entry = {e[\"file\"]: e for e in ner}\n",
    "scores = []\n",
    "for rec in emb_index:\n",
    "    fname = rec[\"file\"]\n",
    "    emb = np.load(rec[\"npy\"])\n",
    "    sim = float(cosine_similarity([emb], [job_emb])[0,0])\n",
    "    entry = file_to_entry.get(fname, {})\n",
    "    contact_bonus = WEIGHT_CONTACT if entry.get(\"contact_status\") == \"found\" else 0\n",
    "    skills_count = len(entry.get(\"skills\", []))\n",
    "    skill_score = min(CAP_SKILLS, skills_count) * WEIGHT_SKILL\n",
    "    sim_score = sim * WEIGHT_SIM\n",
    "    total = contact_bonus + skill_score + sim_score\n",
    "    scores.append({\n",
    "        \"file\": fname,\n",
    "        \"primary_name\": entry.get(\"primary_name\",\"\"),\n",
    "        \"contact_status\": entry.get(\"contact_status\",\"missing\"),\n",
    "        \"n_skills\": skills_count,\n",
    "        \"sim\": round(sim, 4),\n",
    "        \"score\": round(total, 4),\n",
    "        \"top_skills\": entry.get(\"skills\", [])[:6],\n",
    "        \"preview\": entry.get(\"original_preview\",\"\")[:300].replace(\"\\n\",\"\\\\n\")\n",
    "    })\n",
    "\n",
    "scores_sorted = sorted(scores, key=lambda x: x[\"score\"], reverse=True)\n",
    "\n",
    "# save results\n",
    "with open(ROOT / \"resume_scores.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(scores_sorted, f, indent=2)\n",
    "\n",
    "# print top 10\n",
    "print(\"\\nTop 10 resumes by baseline score:\")\n",
    "for s in scores_sorted[:10]:\n",
    "    print(f\"{s['score']:7.3f} | sim={s['sim']:.3f} | contact={s['contact_status']:<7} | skills={s['n_skills']:2} | {s['primary_name'][:40]:40} | {s['file']}\")\n",
    "\n",
    "# optionally: save leaderboard CSV for inspection\n",
    "import csv\n",
    "with open(ROOT / \"leaderboard.csv\", \"w\", newline='', encoding=\"utf-8\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\"score\",\"sim\",\"contact_status\",\"n_skills\",\"primary_name\",\"file\",\"top_skills\",\"preview\"])\n",
    "    writer.writeheader()\n",
    "    for r in scores_sorted:\n",
    "        writer.writerow({k: r.get(k,\"\") for k in writer.fieldnames})\n",
    "\n",
    "print(\"Saved resume_scores.json and leaderboard.csv in\", ROOT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "347492d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Work\\Capstone_Project\\resume-nlp\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: all-mpnet-base-v2\n",
      "\n",
      "Refined Top 10 resumes:\n",
      "100.559 | sim=0.447 (0.724) | nameQ=1.00 | exp=25 | contact=found   | skills= 9 | Marital Status                           | sample_00419.txt\n",
      " 99.809 | sim=0.414 (0.707) | nameQ=1.00 | exp=17 | contact=found   | skills=10 | Quang Hưng                               | sample_00218.txt\n",
      " 98.979 | sim=0.377 (0.688) | nameQ=1.00 | exp=40 | contact=found   | skills= 8 | Ly Van Sam                               | sample_00220.txt\n",
      " 97.977 | sim=0.332 (0.666) | nameQ=1.00 | exp=17 | contact=found   | skills= 9 | Nguyen Ngoc Dang                         | sample_00159.txt\n",
      " 96.126 | sim=0.517 (0.758) | nameQ=1.00 | exp=10 | contact=found   | skills= 9 | NGUYEN VAN HUONG\n",
      "Day                     | sample_00261.txt\n",
      " 92.210 | sim=0.432 (0.716) | nameQ=1.00 | exp=16 | contact=missing | skills=11 | Chung Vi Huy                             | sample_00267.txt\n",
      " 90.922 | sim=0.374 (0.687) | nameQ=1.00 | exp=15 | contact=missing | skills= 6 | Nguyen Van Tao Street                    | sample_00236.txt\n",
      " 90.233 | sim=0.433 (0.716) | nameQ=1.00 | exp=16 | contact=found   | skills= 4 | Van Luong                                | sample_00465.txt\n",
      " 88.647 | sim=0.433 (0.717) | nameQ=0.60 | exp=14 | contact=missing | skills= 6 | • Strong                                 | sample_00180.txt\n",
      " 86.899 | sim=0.480 (0.740) | nameQ=1.00 | exp= 3 | contact=found   | skills=11 | Trần Dưỡng                               | sample_00444.txt\n",
      "\n",
      "Saved: ..\\data\\resumes_preocr\\resume_scores_refined.json and ..\\data\\resumes_preocr\\leaderboard_refined.csv\n"
     ]
    }
   ],
   "source": [
    "# === Improved scoring: name_quality, language_match, rough_experience, sim normalization ===\n",
    "import json, re, csv\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "ROOT = Path(\"../data/resumes_preocr\")\n",
    "TXT_DIR = ROOT / \"txt\"\n",
    "NER_ENH = ROOT / \"ner_enhanced.json\"\n",
    "EMB_INDEX = ROOT / \"emb_index.json\"   # created by previous embedding run\n",
    "OUT_JSON = ROOT / \"resume_scores_refined.json\"\n",
    "OUT_CSV = ROOT / \"leaderboard_refined.csv\"\n",
    "\n",
    "# load data\n",
    "with open(NER_ENH, \"r\", encoding=\"utf-8\") as f:\n",
    "    ner = json.load(f)\n",
    "with open(EMB_INDEX, \"r\", encoding=\"utf-8\") as f:\n",
    "    emb_index = json.load(f)\n",
    "\n",
    "file_to_entry = {e[\"file\"]: e for e in ner}\n",
    "file_to_npy = {e[\"file\"]: e[\"npy\"] for e in emb_index}\n",
    "\n",
    "# load spaCy for name quality check\n",
    "try:\n",
    "    import spacy\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    SPACY_AVAILABLE = True\n",
    "except Exception:\n",
    "    SPACY_AVAILABLE = False\n",
    "    print(\"spaCy not available — name quality check will be skipped. Install en_core_web_sm to enable it.\")\n",
    "\n",
    "# langdetect for JD language\n",
    "try:\n",
    "    from langdetect import detect, DetectorFactory\n",
    "    DetectorFactory.seed = 0\n",
    "    LANGDET_AVAILABLE = True\n",
    "except Exception:\n",
    "    LANGDET_AVAILABLE = False\n",
    "    print(\"langdetect not available — language match will be skipped. pip install langdetect to enable it.\")\n",
    "\n",
    "# helper: load text\n",
    "def load_text(fname):\n",
    "    p = TXT_DIR / fname\n",
    "    return p.read_text(encoding=\"utf-8\", errors=\"ignore\") if p.exists() else \"\"\n",
    "\n",
    "# rough experience extractor: looks for \"X years\" or year ranges like 2016-2021, and counts span\n",
    "YEARS_RANGE_RE = re.compile(r\"(\\b(19|20)\\d{2})\\s*[\\-–—]\\s*(\\b(19|20)\\d{2})\")\n",
    "YEARS_PHRASE_RE = re.compile(r\"(\\d+)\\s+(?:years|yrs)\\b\", re.I)\n",
    "\n",
    "def estimate_experience(text):\n",
    "    # try \"X years\" phrase\n",
    "    m = YEARS_PHRASE_RE.search(text)\n",
    "    if m:\n",
    "        try:\n",
    "            val = int(m.group(1))\n",
    "            return min(val, 40)  # cap at 40\n",
    "        except:\n",
    "            pass\n",
    "    # try year ranges\n",
    "    m = YEARS_RANGE_RE.search(text)\n",
    "    if m:\n",
    "        try:\n",
    "            start = int(m.group(1))\n",
    "            end = int(m.group(3))\n",
    "            span = max(0, end - start)\n",
    "            return min(span, 40)\n",
    "        except:\n",
    "            pass\n",
    "    return 0\n",
    "\n",
    "# helper: check if primary_name is likely a PERSON using spaCy\n",
    "def name_quality(name):\n",
    "    if not name or not SPACY_AVAILABLE:\n",
    "        return 0\n",
    "    doc = nlp(name)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PERSON\":\n",
    "            return 1\n",
    "    # fallback: heuristic — at least two words and letters\n",
    "    if len(name.split()) >= 2 and re.search(r\"[A-Za-z\\u00C0-\\u017F]\", name):\n",
    "        return 0.6\n",
    "    return 0\n",
    "\n",
    "# detect JD language (use English as fallback)\n",
    "JOB_DESC = \"\"\"\n",
    "We are hiring an NLP Engineer with strong Python skills, proven experience with PyTorch and Transformers,\n",
    "solid foundations in NLP methods (tokenization, attention, sequence models), and experience deploying models to production.\n",
    "Experience with ML pipelines, REST APIs, and cloud deployment (AWS/GCP) is a plus.\n",
    "\"\"\"\n",
    "JD_LANG = \"en\"\n",
    "if LANGDET_AVAILABLE:\n",
    "    try:\n",
    "        JD_LANG = detect(JOB_DESC[:2000])\n",
    "    except Exception:\n",
    "        JD_LANG = \"en\"\n",
    "\n",
    "# load embeddings model (for job embedding)\n",
    "MODEL_NAME = \"all-mpnet-base-v2\"\n",
    "print(\"Loading embedding model:\", MODEL_NAME)\n",
    "model = SentenceTransformer(MODEL_NAME)\n",
    "job_emb = model.encode(JOB_DESC)\n",
    "\n",
    "# scoring hyperparams (tweakable)\n",
    "WEIGHT_CONTACT = 8          # smaller contact bonus\n",
    "WEIGHT_SKILL = 5            # per matched skill\n",
    "WEIGHT_SIM = 45             # semantic sim weight\n",
    "WEIGHT_NAME_QUALITY = 6     # boost if name looks like a person\n",
    "WEIGHT_LANG_MATCH = 6       # bonus if resume language == JD language\n",
    "WEIGHT_EXP_PER_YEAR = 1.2   # per estimated year of experience (capped)\n",
    "CAP_EXP = 15\n",
    "CAP_SKILLS = 6\n",
    "\n",
    "# compute all scores\n",
    "results = []\n",
    "for rec in emb_index:\n",
    "    fname = rec[\"file\"]\n",
    "    npy = rec[\"npy\"]\n",
    "    if fname not in file_to_entry:\n",
    "        continue\n",
    "    entry = file_to_entry[fname]\n",
    "    # load emb\n",
    "    try:\n",
    "        emb = np.load(npy)\n",
    "    except Exception as e:\n",
    "        continue\n",
    "    # raw cosine sim\n",
    "    raw_sim = float(cosine_similarity([emb], [job_emb])[0,0])\n",
    "    # normalize sim from [-1,1] to [0,1]\n",
    "    sim_norm = (raw_sim + 1) / 2.0\n",
    "    # contact\n",
    "    contact_bonus = WEIGHT_CONTACT if entry.get(\"contact_status\") == \"found\" else 0\n",
    "    # skills\n",
    "    n_skills = len(entry.get(\"skills\", []))\n",
    "    skill_score = min(CAP_SKILLS, n_skills) * WEIGHT_SKILL\n",
    "    # name quality\n",
    "    nq = name_quality(entry.get(\"primary_name\",\"\"))\n",
    "    name_score = nq * WEIGHT_NAME_QUALITY\n",
    "    # language match\n",
    "    lang = entry.get(\"language\",\"unknown\")\n",
    "    lang_bonus = WEIGHT_LANG_MATCH if (LANGDET_AVAILABLE and lang and JD_LANG and lang == JD_LANG) else 0\n",
    "    # rough experience\n",
    "    text = load_text(fname)\n",
    "    exp_years = estimate_experience(text)\n",
    "    exp_score = min(exp_years, CAP_EXP) * WEIGHT_EXP_PER_YEAR\n",
    "    # final score\n",
    "    sim_score = sim_norm * WEIGHT_SIM\n",
    "    total = contact_bonus + skill_score + sim_score + name_score + lang_bonus + exp_score\n",
    "    results.append({\n",
    "        \"file\": fname,\n",
    "        \"primary_name\": entry.get(\"primary_name\",\"\"),\n",
    "        \"contact_status\": entry.get(\"contact_status\",\"missing\"),\n",
    "        \"n_skills\": n_skills,\n",
    "        \"sim_raw\": round(raw_sim,4),\n",
    "        \"sim_norm\": round(sim_norm,4),\n",
    "        \"name_quality\": round(nq,3),\n",
    "        \"lang\": lang,\n",
    "        \"exp_years\": exp_years,\n",
    "        \"score\": round(total,3),\n",
    "        \"top_skills\": entry.get(\"skills\", [])[:8],\n",
    "        \"preview\": entry.get(\"original_preview\",\"\")[:300].replace(\"\\n\",\"\\\\n\")\n",
    "    })\n",
    "\n",
    "results_sorted = sorted(results, key=lambda x: x[\"score\"], reverse=True)\n",
    "\n",
    "# save results\n",
    "with open(OUT_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results_sorted, f, indent=2)\n",
    "\n",
    "# save CSV\n",
    "with open(OUT_CSV, \"w\", newline='', encoding=\"utf-8\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\"score\",\"sim_raw\",\"sim_norm\",\"name_quality\",\"exp_years\",\"lang\",\"contact_status\",\"n_skills\",\"primary_name\",\"file\",\"top_skills\",\"preview\"])\n",
    "    writer.writeheader()\n",
    "    for r in results_sorted:\n",
    "        writer.writerow({k: r.get(k,\"\") for k in writer.fieldnames})\n",
    "\n",
    "# print top 10\n",
    "print(\"\\nRefined Top 10 resumes:\")\n",
    "for s in results_sorted[:10]:\n",
    "    print(f\"{s['score']:7.3f} | sim={s['sim_raw']:.3f} ({s['sim_norm']:.3f}) | nameQ={s['name_quality']:.2f} | exp={s['exp_years']:2} | contact={s['contact_status']:<7} | skills={s['n_skills']:2} | {s['primary_name'][:40]:40} | {s['file']}\")\n",
    "print(\"\\nSaved:\", OUT_JSON, \"and\", OUT_CSV)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1207b2",
   "metadata": {},
   "source": [
    "### Step 5: Giving summary to users using the score of the resume via Gemini API (To be completed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52f8d5f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Text generation ===\n",
      "An NLP resume parser first extracts text from various document formats (PDF, DOCX). It then uses Natural Language Processing techniques like Named Entity Recognition (NER) and text classification to identify and categorize key information such as skills, experience, education, and contact details. Finally, it structures this extracted data into a machine-readable format, making it easy for Applicant Tracking Systems (ATS) to store, search, and analyze candidate profiles.\n",
      "\n",
      "=== Structured (parsed) output ===\n",
      "{\n",
      "  \"name\": null,\n",
      "  \"email\": \"john.doe@example.com\",\n",
      "  \"phone\": \"+1 (555) 123-4567\",\n",
      "  \"skills\": [\n",
      "    \"Python\",\n",
      "    \"Docker\",\n",
      "    \"SQL\"\n",
      "  ],\n",
      "  \"years_experience\": 6\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abhis\\AppData\\Local\\Temp\\ipykernel_18112\\2897953912.py:70: PydanticDeprecatedSince20: The `json` method is deprecated; use `model_dump_json` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/\n",
      "  print(parsed.json(indent=2))\n"
     ]
    }
   ],
   "source": [
    "# Jupyter cell: Google GenAI (Gemini) quick examples\n",
    "# 1) install (run once in your env / notebook). Uncomment if needed.\n",
    "# !pip install -q -U google-genai pydantic\n",
    "\n",
    "import os\n",
    "from google import genai\n",
    "from pydantic import BaseModel\n",
    "from typing import List, Optional\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # load .env if present\n",
    "\n",
    "# --- AUTH: either set environment variable GEMINI_API_KEY (recommended)\n",
    "# In a terminal / OS:\n",
    "#   export GEMINI_API_KEY=\"your_api_key_here\"      # linux / mac\n",
    "#   setx GEMINI_API_KEY \"your_api_key_here\"       # windows (restart shell)\n",
    "# Or pass the key explicitly to Client(api_key=...)\n",
    "#\n",
    "# The client will pick GEMINI_API_KEY from env automatically:\n",
    "client = genai.Client()  # uses os.environ['GEMINI_API_KEY'] if present\n",
    "\n",
    "# -----------------------\n",
    "# Example A: simple text generation\n",
    "# -----------------------\n",
    "resp = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",               # recommended model from quickstart\n",
    "    contents=\"Explain how an NLP resume parser should work in 3 sentences.\"\n",
    ")\n",
    "print(\"=== Text generation ===\")\n",
    "print(resp.text)   # concise plain text result\n",
    "\n",
    "# -----------------------\n",
    "# Example B: Structured output (JSON) using a Pydantic schema\n",
    "# Useful for extracting structured fields from a resume text.\n",
    "# NOTE: structured output requires `response_mime_type=\"application/json\"`,\n",
    "# and we pass a Pydantic model as response_schema.\n",
    "# -----------------------\n",
    "class ResumeSchema(BaseModel):\n",
    "    name: Optional[str]\n",
    "    email: Optional[str]\n",
    "    phone: Optional[str]\n",
    "    skills: List[str] = []\n",
    "    years_experience: Optional[int]\n",
    "\n",
    "resume_prompt = \"\"\"\n",
    "Extract the main contact information and skills from this resume text.\n",
    "Return only JSON that conforms to the schema: name, email, phone, skills (array of skill strings),\n",
    "years_experience (approximate integer).\n",
    "Resume text:\n",
    "---\n",
    "Software engineer with 6 years experience in backend and cloud, expert in Python, Docker, and SQL.\n",
    "Contact: john.doe@example.com, +1 (555) 123-4567\n",
    "Worked at Acme Corp and Beta Systems.\n",
    "---\n",
    "\"\"\"\n",
    "\n",
    "# call with 'config' specifying response_mime_type and response_schema (Pydantic model)\n",
    "resp_json = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=resume_prompt,\n",
    "    config={\n",
    "        \"response_mime_type\": \"application/json\",   # ask for JSON output\n",
    "        \"response_schema\": ResumeSchema,            # pydantic model to validate/parse\n",
    "    },\n",
    ")\n",
    "\n",
    "print(\"\\n=== Structured (parsed) output ===\")\n",
    "# .parsed (if available) gives the parsed Pydantic instance; else fallback to text\n",
    "try:\n",
    "    parsed = resp_json.parsed  # a ResumeSchema instance (if SDK parsed successfully)\n",
    "    print(parsed.json(indent=2))\n",
    "except Exception:\n",
    "    # fallback: print model text (raw JSON string from model)\n",
    "    print(resp_json.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ff72ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing to summarize 200 resumes with model gemini-2.5-flash...\n",
      "Wrote JSON -> ..\\data\\resumes_preocr\\summaries_gemini.json\n",
      "Wrote CSV  -> ..\\data\\resumes_preocr\\summaries_gemini.csv\n",
      "Completed with errors for 200 resumes. Check JSON 'errors' field.\n"
     ]
    }
   ],
   "source": [
    "# === Gemini structured summarization for scored resumes (ready-to-run) ===\n",
    "# Requirements:\n",
    "# pip install -U google-genai pydantic pandas\n",
    "#\n",
    "# Set env var: GEMINI_API_KEY (or use genai.Client(api_key=\"...\") below)\n",
    "#\n",
    "import os, json, re, time, csv\n",
    "from pathlib import Path\n",
    "from typing import List, Optional\n",
    "from pydantic import BaseModel\n",
    "import google.genai as genai  # updated name in SDK\n",
    "\n",
    "# ---------- Config ----------\n",
    "DATA_DIR = Path(\"../data/resumes_preocr\")\n",
    "SCORES_FILE = DATA_DIR / \"resume_scores_refined_clean.json\"\n",
    "OUT_JSON = DATA_DIR / \"summaries_gemini.json\"\n",
    "OUT_CSV = DATA_DIR / \"summaries_gemini.csv\"\n",
    "MODEL = \"gemini-2.5-flash\"   # change if needed (use model you have access to)\n",
    "BATCH_SIZE = 6               # lower to respect rate limits\n",
    "TOP_N = 200                  # how many resumes to summarize (set smaller to test)\n",
    "MAX_RETRIES = 2\n",
    "BACKOFF_BASE = 1.5\n",
    "\n",
    "# ---------- Pydantic schema for structured output ----------\n",
    "class ResumeSummary(BaseModel):\n",
    "    resume_id: str\n",
    "    name: Optional[str] = None\n",
    "    best_role: Optional[str] = None\n",
    "    years_experience: Optional[int] = None\n",
    "    top_skills: List[str] = []\n",
    "    key_achievements: List[str] = []\n",
    "    education: List[str] = []\n",
    "    contact: dict = {}  # {email:..., phone:..., linkedin:...}\n",
    "    fit_score: Optional[float] = None\n",
    "    summary: Optional[str] = None\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def mask_pii(text: str) -> str:\n",
    "    if not text:\n",
    "        return text\n",
    "    t = re.sub(r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b', '<EMAIL>', text)\n",
    "    t = re.sub(r'\\b(?:\\+?\\d[\\d\\s\\-\\(\\)]{3,}\\d)\\b', '<PHONE>', t)\n",
    "    return t\n",
    "\n",
    "def extract_json_from_text(s: str):\n",
    "    \"\"\"Naive JSON extraction; we prefer SDK's parsed output but keep fallback.\"\"\"\n",
    "    if not s:\n",
    "        return None\n",
    "    try:\n",
    "        return json.loads(s)\n",
    "    except Exception:\n",
    "        start = s.find('{')\n",
    "        if start == -1:\n",
    "            return None\n",
    "        depth = 0\n",
    "        for i in range(start, len(s)):\n",
    "            if s[i] == '{': depth += 1\n",
    "            elif s[i] == '}':\n",
    "                depth -= 1\n",
    "                if depth == 0:\n",
    "                    try:\n",
    "                        return json.loads(s[start:i+1])\n",
    "                    except Exception:\n",
    "                        return None\n",
    "    return None\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are an assistant that reads a compact resume context and returns a strict JSON summary. \"\n",
    "    \"Do not invent facts. If a field is not present, return null or an empty list. Output valid JSON matching the schema exactly.\"\n",
    ")\n",
    "\n",
    "USER_SCHEMA_AND_INSTR = \"\"\"\n",
    "Input fields:\n",
    "- resume_id, name, snippet, skills, orgs, years_experience, contact_status, resume_score\n",
    "\n",
    "Return JSON matching the ResumeSummary schema exactly:\n",
    "{\n",
    " \"resume_id\": \"<same>\",\n",
    " \"name\": \"<string or null>\",\n",
    " \"best_role\": \"<one-line role/title or null>\",\n",
    " \"years_experience\": <int or null>,\n",
    " \"top_skills\": [\"...\"],\n",
    " \"key_achievements\": [\"...\"],\n",
    " \"education\": [\"...\"],\n",
    " \"contact\": {\"email\": null, \"phone\": null, \"linkedin\": null},\n",
    " \"fit_score\": <float 0-100>,\n",
    " \"summary\": \"<2-4 sentence summary>\"\n",
    "}\n",
    "\n",
    "Rules:\n",
    "- Only use facts present in snippet, skills, or orgs. If uncertain, use null/empty.\n",
    "- Fit_score should be consistent with resume_score (you may rescale directly).\n",
    "- summary must be concise (2-4 short sentences), factual and use provided text.\n",
    "- Return JSON only.\n",
    "\"\"\"\n",
    "\n",
    "# ---------- Prepare client ----------\n",
    "API_KEY = os.environ.get(\"GEMINI_API_KEY\") or os.environ.get(\"GOOGLE_API_KEY\")\n",
    "if not API_KEY:\n",
    "    raise RuntimeError(\"Set GEMINI_API_KEY (or GOOGLE_API_KEY) environment variable with your API key.\")\n",
    "client = genai.Client(api_key=API_KEY)\n",
    "\n",
    "# ---------- Load scored resumes ----------\n",
    "with open(SCORES_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    scored = json.load(f)\n",
    "\n",
    "scored = scored[:TOP_N] if TOP_N and len(scored) > TOP_N else scored\n",
    "print(f\"Preparing to summarize {len(scored)} resumes with model {MODEL}...\")\n",
    "\n",
    "summaries = []\n",
    "errors = []\n",
    "\n",
    "# ---------- Loop and call Gemini ----------\n",
    "for idx, rec in enumerate(scored, start=1):\n",
    "    resume_id = rec.get(\"file\")\n",
    "    name = rec.get(\"primary_name\") or rec.get(\"primary_name_clean\") or None\n",
    "    snippet = rec.get(\"preview\", \"\")[:1200]  # short excerpt; adjust length if needed\n",
    "    snippet_masked = mask_pii(snippet)\n",
    "    skills = rec.get(\"top_skills\") or rec.get(\"skills\") or []\n",
    "    orgs = rec.get(\"orgs\") or []\n",
    "    years = rec.get(\"exp_years\") or rec.get(\"exp_years\", None) or None\n",
    "    contact_status = rec.get(\"contact_status\", \"missing\")\n",
    "    resume_score = float(rec.get(\"score\", 0.0))\n",
    "\n",
    "    input_json = {\n",
    "        \"resume_id\": resume_id,\n",
    "        \"name\": name,\n",
    "        \"snippet\": snippet_masked,\n",
    "        \"skills\": skills,\n",
    "        \"orgs\": orgs,\n",
    "        \"years_experience\": years,\n",
    "        \"contact_status\": contact_status,\n",
    "        \"resume_score\": resume_score\n",
    "    }\n",
    "\n",
    "    prompt_text = SYSTEM_PROMPT + \"\\n\\n\" + USER_SCHEMA_AND_INSTR + \"\\n\\n\" + \"Input JSON:\\n\" + json.dumps(input_json, ensure_ascii=False, indent=2)\n",
    "\n",
    "    parsed_obj = None\n",
    "    last_err = None\n",
    "    for attempt in range(MAX_RETRIES + 1):\n",
    "        try:\n",
    "            resp = client.models.generate_content(\n",
    "                model=MODEL,\n",
    "                contents=prompt_text,\n",
    "                config={\n",
    "                    \"response_mime_type\": \"application/json\",\n",
    "                    \"response_schema\": ResumeSummary,  # Pydantic model, SDK will validate/parse\n",
    "                    # You can add other options supported by SDK if needed\n",
    "                },\n",
    "            )\n",
    "            # SDK tries to parse into Pydantic model; access .parsed\n",
    "            parsed = getattr(resp, \"parsed\", None)\n",
    "            if parsed is not None:\n",
    "                # parsed is a Pydantic model instance or list; ensure dict form\n",
    "                if isinstance(parsed, list):\n",
    "                    parsed_obj = parsed[0]\n",
    "                else:\n",
    "                    parsed_obj = parsed\n",
    "            else:\n",
    "                # fallback: attempt to extract JSON from resp.text\n",
    "                raw_text = getattr(resp, \"text\", None) or str(resp)\n",
    "                j = extract_json_from_text(raw_text)\n",
    "                if j is None:\n",
    "                    raise ValueError(\"No JSON parsed by SDK and fallback failed.\")\n",
    "                parsed_obj = ResumeSummary(**j)\n",
    "            # ensure resume_id and fit_score\n",
    "            if not parsed_obj.resume_id:\n",
    "                parsed_obj.resume_id = resume_id\n",
    "            if parsed_obj.fit_score is None:\n",
    "                parsed_obj.fit_score = round(min(100.0, resume_score), 2)\n",
    "            summaries.append(parsed_obj.model_dump())  # convert to plain dict\n",
    "            last_err = None\n",
    "            break\n",
    "        except Exception as e:\n",
    "            last_err = str(e)\n",
    "            time.sleep(BACKOFF_BASE * (2 ** attempt))\n",
    "    if last_err:\n",
    "        errors.append({\"resume_id\": resume_id, \"error\": last_err})\n",
    "        # fallback minimal summary\n",
    "        fallback = ResumeSummary(\n",
    "            resume_id=resume_id,\n",
    "            name=name,\n",
    "            best_role=None,\n",
    "            years_experience=years,\n",
    "            top_skills=skills[:6],\n",
    "            key_achievements=[],\n",
    "            education=[],\n",
    "            contact={\"email\": None, \"phone\": None, \"linkedin\": None},\n",
    "            fit_score=round(min(100.0, resume_score),2),\n",
    "            summary=f\"Auto-fallback summary (resume_score={resume_score})\"\n",
    "        )\n",
    "        summaries.append(fallback.model_dump())\n",
    "\n",
    "    # polite pacing\n",
    "    if idx % BATCH_SIZE == 0:\n",
    "        time.sleep(1.0)\n",
    "\n",
    "# ---------- Save outputs ----------\n",
    "with open(OUT_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"summaries\": summaries, \"errors\": errors}, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# CSV quick summary\n",
    "csv_fields = [\"resume_id\",\"name\",\"best_role\",\"years_experience\",\"top_skills\",\"fit_score\",\"summary\"]\n",
    "with open(OUT_CSV, \"w\", newline='', encoding=\"utf-8\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=csv_fields)\n",
    "    writer.writeheader()\n",
    "    for s in summaries:\n",
    "        writer.writerow({\n",
    "            \"resume_id\": s.get(\"resume_id\"),\n",
    "            \"name\": s.get(\"name\"),\n",
    "            \"best_role\": s.get(\"best_role\"),\n",
    "            \"years_experience\": s.get(\"years_experience\"),\n",
    "            \"top_skills\": \",\".join(s.get(\"top_skills\") or []),\n",
    "            \"fit_score\": s.get(\"fit_score\"),\n",
    "            \"summary\": (s.get(\"summary\") or \"\")[:300]\n",
    "        })\n",
    "\n",
    "print(\"Wrote JSON ->\", OUT_JSON)\n",
    "print(\"Wrote CSV  ->\", OUT_CSV)\n",
    "if errors:\n",
    "    print(\"Completed with errors for\", len(errors), \"resumes. Check JSON 'errors' field.\")\n",
    "else:\n",
    "    print(\"All summaries produced successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8122e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed count: 200\n",
      "Batches to process: 67 (batch size 3)\n",
      "Batch 1/67 done. Saved 3 outputs (some may be None).\n",
      "Batch 2/67 done. Saved 3 outputs (some may be None).\n",
      "Batch 3/67 done. Saved 3 outputs (some may be None).\n",
      "Batch 4/67 done. Saved 3 outputs (some may be None).\n",
      "Batch 5/67 done. Saved 3 outputs (some may be None).\n",
      "Batch 6/67 done. Saved 3 outputs (some may be None).\n",
      "Batch 7/67 done. Saved 3 outputs (some may be None).\n",
      "Batch 8/67 done. Saved 3 outputs (some may be None).\n",
      "Batch 9/67 done. Saved 3 outputs (some may be None).\n",
      "Batch 10/67 done. Saved 3 outputs (some may be None).\n",
      "Batch 11/67 done. Saved 3 outputs (some may be None).\n",
      "Batch 12/67 done. Saved 3 outputs (some may be None).\n",
      "Batch 13/67 done. Saved 3 outputs (some may be None).\n",
      "Batch 14/67 done. Saved 3 outputs (some may be None).\n",
      "Batch 15/67 done. Saved 3 outputs (some may be None).\n",
      "Batch 16/67 done. Saved 3 outputs (some may be None).\n",
      " Batch 16 attempt 0 failed: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 15, model: gemini-2.5-flash-lite\\nPlease retry in 29.616815167s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash-lite'}, 'quotaValue': '15'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '29s'}]}} — sleeping 1.5s\n",
      " Batch 16 attempt 1 failed: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 15, model: gemini-2.5-flash-lite\\nPlease retry in 27.952234747s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash-lite'}, 'quotaValue': '15'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '27s'}]}} — sleeping 3.0s\n",
      " Batch 16 attempt 2 failed: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 15, model: gemini-2.5-flash-lite\\nPlease retry in 24.772572574s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash-lite'}, 'quotaValue': '15'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '24s'}]}} — sleeping 6.0s\n",
      "Batch 16 failed after retries: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 15, model: gemini-2.5-flash-lite\\nPlease retry in 24.772572574s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash-lite'}, 'quotaValue': '15'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '24s'}]}}. Saving raw output for debug.\n",
      "Batch 17/67 done. Saved 3 outputs (some may be None).\n",
      " Batch 17 attempt 0 failed: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 15, model: gemini-2.5-flash-lite\\nPlease retry in 17.152475083s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash-lite'}, 'quotaValue': '15'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '17s'}]}} — sleeping 1.5s\n",
      " Batch 17 attempt 1 failed: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 15, model: gemini-2.5-flash-lite\\nPlease retry in 15.485553617s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash-lite'}, 'quotaValue': '15'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '15s'}]}} — sleeping 3.0s\n",
      "Batch 18/67 done. Saved 3 outputs (some may be None).\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 329\u001b[39m\n\u001b[32m    326\u001b[39m                 outputs_by_id[inp[\u001b[33m\"\u001b[39m\u001b[33mresume_id\u001b[39m\u001b[33m\"\u001b[39m]] = out\n\u001b[32m    328\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBatch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mb+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatches_to_run\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m done. Saved \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(batch_inputs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m outputs (some may be None).\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m329\u001b[39m     time.sleep(SLEEP_BETWEEN_BATCHES)\n\u001b[32m    331\u001b[39m \u001b[38;5;66;03m# Merge results & save\u001b[39;00m\n\u001b[32m    332\u001b[39m merged = []\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Full re-run cell: batch retry using gemini-2.5-flash-lite + strict prompt + normalization\n",
    "# Paste into notebook and run. Set TEST_ONLY=False to process all batches.\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import math\n",
    "from pathlib import Path\n",
    "from typing import List, Optional\n",
    "import google.genai as genai\n",
    "import html\n",
    "\n",
    "# ---------- Config ----------\n",
    "DATA_DIR = Path(\"../data/resumes_preocr\")\n",
    "IN_SUMMARIES = DATA_DIR / \"summaries_gemini.json\"       # previous output with fallbacks\n",
    "SCORES_FILE = DATA_DIR / \"resume_scores_refined_clean.json\"\n",
    "OUT_RETRY = DATA_DIR / \"summaries_gemini_rerun.json\"\n",
    "RAW_DIR = DATA_DIR / \"gemini_rerun_raw\"\n",
    "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Model & throttling\n",
    "MODEL = \"gemini-2.5-flash-lite\"   # <- using the working model you reported\n",
    "BATCH_SIZE = 3                   # small to be safe\n",
    "TEST_ONLY = False                 # set False to process all batches\n",
    "MAX_BATCH_RETRIES = 2\n",
    "SLEEP_BETWEEN_BATCHES = 1.0\n",
    "SNIPPET_MAX = 120                # short snippet to save tokens\n",
    "MAX_OUTPUT_TOKENS = 900\n",
    "\n",
    "# Summary / skill limits\n",
    "MAX_SUMMARY_WORDS = 35\n",
    "MAX_TOP_SKILLS = 6\n",
    "\n",
    "# API key\n",
    "API_KEY = os.environ.get(\"GEMINI_API_KEY\") or os.environ.get(\"GOOGLE_API_KEY\")\n",
    "if not API_KEY:\n",
    "    raise RuntimeError(\"Set GEMINI_API_KEY or GOOGLE_API_KEY in environment.\")\n",
    "client = genai.Client(api_key=API_KEY)\n",
    "\n",
    "# ---------- Helper functions ----------\n",
    "def flatten_json_array_text(text: str) -> Optional[list]:\n",
    "    if not text:\n",
    "        return None\n",
    "    # direct parse\n",
    "    try:\n",
    "        obj = json.loads(text)\n",
    "        if isinstance(obj, list):\n",
    "            return obj\n",
    "    except Exception:\n",
    "        pass\n",
    "    # find [ ... ] region\n",
    "    a = text.find('[')\n",
    "    if a != -1:\n",
    "        depth = 0\n",
    "        for i in range(a, len(text)):\n",
    "            ch = text[i]\n",
    "            if ch == '[':\n",
    "                depth += 1\n",
    "            elif ch == ']':\n",
    "                depth -= 1\n",
    "                if depth == 0:\n",
    "                    candidate = text[a:i+1]\n",
    "                    candidate = re.sub(r',\\s*]', ']', candidate)\n",
    "                    candidate = re.sub(r',\\s*}', '}', candidate)\n",
    "                    try:\n",
    "                        parsed = json.loads(candidate)\n",
    "                        if isinstance(parsed, list):\n",
    "                            return parsed\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                    break\n",
    "    # fallback: grab {...} objects\n",
    "    objs = []\n",
    "    start = text.find('{')\n",
    "    while start != -1:\n",
    "        depth = 0; end = None\n",
    "        for i in range(start, len(text)):\n",
    "            if text[i] == '{':\n",
    "                depth += 1\n",
    "            elif text[i] == '}':\n",
    "                depth -= 1\n",
    "                if depth == 0:\n",
    "                    end = i; break\n",
    "        if end is None:\n",
    "            break\n",
    "        candidate = text[start:end+1]\n",
    "        cand2 = re.sub(r',\\s*}', '}', candidate).replace(\"'\", '\"')\n",
    "        try:\n",
    "            parsed = json.loads(cand2)\n",
    "            objs.append(parsed)\n",
    "        except Exception:\n",
    "            pass\n",
    "        start = text.find('{', end+1)\n",
    "    return objs if objs else None\n",
    "\n",
    "def extract_raw_text_from_resp(resp) -> str:\n",
    "    raw = getattr(resp, \"text\", None)\n",
    "    if raw:\n",
    "        return raw\n",
    "    # try candidates\n",
    "    try:\n",
    "        cand_list = getattr(resp, \"candidates\", None)\n",
    "        if cand_list and len(cand_list) > 0:\n",
    "            first = cand_list[0]\n",
    "            content = getattr(first, \"content\", None)\n",
    "            if isinstance(content, dict):\n",
    "                if \"text\" in content and content[\"text\"]:\n",
    "                    return content[\"text\"]\n",
    "                if \"parts\" in content and isinstance(content[\"parts\"], list):\n",
    "                    return \"\".join(content[\"parts\"])\n",
    "                return json.dumps(content)\n",
    "            txt = getattr(content, \"text\", None)\n",
    "            if txt:\n",
    "                return txt\n",
    "            parts = getattr(content, \"parts\", None)\n",
    "            if parts:\n",
    "                return \"\".join(parts)\n",
    "            t = getattr(first, \"text\", None)\n",
    "            if t:\n",
    "                return t\n",
    "            return repr(first)\n",
    "    except Exception:\n",
    "        pass\n",
    "    # older style\n",
    "    try:\n",
    "        choices = getattr(resp, \"choices\", None)\n",
    "        if choices and len(choices) > 0:\n",
    "            ch = choices[0]\n",
    "            if isinstance(ch, dict) and \"text\" in ch:\n",
    "                return ch[\"text\"]\n",
    "            return getattr(ch, \"text\", None) or repr(ch)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return repr(resp)\n",
    "\n",
    "def clean_and_shorten_snippet(raw: str, max_len: int = SNIPPET_MAX) -> str:\n",
    "    if not raw:\n",
    "        return \"\"\n",
    "    s = re.sub(r'[\\x00-\\x1f\\x7f-\\x9f]', ' ', raw)\n",
    "    s = re.sub(r'\\s+', ' ', s).strip()\n",
    "    s = re.sub(r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b', '<EMAIL>', s)\n",
    "    s = re.sub(r'\\b(?:\\+?\\d[\\d\\s\\-\\(\\)]{3,}\\d)\\b', '<PHONE>', s)\n",
    "    return s[:max_len]\n",
    "\n",
    "def clean_summary_text(raw_summary: str, max_words: int = MAX_SUMMARY_WORDS) -> Optional[str]:\n",
    "    if raw_summary is None:\n",
    "        return None\n",
    "    s = str(raw_summary)\n",
    "    s = re.sub(r'(?i)\\b(resum[eé]|cv|personal data|personal details|contact|contact info|email|mobile|phone)\\b[:\\-\\s]*', ' ', s)\n",
    "    s = re.sub(r'[\\r\\n]+', ' ', s)\n",
    "    s = re.sub(r'\\s+', ' ', s).strip()\n",
    "    s = html.unescape(s)\n",
    "    words = s.split()\n",
    "    if len(words) <= max_words:\n",
    "        return s\n",
    "    return \" \".join(words[:max_words]).rstrip(' ,;:') + \"...\"\n",
    "\n",
    "def normalize_model_output(obj: dict) -> dict:\n",
    "    out = dict(obj)  # copy\n",
    "    # fit_score\n",
    "    if out.get(\"fit_score\") is None:\n",
    "        out[\"fit_score\"] = 0.0\n",
    "    try:\n",
    "        out[\"fit_score\"] = float(out[\"fit_score\"])\n",
    "    except Exception:\n",
    "        out[\"fit_score\"] = 0.0\n",
    "    out[\"fit_score\"] = max(0.0, min(100.0, out[\"fit_score\"]))\n",
    "\n",
    "    # top_skills: unique and trimmed\n",
    "    skills = out.get(\"top_skills\") or []\n",
    "    if isinstance(skills, list):\n",
    "        cleaned = []\n",
    "        seen = set()\n",
    "        for s in skills:\n",
    "            if not isinstance(s, str): continue\n",
    "            tok = s.strip()\n",
    "            if not tok: continue\n",
    "            low = tok.lower()\n",
    "            if low in seen: continue\n",
    "            seen.add(low)\n",
    "            cleaned.append(tok)\n",
    "        out[\"top_skills\"] = cleaned[:MAX_TOP_SKILLS]\n",
    "    else:\n",
    "        out[\"top_skills\"] = []\n",
    "\n",
    "    # summary cleaning/truncation\n",
    "    out[\"summary\"] = clean_summary_text(out.get(\"summary\"), max_words=MAX_SUMMARY_WORDS)\n",
    "\n",
    "    # ensure keys exist\n",
    "    for k in [\"resume_id\",\"name\",\"best_role\",\"years_experience\",\"key_achievements\",\"education\",\"contact\"]:\n",
    "        out.setdefault(k, None)\n",
    "    # ensure lists for achievements/education\n",
    "    if not isinstance(out.get(\"key_achievements\"), list):\n",
    "        out[\"key_achievements\"] = []\n",
    "    if not isinstance(out.get(\"education\"), list):\n",
    "        out[\"education\"] = []\n",
    "    # ensure contact has fields\n",
    "    contact = out.get(\"contact\") or {}\n",
    "    if not isinstance(contact, dict):\n",
    "        contact = {}\n",
    "    contact.setdefault(\"email\", None); contact.setdefault(\"phone\", None); contact.setdefault(\"linkedin\", None)\n",
    "    out[\"contact\"] = contact\n",
    "\n",
    "    return out\n",
    "\n",
    "# ---------- Strict prompt builder ----------\n",
    "def build_batch_prompt(inputs: List[dict]) -> str:\n",
    "    example_input = {\"resume_id\":\"sample_EX.txt\",\"name\":\"Jane Doe\",\"snippet\":\"Backend engineer: Python, SQL.\",\"skills\":[\"python\",\"sql\"],\"years_experience\":6,\"resume_score\":78.3}\n",
    "    example_output = {\n",
    "        \"resume_id\":\"sample_EX.txt\",\"name\":\"Jane Doe\",\"best_role\":\"Backend Engineer\",\"years_experience\":6,\n",
    "        \"top_skills\":[\"python\",\"sql\"],\"key_achievements\":[\"Built ETL pipeline\"],\"education\":[\"B.Sc.\"],\n",
    "        \"contact\":{\"email\":None,\"phone\":None,\"linkedin\":None},\"fit_score\":78.3,\n",
    "        \"summary\":\"Backend engineer with 6 years experience in Python and SQL; built production ETL pipeline.\"\n",
    "    }\n",
    "    error_fallback = (\n",
    "        '{\"resume_id\":\"<same id>\",\"error\":\"cannot_parse\",\"best_role\":null,\"years_experience\":null,'\n",
    "        '\"top_skills\":[],\"key_achievements\":[],\"education\":[],\"contact\":{\"email\":null,\"phone\":null,\"linkedin\":null},'\n",
    "        '\"fit_score\":0.0,\"summary\":null}'\n",
    "    )\n",
    "    instr = (\n",
    "        \"RETURN JSON ARRAY ONLY. No commentary. Do NOT think out loud.\\n\"\n",
    "        \"Schema (each element must contain exactly these keys):\\n\"\n",
    "        \"resume_id,name,best_role,years_experience,top_skills,key_achievements,education,contact,fit_score,summary\\n\\n\"\n",
    "        f\"Constraints:\\n- 'summary' must be a concise single-sentence paragraph (<= {MAX_SUMMARY_WORDS} words).\\n\"\n",
    "        f\"- 'top_skills' max {MAX_TOP_SKILLS} items.\\n\"\n",
    "        \"- Use null or empty list when unknown.\\n\"\n",
    "        \"- Do NOT include resume headings or raw labeled lines; summarize.\\n\\n\"\n",
    "        \"Compact example input and its exact output (style guide):\\n\"\n",
    "        + json.dumps(example_input, ensure_ascii=False) + \"\\n\"\n",
    "        + json.dumps(example_output, ensure_ascii=False) + \"\\n\\n\"\n",
    "        \"If you cannot produce an entry, return this fallback object for that item:\\n\"\n",
    "        + error_fallback + \"\\n\\n\"\n",
    "        \"Now process this list of inputs and return a JSON array where the i-th element is the i-th input's output.\\n\\n\"\n",
    "        \"Inputs:\\n\" + json.dumps(inputs, ensure_ascii=False)\n",
    "    )\n",
    "    return instr\n",
    "\n",
    "# ---------- Load previous summaries & failed set ----------\n",
    "with open(IN_SUMMARIES, \"r\", encoding=\"utf-8\") as f:\n",
    "    prev = json.load(f)\n",
    "summaries = prev.get(\"summaries\", prev) if isinstance(prev, dict) else prev\n",
    "\n",
    "failed = [s for s in summaries if isinstance(s.get(\"summary\",\"\"), str) and s[\"summary\"].startswith(\"Auto-fallback summary\")]\n",
    "print(\"Failed count:\", len(failed))\n",
    "if not failed:\n",
    "    print(\"No failed items to retry. Exiting.\")\n",
    "else:\n",
    "    # load scored info for richer inputs\n",
    "    all_scored = {}\n",
    "    if SCORES_FILE.exists():\n",
    "        sf = json.load(open(SCORES_FILE, \"r\", encoding=\"utf-8\"))\n",
    "        all_scored = {r.get(\"file\"): r for r in sf}\n",
    "\n",
    "    inputs = []\n",
    "    for fentry in failed:\n",
    "        rid = fentry[\"resume_id\"]\n",
    "        scored_rec = all_scored.get(rid, {})\n",
    "        raw_snip = (scored_rec.get(\"preview\") or fentry.get(\"summary\") or \"\")[:2000]\n",
    "        snippet = clean_and_shorten_snippet(raw_snip, max_len=SNIPPET_MAX)\n",
    "        inp = {\n",
    "            \"resume_id\": rid,\n",
    "            \"name\": (scored_rec.get(\"primary_name\") or fentry.get(\"name\") or \"\")[:100],\n",
    "            \"snippet\": snippet,\n",
    "            \"skills\": scored_rec.get(\"top_skills\") or fentry.get(\"top_skills\") or [],\n",
    "            \"years_experience\": scored_rec.get(\"exp_years\") or fentry.get(\"years_experience\") or None,\n",
    "            \"resume_score\": float(scored_rec.get(\"score\") or fentry.get(\"fit_score\") or 0.0)\n",
    "        }\n",
    "        inputs.append(inp)\n",
    "\n",
    "    num_batches = math.ceil(len(inputs) / BATCH_SIZE)\n",
    "    print(f\"Batches to process: {num_batches} (batch size {BATCH_SIZE})\")\n",
    "    outputs_by_id = {}\n",
    "    raw_logs = {}\n",
    "\n",
    "    batches_to_run = 1 if TEST_ONLY else num_batches\n",
    "    for b in range(batches_to_run):\n",
    "        start_i = b * BATCH_SIZE\n",
    "        batch_inputs = inputs[start_i:start_i + BATCH_SIZE]\n",
    "        if not batch_inputs:\n",
    "            continue\n",
    "        prompt = build_batch_prompt(batch_inputs)\n",
    "\n",
    "        parsed_list = None\n",
    "        last_err = None\n",
    "        for attempt in range(MAX_BATCH_RETRIES + 1):\n",
    "            try:\n",
    "                resp = client.models.generate_content(\n",
    "                    model=MODEL,\n",
    "                    contents=prompt,\n",
    "                    config={\"max_output_tokens\": MAX_OUTPUT_TOKENS, \"temperature\": 0.0}\n",
    "                )\n",
    "                raw_text = extract_raw_text_from_resp(resp)\n",
    "                raw_logs[f\"batch_{b}\"] = raw_text\n",
    "                raw_logs[f\"batch_{b}_repr\"] = repr(resp)[:2000]\n",
    "                parsed_list = flatten_json_array_text(raw_text)\n",
    "                if parsed_list is None:\n",
    "                    raise ValueError(\"No JSON array or objects parsed from model output.\")\n",
    "                # normalize parsed outputs\n",
    "                parsed_list = [normalize_model_output(o) if isinstance(o, dict) else o for o in parsed_list]\n",
    "                break\n",
    "            except Exception as e:\n",
    "                last_err = str(e)\n",
    "                wait = 1.5 * (2 ** attempt)\n",
    "                print(f\" Batch {b} attempt {attempt} failed: {last_err} — sleeping {wait}s\")\n",
    "                time.sleep(wait)\n",
    "        if parsed_list is None:\n",
    "            print(f\"Batch {b} failed after retries: {last_err}. Saving raw output for debug.\")\n",
    "            raw_logs[f\"batch_{b}_error\"] = last_err\n",
    "            for inp in batch_inputs:\n",
    "                outputs_by_id[inp[\"resume_id\"]] = None\n",
    "        else:\n",
    "            # map outputs to resume_ids robustly\n",
    "            if len(parsed_list) != len(batch_inputs):\n",
    "                for obj in parsed_list:\n",
    "                    rid = obj.get(\"resume_id\")\n",
    "                    if rid:\n",
    "                        outputs_by_id[rid] = obj\n",
    "                for i, inp in enumerate(batch_inputs):\n",
    "                    if inp[\"resume_id\"] not in outputs_by_id:\n",
    "                        if i < len(parsed_list) and isinstance(parsed_list[i], dict):\n",
    "                            outputs_by_id[inp[\"resume_id\"]] = parsed_list[i]\n",
    "                        else:\n",
    "                            outputs_by_id[inp[\"resume_id\"]] = None\n",
    "            else:\n",
    "                for inp, out in zip(batch_inputs, parsed_list):\n",
    "                    outputs_by_id[inp[\"resume_id\"]] = out\n",
    "\n",
    "        print(f\"Batch {b+1}/{batches_to_run} done. Saved {len(batch_inputs)} outputs (some may be None).\")\n",
    "        time.sleep(SLEEP_BETWEEN_BATCHES)\n",
    "\n",
    "    # Merge results & save\n",
    "    merged = []\n",
    "    for s in summaries:\n",
    "        rid = s.get(\"resume_id\")\n",
    "        if rid in outputs_by_id and outputs_by_id[rid]:\n",
    "            merged.append(outputs_by_id[rid])\n",
    "        else:\n",
    "            merged.append(s)\n",
    "\n",
    "    with open(OUT_RETRY, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\"summaries\": merged}, f, ensure_ascii=False, indent=2)\n",
    "    with open(RAW_DIR / \"batch_raw_logs.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(raw_logs, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(\"Batch re-run finished. Results saved to:\", OUT_RETRY)\n",
    "    print(\"Raw logs saved to:\", RAW_DIR / \"batch_raw_logs.json\")\n",
    "    if TEST_ONLY:\n",
    "        print(\"TEST_ONLY=True so only first batch was processed. Set TEST_ONLY=False to process all batches.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00e9a868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved reviews JSON: ..\\data\\resumes_preocr\\final\\reviews\\resume_reviews.json\n",
      "Saved reviews CSV : ..\\data\\resumes_preocr\\final\\reviews\\resume_reviews_flat.csv\n",
      "Saved HTML preview  : ..\\data\\resumes_preocr\\final\\reviews\\reviews_preview.html\n",
      "\n",
      "Sample review (first item):\n",
      "{'fit_score': 100.0,\n",
      " 'gaps': ['Missing contact details (email/phone/linkedin).',\n",
      "          'No measurable achievements detected.',\n",
      "          'Seniority signals (years) present but lacking leadership/impact '\n",
      "          'bullets.'],\n",
      " 'name': 'LE HOANG',\n",
      " 'resume_id': 'sample_00419.txt',\n",
      " 'short_review': 'Strengths: Clear technical keywords: c, c#, c++, '\n",
      "                 'communication, git, marketing. 25 years experience — '\n",
      "                 'highlights seniority. Gaps: Missing contact details '\n",
      "                 '(email/phone/linkedin). No measurable achievements detected. '\n",
      "                 'Seniority signals (years) present but lacking '\n",
      "                 'leadership/impact bullets. Suggestions: Add at least an '\n",
      "                 'email and one contact method (phone or LinkedIn) in the '\n",
      "                 'header. Avoid putting PII only as images; provide selectable '\n",
      "                 'text so ATS can parse it. Replace generic bullets with '\n",
      "                 \"quantified achievements (e.g., 'Reduced query time by 40%'). \"\n",
      "                 'Use metrics, timeframe and scope. Add your highest degree or '\n",
      "                 'relevant certifications (degree, institution, year). For '\n",
      "                 'senior profiles, emphasize leadership, scope and measurable '\n",
      "                 'impact (teams led, revenue/population served).',\n",
      " 'strengths': ['Clear technical keywords: c, c#, c++, communication, git, '\n",
      "               'marketing.',\n",
      "               '25 years experience — highlights seniority.'],\n",
      " 'suggestions': ['Add at least an email and one contact method (phone or '\n",
      "                 'LinkedIn) in the header. Avoid putting PII only as images; '\n",
      "                 'provide selectable text so ATS can parse it.',\n",
      "                 'Replace generic bullets with quantified achievements (e.g., '\n",
      "                 \"'Reduced query time by 40%'). Use metrics, timeframe and \"\n",
      "                 'scope.',\n",
      "                 'Add your highest degree or relevant certifications (degree, '\n",
      "                 'institution, year).',\n",
      "                 'For senior profiles, emphasize leadership, scope and '\n",
      "                 'measurable impact (teams led, revenue/population served).'],\n",
      " 'summary': 'Experienced professional with 25 years of experience in C, C#, '\n",
      "            'C++, communication, Git, and marketing.',\n",
      " 'top_skills': ['c', 'c#', 'c++', 'communication', 'git', 'marketing'],\n",
      " 'years_experience': 25}\n"
     ]
    }
   ],
   "source": [
    "# Resume feedback generator — produces strengths, gaps, and actionable suggestions\n",
    "# Run in the same environment as the final summaries cell output.\n",
    "import json, re, math, os\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "import pandas as pd\n",
    "import html\n",
    "\n",
    "# ---------- Config ----------\n",
    "DATA_DIR = Path(\"../data/resumes_preocr\")\n",
    "FINAL_JSON = DATA_DIR / \"final\" / \"final_summaries.json\"\n",
    "OUT_DIR = DATA_DIR / \"final\" / \"reviews\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "OUT_JSON = OUT_DIR / \"resume_reviews.json\"\n",
    "OUT_CSV = OUT_DIR / \"resume_reviews_flat.csv\"\n",
    "OUT_HTML = OUT_DIR / \"reviews_preview.html\"\n",
    "\n",
    "# thresholds & heuristics (tuneable)\n",
    "MIN_TOP_SKILLS = 4           # fewer -> suggest add more skills\n",
    "MIN_SUMMARY_WORDS = 6        # very short summary -> suggest expand\n",
    "MIN_EXPERIENCE_FOR_SENIOR = 5  # years to consider \"senior\"\n",
    "ACHIEVEMENTS_MIN = 1\n",
    "SUMMARY_WORDS_GOOD = 20\n",
    "\n",
    "# helper: safe load\n",
    "def load_final_summaries(path: Path) -> List[Dict[str,Any]]:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Expected final summaries at {path} (run finalization cell first).\")\n",
    "    data = json.load(open(path, \"r\", encoding=\"utf-8\"))\n",
    "    items = data.get(\"summaries\", data if isinstance(data, list) else [])\n",
    "    return [i for i in items if isinstance(i, dict)]\n",
    "\n",
    "# text helpers\n",
    "def count_words(s):\n",
    "    return 0 if not s else len(re.findall(r\"\\w+\", str(s)))\n",
    "\n",
    "def has_contact(contact):\n",
    "    if not contact: return False\n",
    "    if not isinstance(contact, dict): return False\n",
    "    return bool(contact.get(\"email\") or contact.get(\"phone\") or contact.get(\"linkedin\"))\n",
    "\n",
    "# rule-based feedback generator\n",
    "def generate_feedback(item: Dict[str,Any]) -> Dict[str,Any]:\n",
    "    \"\"\"\n",
    "    Produces a review dict:\n",
    "      - strengths: list[str]\n",
    "      - gaps: list[str]\n",
    "      - suggestions: list[str]\n",
    "      - short_review: one-paragraph friendly summary\n",
    "    \"\"\"\n",
    "    name = item.get(\"name\") or item.get(\"resume_id\")\n",
    "    skills = item.get(\"top_skills\") or []\n",
    "    summary = (item.get(\"summary\") or \"\").strip()\n",
    "    achievements = item.get(\"key_achievements\") or []\n",
    "    education = item.get(\"education\") or []\n",
    "    years = item.get(\"years_experience\")\n",
    "    fit_score = item.get(\"fit_score\") or 0.0\n",
    "    contact = item.get(\"contact\") or {}\n",
    "\n",
    "    strengths = []\n",
    "    gaps = []\n",
    "    suggestions = []\n",
    "\n",
    "    # Strengths (positive signals)\n",
    "    if skills:\n",
    "        strengths.append(f\"Clear technical keywords: {', '.join(skills[:6])}.\")\n",
    "    if summary and count_words(summary) >= SUMMARY_WORDS_GOOD:\n",
    "        strengths.append(\"Concise summary present (good quick overview).\")\n",
    "    if years and isinstance(years, (int,float)) and years >= MIN_EXPERIENCE_FOR_SENIOR:\n",
    "        strengths.append(f\"{int(years)} years experience — highlights seniority.\")\n",
    "    if achievements and len(achievements) >= ACHIEVEMENTS_MIN:\n",
    "        strengths.append(f\"Has {len(achievements)} achievement(s) listed (good).\")\n",
    "    if education:\n",
    "        strengths.append(\"Education entries present.\")\n",
    "\n",
    "    # Gaps & suggestions\n",
    "    # 1. Contact\n",
    "    if not has_contact(contact):\n",
    "        gaps.append(\"Missing contact details (email/phone/linkedin).\")\n",
    "        suggestions.append(\"Add at least an email and one contact method (phone or LinkedIn) in the header. \"\n",
    "                           \"Avoid putting PII only as images; provide selectable text so ATS can parse it.\")\n",
    "    else:\n",
    "        # if contact exists but empty fields\n",
    "        if not contact.get(\"email\"):\n",
    "            suggestions.append(\"Add a professional email (important for recruiters).\")\n",
    "    # 2. Skills low count\n",
    "    if len(skills) < MIN_TOP_SKILLS:\n",
    "        gaps.append(f\"Low number of extracted skills ({len(skills)}).\")\n",
    "        suggestions.append(\"List 5–12 focused skills in a 'Skills' section, prioritized by relevance to the role.\")\n",
    "    # 3. Achievements missing / weak\n",
    "    if not achievements or len(achievements) < ACHIEVEMENTS_MIN:\n",
    "        gaps.append(\"No measurable achievements detected.\")\n",
    "        suggestions.append(\"Replace generic bullets with quantified achievements (e.g., 'Reduced query time by 40%'). \"\n",
    "                           \"Use metrics, timeframe and scope.\")\n",
    "    # 4. Education missing or unclear\n",
    "    if not education:\n",
    "        # don't insist — but mention\n",
    "        suggestions.append(\"Add your highest degree or relevant certifications (degree, institution, year).\")\n",
    "    # 5. Summary too short or too verbose\n",
    "    sw = count_words(summary)\n",
    "    if sw == 0:\n",
    "        gaps.append(\"No summary detected.\")\n",
    "        suggestions.append(\"Add a 1–2 sentence professional summary at the top describing role, years, and core strengths.\")\n",
    "    elif sw < MIN_SUMMARY_WORDS:\n",
    "        gaps.append(\"Summary is too short.\")\n",
    "        suggestions.append(\"Expand the summary to include 2–3 concrete strengths or focus areas (technologies, domain, impact).\")\n",
    "    # 6. Seniority mismatch (years vs skills/achievements)\n",
    "    if isinstance(years, (int,float)) and years >= MIN_EXPERIENCE_FOR_SENIOR:\n",
    "        # expect achievements & leadership skills\n",
    "        leadership_keywords = {\"lead\", \"manage\", \"mentorship\", \"team\", \"senior\", \"architect\"}\n",
    "        has_lead = any(kw for kw in skills if kw and any(lk in kw.lower() for lk in leadership_keywords))\n",
    "        if (not achievements or len(achievements) < 2) and not has_lead:\n",
    "            gaps.append(\"Seniority signals (years) present but lacking leadership/impact bullets.\")\n",
    "            suggestions.append(\"For senior profiles, emphasize leadership, scope and measurable impact (teams led, revenue/population served).\")\n",
    "    # 7. Skill/summary mismatch\n",
    "    # If summary mentions only general words and skills are concrete -> suggest alignment\n",
    "    if summary and skills:\n",
    "        # check overlap approximate\n",
    "        summary_lower = summary.lower()\n",
    "        common = sum(1 for s in skills if s.lower() in summary_lower)\n",
    "        if common == 0:\n",
    "            suggestions.append(\"Align summary with skills: mention 2–3 top technologies from the Skills section in the summary.\")\n",
    "    # 8. Readability / formatting hints\n",
    "    # heuristics: very long summaries or very long resumes (we have summary only)\n",
    "    if sw > 80:\n",
    "        suggestions.append(\"Summary is long — keep it to one sentence (<=~35 words) to improve skim-ability.\")\n",
    "    # 9. Language detection hint (if summary has non-ASCII heavy content)\n",
    "    if summary and len(re.findall(r'[^\\x00-\\x7f]', summary)) > 5:\n",
    "        suggestions.append(\"Resume contains non-ASCII characters (likely not-English). If applying to English roles, provide an English version.\")\n",
    "\n",
    "    # Generate a short friendly paragraph review (rule-based composition)\n",
    "    short_lines = []\n",
    "    if strengths:\n",
    "        short_lines.append(\"Strengths: \" + \" \".join(strengths))\n",
    "    if gaps:\n",
    "        short_lines.append(\"Gaps: \" + \" \".join(gaps))\n",
    "    if suggestions:\n",
    "        short_lines.append(\"Suggestions: \" + \" \".join(suggestions[:5]))  # keep brief\n",
    "    short_review = \" \".join(short_lines) if short_lines else \"No specific feedback — seems fine.\"\n",
    "\n",
    "    return {\n",
    "        \"resume_id\": item.get(\"resume_id\"),\n",
    "        \"name\": name,\n",
    "        \"fit_score\": fit_score,\n",
    "        \"years_experience\": years,\n",
    "        \"top_skills\": skills,\n",
    "        \"summary\": summary,\n",
    "        \"strengths\": strengths,\n",
    "        \"gaps\": gaps,\n",
    "        \"suggestions\": suggestions,\n",
    "        \"short_review\": short_review\n",
    "    }\n",
    "\n",
    "# ---------- Run generator on all final summaries ----------\n",
    "finals = load_final_summaries(FINAL_JSON)\n",
    "reviews = [generate_feedback(x) for x in finals]\n",
    "\n",
    "# Save JSON\n",
    "with open(OUT_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"reviews\": reviews}, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# Save flat CSV for reviewers\n",
    "rows = []\n",
    "for r in reviews:\n",
    "    rows.append({\n",
    "        \"resume_id\": r[\"resume_id\"],\n",
    "        \"name\": r[\"name\"],\n",
    "        \"fit_score\": r[\"fit_score\"],\n",
    "        \"years_experience\": r[\"years_experience\"],\n",
    "        \"top_skills\": \"; \".join(r[\"top_skills\"][:10]),\n",
    "        \"summary\": (r[\"summary\"] or \"\")[:600],\n",
    "        \"strengths\": \" | \".join(r[\"strengths\"]),\n",
    "        \"gaps\": \" | \".join(r[\"gaps\"]),\n",
    "        \"suggestions\": \" | \".join(r[\"suggestions\"]),\n",
    "        \"short_review\": r[\"short_review\"][:400]\n",
    "    })\n",
    "df = pd.DataFrame(rows)\n",
    "df.to_csv(OUT_CSV, index=False, encoding=\"utf-8\")\n",
    "\n",
    "# Save a small HTML preview (cards)\n",
    "def make_html_cards(reviews, out_html: Path, top_n=50):\n",
    "    html_lines = [\"<html><head><meta charset='utf-8'><title>Resume Reviews</title>\",\n",
    "                  \"<style>body{font-family:Inter,system-ui,Arial;margin:16px} .card{border:1px solid #ddd;padding:12px;margin:12px;border-radius:8px;max-width:900px} h3{margin:0} .meta{color:#666;font-size:13px}</style></head><body>\"]\n",
    "    html_lines.append(f\"<h1>Resume Reviews — {len(reviews)} items</h1>\")\n",
    "    for r in sorted(reviews, key=lambda x: -float(x.get(\"fit_score\") or 0.0))[:top_n]:\n",
    "        html_lines.append(\"<div class='card'>\")\n",
    "        html_lines.append(f\"<h3>{html.escape(str(r.get('name') or r.get('resume_id')))} — {r.get('resume_id')}</h3>\")\n",
    "        html_lines.append(f\"<div class='meta'>Fit: {r.get('fit_score')} | Years: {r.get('years_experience') or ''} | Skills: {', '.join(r.get('top_skills')[:7])}</div>\")\n",
    "        html_lines.append(f\"<p><strong>Summary:</strong> {html.escape(r.get('summary') or '')}</p>\")\n",
    "        if r['strengths']:\n",
    "            html_lines.append(\"<p><strong>Strengths:</strong><ul>\" + \"\".join([f\"<li>{html.escape(s)}</li>\" for s in r['strengths']]) + \"</ul></p>\")\n",
    "        if r['gaps']:\n",
    "            html_lines.append(\"<p><strong>Areas to improve:</strong><ul>\" + \"\".join([f\"<li>{html.escape(g)}</li>\" for g in r['gaps']]) + \"</ul></p>\")\n",
    "        if r['suggestions']:\n",
    "            html_lines.append(\"<p><strong>Suggestions:</strong><ol>\" + \"\".join([f\"<li>{html.escape(s)}</li>\" for s in r['suggestions'][:10]]) + \"</ol></p>\")\n",
    "        html_lines.append(\"</div>\")\n",
    "    html_lines.append(\"</body></html>\")\n",
    "    out_html.write_text(\"\\n\".join(html_lines), encoding=\"utf-8\")\n",
    "    return out_html\n",
    "\n",
    "html_path = make_html_cards(reviews, OUT_HTML, top_n=60)\n",
    "\n",
    "print(\"Saved reviews JSON:\", OUT_JSON)\n",
    "print(\"Saved reviews CSV :\", OUT_CSV)\n",
    "print(\"Saved HTML preview  :\", html_path)\n",
    "print(\"\\nSample review (first item):\")\n",
    "import pprint\n",
    "pprint.pprint(reviews[0] if reviews else \"No reviews generated.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "087e9b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 candidates (rank | fit_score | years | name | resume_id):\n",
      " 1 | 100.00 | 25 | LE HOANG                       | sample_00419.txt\n",
      " 2 |  99.87 | 17 | Hoàng Quang Hưng               | sample_00218.txt\n",
      " 3 |  98.12 | 40 | Doan Minh Hoang                | sample_00220.txt\n",
      " 4 |  97.98 | 17 | Nguyen Ngoc Dang               | sample_00159.txt\n",
      " 5 |  96.13 | 10 | NGUYEN VAN HUONG\n",
      "Day           | sample_00261.txt\n",
      " 6 |  92.21 | 16 | Chung Vi Huy                   | sample_00267.txt\n",
      " 7 |  91.01 | 15 | DINH NGUYEN DANG KHOA          | sample_00236.txt\n",
      " 8 |  90.23 | 16 | Van Luong                      | sample_00465.txt\n",
      " 9 |  89.77 | 14 | PHAN MINH THỌ                  | sample_00180.txt\n",
      "10 |  86.90 |  3 | Trần Dưỡng                     | sample_00444.txt\n",
      "\n",
      "Files written:\n",
      " - Full HR CSV:  ..\\data\\resumes_preocr\\final\\hr_ready\\hr_all_reviews.csv\n",
      " - Top 20 CSV :  ..\\data\\resumes_preocr\\final\\hr_ready\\top20_for_HR.csv\n",
      " - HR HTML page  :  ..\\data\\resumes_preocr\\final\\hr_ready\\hr_review_page.html\n",
      "\n",
      "Done — The HR-ready exports are saved. Open the HTML file in a browser to review the top candidates visually.\n"
     ]
    }
   ],
   "source": [
    "# Final ranking + HR-ready exports (copy & run)\n",
    "import json, math, html\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# ---------- Config (adjust paths if needed) ----------\n",
    "DATA_DIR = Path(\"../data/resumes_preocr\")\n",
    "REVIEWS_JSON = DATA_DIR / \"final\" / \"reviews\" / \"resume_reviews.json\"   # from previous feedback cell\n",
    "# fallback locations if you used a different path\n",
    "if not REVIEWS_JSON.exists():\n",
    "    REVIEWS_JSON = DATA_DIR / \"final\" / \"resume_reviews.json\"  # try alternate\n",
    "\n",
    "OUT_DIR = DATA_DIR / \"final\" / \"hr_ready\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "OUT_CSV_ALL = OUT_DIR / \"hr_all_reviews.csv\"\n",
    "OUT_CSV_TOP20 = OUT_DIR / \"top20_for_HR.csv\"\n",
    "OUT_HTML = OUT_DIR / \"hr_review_page.html\"\n",
    "\n",
    "TOP_K = 20\n",
    "\n",
    "# ---------- Load reviews ----------\n",
    "if not REVIEWS_JSON.exists():\n",
    "    raise FileNotFoundError(f\"Could not find reviews JSON at {REVIEWS_JSON}. Run the review-generation cell first.\")\n",
    "\n",
    "reviews_j = json.load(open(REVIEWS_JSON, \"r\", encoding=\"utf-8\"))\n",
    "reviews = reviews_j.get(\"reviews\", reviews_j if isinstance(reviews_j, list) else [])\n",
    "\n",
    "# ---------- Normalize into dataframe ----------\n",
    "rows = []\n",
    "for r in reviews:\n",
    "    # safe defaults\n",
    "    resume_id = r.get(\"resume_id\")\n",
    "    name = r.get(\"name\") or \"\"\n",
    "    fit_score = float(r.get(\"fit_score\") or 0.0)\n",
    "    years = r.get(\"years_experience\")\n",
    "    try:\n",
    "        years = int(years) if years is not None else None\n",
    "    except:\n",
    "        years = None\n",
    "    top_skills = r.get(\"top_skills\") or []\n",
    "    strengths = r.get(\"strengths\") or []\n",
    "    gaps = r.get(\"gaps\") or []\n",
    "    suggestions = r.get(\"suggestions\") or []\n",
    "    short_review = r.get(\"short_review\") or \"\"\n",
    "    summary = r.get(\"summary\") or \"\"\n",
    "\n",
    "    rows.append({\n",
    "        \"resume_id\": resume_id,\n",
    "        \"name\": name,\n",
    "        \"fit_score\": fit_score,\n",
    "        \"years_experience\": years if years is not None else -1,\n",
    "        \"top_skills\": \"; \".join(top_skills[:12]),\n",
    "        \"summary\": summary.strip(),\n",
    "        \"short_review\": short_review.strip(),\n",
    "        \"strengths\": \" | \".join(strengths),\n",
    "        \"gaps\": \" | \".join(gaps),\n",
    "        \"suggestions\": \" | \".join(suggestions)\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# ---------- Rank: primary fit_score desc, secondary years desc ----------\n",
    "df_sorted = df.sort_values(by=[\"fit_score\", \"years_experience\"], ascending=[False, False]).reset_index(drop=True)\n",
    "\n",
    "# fix years display (-1 => empty)\n",
    "df_sorted[\"years_experience\"] = df_sorted[\"years_experience\"].apply(lambda x: \"\" if x == -1 else int(x))\n",
    "\n",
    "# ---------- Save full CSV and Top-K CSV ----------\n",
    "df_sorted.to_csv(OUT_CSV_ALL, index=False, encoding=\"utf-8\")\n",
    "df_sorted.head(TOP_K).to_csv(OUT_CSV_TOP20, index=False, encoding=\"utf-8\")\n",
    "\n",
    "# ---------- Create a simple HR HTML page (top-K) ----------\n",
    "def make_hr_html(df_top, out_path):\n",
    "    lines = [\"<!doctype html><html><head><meta charset='utf-8'><title>HR — Top Candidates</title>\",\n",
    "             \"<style>body{font-family:Inter,Arial,Helvetica;margin:20px}table{border-collapse:collapse;width:100%}th,td{border:1px solid #ddd;padding:8px}th{background:#f3f4f6;text-align:left}tr:nth-child(odd){background:#fff}tr:nth-child(even){background:#fbfbfb}</style></head><body>\"]\n",
    "    lines.append(f\"<h1>Top {len(df_top)} Candidates — Ranked by fit_score</h1>\")\n",
    "    lines.append(\"<table><thead><tr><th>#</th><th>Name</th><th>resume_id</th><th>fit_score</th><th>years</th><th>top_skills</th><th>one-line summary</th><th>quick feedback</th></tr></thead><tbody>\")\n",
    "    for i, row in df_top.iterrows():\n",
    "        idx = i + 1\n",
    "        # small tidy up\n",
    "        summary_preview = (row[\"summary\"] or \"\")[:240].replace(\"\\n\",\" \").strip()\n",
    "        quick = (row[\"short_review\"] or \"\")[:240].replace(\"\\n\",\" \").strip()\n",
    "        lines.append(\"<tr>\")\n",
    "        lines.append(f\"<td>{idx}</td>\")\n",
    "        lines.append(f\"<td>{html.escape(str(row['name']))}</td>\")\n",
    "        lines.append(f\"<td>{html.escape(str(row['resume_id']))}</td>\")\n",
    "        lines.append(f\"<td>{row['fit_score']}</td>\")\n",
    "        lines.append(f\"<td>{row['years_experience']}</td>\")\n",
    "        lines.append(f\"<td>{html.escape(str(row['top_skills']))}</td>\")\n",
    "        lines.append(f\"<td>{html.escape(summary_preview)}</td>\")\n",
    "        lines.append(f\"<td>{html.escape(quick)}</td>\")\n",
    "        lines.append(\"</tr>\")\n",
    "    lines.append(\"</tbody></table></body></html>\")\n",
    "    out_path.write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n",
    "    return out_path\n",
    "\n",
    "make_hr_html(df_sorted.head(TOP_K).reset_index(drop=True), OUT_HTML)\n",
    "\n",
    "# ---------- Notebook preview: print top 10 nicely ----------\n",
    "print(\"Top 10 candidates (rank | fit_score | years | name | resume_id):\")\n",
    "for i, row in df_sorted.head(10).iterrows():\n",
    "    print(f\"{i+1:2d} | {row['fit_score']:6.2f} | {str(row['years_experience']).rjust(2)} | {row['name'][:30]:30} | {row['resume_id']}\")\n",
    "\n",
    "print(\"\\nFiles written:\")\n",
    "print(\" - Full HR CSV: \", OUT_CSV_ALL)\n",
    "print(f\" - Top {TOP_K} CSV : \", OUT_CSV_TOP20)\n",
    "print(\" - HR HTML page  : \", OUT_HTML)\n",
    "\n",
    "print(\"\\nDone — The HR-ready exports are saved. Open the HTML file in a browser to review the top candidates visually.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d452ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sdk_http_response=HttpResponse(\n",
      "  headers=<dict len=11>\n",
      ") candidates=[Candidate(\n",
      "  content=Content(\n",
      "    parts=[\n",
      "      Part(\n",
      "        text='OK'\n",
      "      ),\n",
      "    ],\n",
      "    role='model'\n",
      "  ),\n",
      "  finish_reason=<FinishReason.STOP: 'STOP'>,\n",
      "  index=0\n",
      ")] create_time=None model_version='gemini-2.5-flash-lite' prompt_feedback=None response_id='1K4aaYKFPM7b4-EP2KOZoA8' usage_metadata=GenerateContentResponseUsageMetadata(\n",
      "  candidates_token_count=1,\n",
      "  prompt_token_count=6,\n",
      "  prompt_tokens_details=[\n",
      "    ModalityTokenCount(\n",
      "      modality=<MediaModality.TEXT: 'TEXT'>,\n",
      "      token_count=6\n",
      "    ),\n",
      "  ],\n",
      "  total_token_count=7\n",
      ") automatic_function_calling_history=[] parsed=None\n"
     ]
    }
   ],
   "source": [
    "from google.genai import Client\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # load .env if present\n",
    "\n",
    "client = Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "resp = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash-lite\",\n",
    "    contents=\"Say the word OK.\",\n",
    "    config={\"max_output_tokens\": 10}\n",
    ")\n",
    "print(resp)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
