{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fa69f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\\\python311.zip', 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\\\DLLs', 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\\\Lib', 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0', 'd:\\\\Work\\\\Capstone_Project\\\\resume-nlp\\\\venv', '', 'd:\\\\Work\\\\Capstone_Project\\\\resume-nlp\\\\venv\\\\Lib\\\\site-packages', 'd:\\\\Work\\\\Capstone_Project\\\\resume-nlp']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# get project root path (parent of 'notebooks' directory)\n",
    "root_path = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "if root_path not in sys.path:\n",
    "    sys.path.append(root_path)\n",
    "\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f049d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abhishek Singh\n",
      "8010852459 | abhisheksingh.vizag@gmail.com | LinkedIn | GitHub | LeetCode\n",
      "Education\n",
      "•\n",
      "VIT Bhopal University | CGPA 9.01\n",
      "Oct 2022 – Present\n",
      "Bachelor of Technology in Computer Science and Engineering\n",
      "Bhopal, Madhya Pradesh\n",
      "•\n",
      "Higher Secondary Education | Grade: 95.4%\n",
      "July 2021\n",
      "Navy Children School, Goa\n",
      "Vasco Da Gama, Goa\n",
      "Technical Skills\n",
      "• Languages/Databases: C++, Python, SQL\n",
      "• Framew\n",
      "email found: True\n",
      "phone found: True\n",
      "education header: True\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from src.ingest import extract_text_pymupdf\n",
    "\n",
    "\n",
    "text = extract_text_pymupdf('../data/Abhishek_Singh_Resume.pdf')\n",
    "print(text[:400])\n",
    "print('email found:', bool(re.search(r\"[\\w\\.-]+@[\\w\\.-]+\", text)))\n",
    "print('phone found:', bool(re.search(r\"\\+?\\d[\\d\\s\\-()]{6,}\\d\", text)))\n",
    "print('education header:', 'Education' in text or 'EDUCATION' in text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e04e074e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG\n",
    "from pathlib import Path\n",
    "DATA_ROOT = Path('../data/resumes_raw_pdf')\n",
    "PDF_DIR = DATA_ROOT / 'pdfs'\n",
    "TXT_DIR = DATA_ROOT / 'txt'\n",
    "FAIL_DIR = DATA_ROOT / 'failures'\n",
    "REPORT_JSON = DATA_ROOT / 'extraction_report.json'\n",
    "REPORT_CSV = DATA_ROOT / 'extraction_report.csv'\n",
    "for d in [PDF_DIR, TXT_DIR, FAIL_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# How many resumes to download for testing (start small: 50-100)\n",
    "N_SAMPLES = 100\n",
    "\n",
    "\n",
    "# Toggle OCR fallback (requires system Tesseract and pytesseract)\n",
    "ENABLE_OCR = False\n",
    "OCR_LANGUAGE = 'eng'\n",
    "\n",
    "\n",
    "# thresholds for checks\n",
    "MIN_TEXT_CHARS = 200\n",
    "MIN_ALPHA_RATIO = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854c9dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Work\\Capstone_Project\\resume-nlp\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authenticated as: AbhishekProgrammer22\n"
     ]
    }
   ],
   "source": [
    "# Option 1: set token for this notebook session and re-run the download cell\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # take environment variables from .env file\n",
    "\n",
    "token = os.getenv(\"HUGGINGFACE_HUB_TOKEN\")   # <-- paste your token here\n",
    "os.environ[\"HUGGINGFACE_HUB_TOKEN\"] = token\n",
    "\n",
    "# optional check who you are\n",
    "from huggingface_hub import whoami\n",
    "print(\"Authenticated as:\", whoami(token=token).get(\"name\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d94f7769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1940 pdf files in the repo. Will copy/process first 100.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "files:  29%|██▉       | 29/100 [00:27<01:48,  1.53s/it]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "files:  31%|███       | 31/100 [00:32<02:08,  1.87s/it]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "files:  32%|███▏      | 32/100 [00:35<02:26,  2.15s/it]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "files:  36%|███▌      | 36/100 [00:41<01:42,  1.60s/it]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "files:  40%|████      | 40/100 [00:51<02:17,  2.29s/it]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "files:  43%|████▎     | 43/100 [01:00<02:20,  2.46s/it]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "files:  64%|██████▍   | 64/100 [01:52<01:36,  2.68s/it]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "files:  67%|██████▋   | 67/100 [02:01<01:23,  2.53s/it]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "files:  68%|██████▊   | 68/100 [02:05<01:38,  3.07s/it]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "files:  73%|███████▎  | 73/100 [02:17<01:05,  2.43s/it]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "files:  82%|████████▏ | 82/100 [02:31<00:30,  1.67s/it]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "files:  89%|████████▉ | 89/100 [02:44<00:19,  1.80s/it]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "files:  91%|█████████ | 91/100 [02:50<00:21,  2.34s/it]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "files:  97%|█████████▋| 97/100 [03:06<00:08,  2.91s/it]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "files: 100%|██████████| 100/100 [03:16<00:00,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n=== SUMMARY ===\n",
      "Total files processed: 100\n",
      "Success (no detected failure reasons): 1 (1.0%)\n",
      "Files with failures: 99\n",
      "Failure reasons counts: {'missing_contact': 99, 'no_key_section': 67, 'short_text': 24, 'low_alpha_ratio': 24}\n",
      "Report saved -> ..\\data\\resumes_raw_pdf_direct\\extraction_report.json\n",
      "PDFs saved  -> ..\\data\\resumes_raw_pdf_direct\\pdfs\n",
      "Text saved  -> ..\\data\\resumes_raw_pdf_direct\\txt\n",
      "Failures    -> ..\\data\\resumes_raw_pdf_direct\\failures\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Use cached HF snapshot to copy PDFs -> run extraction and produce report\n",
    "from huggingface_hub import list_repo_files, hf_hub_download\n",
    "from pathlib import Path\n",
    "import os, shutil, json, csv, traceback\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# your extractors\n",
    "from src.ingest import extract_text_pymupdf, extract_text_pdfplumber\n",
    "\n",
    "REPO_ID = \"d4rk3r/resumes-raw-pdf\"\n",
    "OUT_ROOT = Path(\"../data/resumes_raw_pdf_direct\")\n",
    "PDF_DIR = OUT_ROOT / \"pdfs\"\n",
    "TXT_DIR = OUT_ROOT / \"txt\"\n",
    "FAIL_DIR = OUT_ROOT / \"failures\"\n",
    "REPORT_JSON = OUT_ROOT / \"extraction_report.json\"\n",
    "REPORT_CSV  = OUT_ROOT / \"extraction_report.csv\"\n",
    "\n",
    "for d in (OUT_ROOT, PDF_DIR, TXT_DIR, FAIL_DIR):\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "N_SAMPLES = 100          # change if you want fewer\n",
    "MIN_TEXT_CHARS = 200\n",
    "MIN_ALPHA_RATIO = 0.2\n",
    "\n",
    "token = os.getenv(\"HUGGINGFACE_HUB_TOKEN\")\n",
    "if not token:\n",
    "    raise RuntimeError(\"HUGGINGFACE_HUB_TOKEN missing - set it before running.\")\n",
    "\n",
    "# list files (we already saw this works)\n",
    "all_files = list_repo_files(REPO_ID, repo_type=\"dataset\", token=token)\n",
    "pdf_files = [f for f in all_files if f.lower().endswith(\".pdf\")]\n",
    "print(f\"Found {len(pdf_files)} pdf files in the repo. Will copy/process first {min(N_SAMPLES, len(pdf_files))}.\")\n",
    "\n",
    "# helper: hf_hub_download returns a local cached path when available\n",
    "def get_cached_path(fname):\n",
    "    try:\n",
    "        local = hf_hub_download(repo_id=REPO_ID, filename=fname, repo_type=\"dataset\", token=token)\n",
    "        return Path(local)\n",
    "    except Exception as e:\n",
    "        print(\"hf_hub_download failed for\", fname, \"->\", type(e).__name__, str(e)[:200])\n",
    "        return None\n",
    "\n",
    "report = []\n",
    "to_process = pdf_files[:min(N_SAMPLES, len(pdf_files))]\n",
    "\n",
    "for idx, fname in enumerate(tqdm(to_process, desc=\"files\")):\n",
    "    rec = {\"filename\": Path(fname).name, \"repo_path\": fname, \"downloaded\": False,\n",
    "           \"extraction_method\": None, \"ocr_used\": False, \"error\": None, \"checks\": None,\n",
    "           \"failure_reasons\": [], \"failure_file\": None}\n",
    "    try:\n",
    "        cached = get_cached_path(fname)\n",
    "        if cached is None:\n",
    "            rec[\"error\"] = \"cache_lookup_failed\"\n",
    "            report.append(rec)\n",
    "            continue\n",
    "\n",
    "        # copy cached file to our PDF_DIR with normalized name\n",
    "        tgt = PDF_DIR / f\"resume_{idx:05d}.pdf\"\n",
    "        shutil.copyfile(cached, tgt)\n",
    "        rec[\"downloaded\"] = True\n",
    "\n",
    "        # extract text: pymupdf primary, pdfplumber fallback\n",
    "        txt = \"\"\n",
    "        try:\n",
    "            txt = extract_text_pymupdf(str(tgt))\n",
    "            rec[\"extraction_method\"] = \"pymupdf\"\n",
    "            if len(txt) < MIN_TEXT_CHARS // 2:\n",
    "                txt2 = extract_text_pdfplumber(str(tgt))\n",
    "                if len(txt2) > len(txt):\n",
    "                    txt = txt2\n",
    "                    rec[\"extraction_method\"] += \"+pdfplumber\"\n",
    "        except Exception as e1:\n",
    "            try:\n",
    "                txt = extract_text_pdfplumber(str(tgt))\n",
    "                rec[\"extraction_method\"] = \"pdfplumber\"\n",
    "            except Exception as e2:\n",
    "                rec[\"error\"] = f\"both_extractors_failed: {e1} | {e2}\"\n",
    "                badf = FAIL_DIR / f\"downloaded_but_extractfail_{idx:05d}.txt\"\n",
    "                with open(badf, \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(f\"Cached path: {cached}\\\\nErrors:\\\\n{e1}\\\\n{e2}\")\n",
    "                rec[\"failure_file\"] = str(badf)\n",
    "                report.append(rec)\n",
    "                continue\n",
    "\n",
    "        txt = (txt or \"\").replace(\"\\r\", \"\\n\").strip()\n",
    "        # save text\n",
    "        with open(TXT_DIR / (tgt.stem + \".txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(txt)\n",
    "\n",
    "        # checks\n",
    "        import re\n",
    "        EMAIL_RE = re.compile(r\"[\\w\\.-]+@[\\w\\.-]+\\.\\w+\")\n",
    "        PHONE_RE = re.compile(r\"\\+?\\d[\\d\\s\\-\\(\\)]{6,}\\d\")\n",
    "        SECTION_KEYWORDS = ['education','experience','skills','projects','certifications','publications','summary','objective']\n",
    "\n",
    "        email = bool(EMAIL_RE.search(txt))\n",
    "        phone = bool(PHONE_RE.search(txt))\n",
    "        txt_low = txt.lower()\n",
    "        sections = {kw: (kw in txt_low) for kw in SECTION_KEYWORDS}\n",
    "        any_section = any(sections.values())\n",
    "        letters = sum(c.isalpha() for c in txt)\n",
    "        alpha_ratio = letters / max(1, len(txt))\n",
    "\n",
    "        checks = {'email': email, 'phone': phone, 'sections': sections, 'any_section': any_section,\n",
    "                  'len_chars': len(txt), 'alpha_ratio': alpha_ratio, 'preview': txt[:800].replace(\"\\n\",\"\\\\n\")}\n",
    "        rec[\"checks\"] = checks\n",
    "\n",
    "        failures = []\n",
    "        if rec[\"error\"]:\n",
    "            failures.append(\"extractor_error\")\n",
    "        if checks['len_chars'] < MIN_TEXT_CHARS:\n",
    "            failures.append(\"short_text\")\n",
    "        if checks['alpha_ratio'] < MIN_ALPHA_RATIO:\n",
    "            failures.append(\"low_alpha_ratio\")\n",
    "        if not checks['any_section']:\n",
    "            failures.append(\"no_key_section\")\n",
    "        if not (checks['email'] and checks['phone']):\n",
    "            failures.append(\"missing_contact\")\n",
    "\n",
    "        rec[\"failure_reasons\"] = failures\n",
    "        if failures:\n",
    "            fname_fail = FAIL_DIR / (tgt.stem + \"_failure.txt\")\n",
    "            with open(fname_fail, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(f\"FILENAME: {tgt.name}\\\\nREPO_PATH: {fname}\\\\nEXTRACTION_METHOD: {rec['extraction_method']}\\\\nFAILURE_REASONS: {failures}\\\\n\\\\n---PREVIEW---\\\\n\\\\n\")\n",
    "                f.write(txt)\n",
    "            rec[\"failure_file\"] = str(fname_fail)\n",
    "\n",
    "    except Exception as e:\n",
    "        rec[\"error\"] = f\"fatal:{type(e).__name__}:{e}\"\n",
    "        rec[\"failure_file\"] = None\n",
    "    report.append(rec)\n",
    "\n",
    "# save reports\n",
    "with open(REPORT_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "with open(REPORT_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as csvf:\n",
    "    writer = csv.writer(csvf)\n",
    "    writer.writerow(['filename','repo_path','downloaded','extraction_method','len_chars','alpha_ratio','email','phone','any_key_section','failure_reasons','failure_file','error'])\n",
    "    for r in report:\n",
    "        ch = r.get('checks') or {}\n",
    "        writer.writerow([r.get('filename'), r.get('repo_path'), r.get('downloaded'), r.get('extraction_method'),\n",
    "                         ch.get('len_chars'), round(ch.get('alpha_ratio',0),3), ch.get('email'), ch.get('phone'), ch.get('any_section'),\n",
    "                         ';'.join(r.get('failure_reasons',[])), r.get('failure_file',''), r.get('error','')])\n",
    "\n",
    "# pretty summary\n",
    "from collections import Counter\n",
    "total = len(report)\n",
    "failures = [r for r in report if r.get('failure_reasons')]\n",
    "success = total - len(failures)\n",
    "fail_reasons = Counter()\n",
    "for r in failures:\n",
    "    fail_reasons.update(r.get('failure_reasons',[]))\n",
    "\n",
    "print(\"\\\\n=== SUMMARY ===\")\n",
    "print(\"Total files processed:\", total)\n",
    "print(\"Success (no detected failure reasons):\", success, f\"({round(100*success/total if total else 0,2)}%)\")\n",
    "print(\"Files with failures:\", len(failures))\n",
    "print(\"Failure reasons counts:\", dict(fail_reasons))\n",
    "print(\"Report saved ->\", REPORT_JSON)\n",
    "print(\"PDFs saved  ->\", PDF_DIR)\n",
    "print(\"Text saved  ->\", TXT_DIR)\n",
    "print(\"Failures    ->\", FAIL_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9042f05a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset lhoestq/resumes-raw-pdf-for-ocr split=train (this may take a minute)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Work\\Capstone_Project\\resume-nlp\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\abhis\\.cache\\huggingface\\hub\\datasets--lhoestq--resumes-raw-pdf-for-ocr. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Generating train split: 100%|██████████| 1585/1585 [00:00<00:00, 3149.88 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded. Columns: ['label', 'images', 'text'] Num examples: 1585\n",
      "Using text field: text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "writing text:  32%|███▏      | 500/1585 [00:03<00:07, 139.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 500 text samples to ..\\data\\resumes_preocr\\txt\n",
      "Report: ..\\data\\resumes_preocr\\report_preocr.json\n",
      "Summary CSV: ..\\data\\resumes_preocr\\summary_preocr.csv\n"
     ]
    }
   ],
   "source": [
    "# Load HF dataset properly and write out text files per example\n",
    "# Paste & run in the same notebook environment\n",
    "\n",
    "from datasets import load_dataset\n",
    "from pathlib import Path\n",
    "import json, csv, shutil\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "\n",
    "OUT_ROOT = Path(\"../data/resumes_preocr\")\n",
    "TXT_DIR = OUT_ROOT / \"txt\"\n",
    "PDF_DIR = OUT_ROOT / \"pdfs\"   # may remain empty for this dataset\n",
    "REPORT = OUT_ROOT / \"report_preocr.json\"\n",
    "SUMMARY_CSV = OUT_ROOT / \"summary_preocr.csv\"\n",
    "\n",
    "OUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "TXT_DIR.mkdir(exist_ok=True)\n",
    "PDF_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "REPO_ID = \"lhoestq/resumes-raw-pdf-for-ocr\"\n",
    "SPLIT = \"train\"   # dataset split\n",
    "N_SAMPLES = 500   # adjust to how many you want to extract (max ~ full dataset size ~ 1585)\n",
    "\n",
    "print(f\"Loading dataset {REPO_ID} split={SPLIT} (this may take a minute)...\")\n",
    "ds = load_dataset(REPO_ID, split=SPLIT)\n",
    "\n",
    "print(\"Dataset loaded. Columns:\", ds.column_names, \"Num examples:\", len(ds))\n",
    "\n",
    "# Which field contains text? Common names: 'text'\n",
    "text_field = None\n",
    "for candidate in (\"text\", \"ocr\", \"page_text\", \"raw_text\"):\n",
    "    if candidate in ds.column_names:\n",
    "        text_field = candidate\n",
    "        break\n",
    "# fallback: look for any string column\n",
    "if text_field is None:\n",
    "    for c in ds.column_names:\n",
    "        # sample few rows to check if column is string-like and non-empty\n",
    "        try:\n",
    "            sample = ds[0].get(c)\n",
    "            if isinstance(sample, str):\n",
    "                text_field = c\n",
    "                break\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "if text_field is None:\n",
    "    raise RuntimeError(f\"Couldn't find a text column in dataset. Columns: {ds.column_names}\")\n",
    "\n",
    "print(\"Using text field:\", text_field)\n",
    "\n",
    "report = []\n",
    "count = 0\n",
    "for i, ex in enumerate(tqdm(ds, desc=\"writing text\")):\n",
    "    if count >= N_SAMPLES:\n",
    "        break\n",
    "    txt = ex.get(text_field) or \"\"\n",
    "    # sometimes the text may be empty (filter as desired)\n",
    "    if not txt or not txt.strip():\n",
    "        # skip empty text entries (optionally you can save them)\n",
    "        continue\n",
    "    fname = f\"sample_{count:05d}.txt\"\n",
    "    tgt = TXT_DIR / fname\n",
    "    with open(tgt, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(txt)\n",
    "    report.append({\"index\": i, \"filename\": fname, \"text_len\": len(txt), \"preview\": txt[:500].replace(\"\\n\",\"\\\\n\")})\n",
    "    count += 1\n",
    "\n",
    "# save the report json and a CSV summary with basic checks\n",
    "with open(REPORT, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "\n",
    "import re\n",
    "EMAIL_RE = re.compile(r\"[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+\")\n",
    "PHONE_RE = re.compile(r\"(?:\\+?\\d{1,3}[-.\\s]?)?(?:\\(?\\d{2,4}\\)?[-.\\s]?)?\\d{6,12}\")\n",
    "SECTIONS = ['education','experience','skills','projects','certifications','publications','summary','objective','work experience']\n",
    "\n",
    "rows = []\n",
    "for r in report:\n",
    "    txt = (TXT_DIR / r['filename']).read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    email = bool(EMAIL_RE.search(txt))\n",
    "    phone = bool(PHONE_RE.search(txt))\n",
    "    any_section = any(k in txt.lower() for k in SECTIONS)\n",
    "    rows.append({\n",
    "        \"file\": r['filename'],\n",
    "        \"chars\": len(txt),\n",
    "        \"email\": email,\n",
    "        \"phone\": phone,\n",
    "        \"any_section\": any_section,\n",
    "        \"preview\": txt[:400].replace(\"\\n\",\"\\\\n\")\n",
    "    })\n",
    "\n",
    "# write CSV\n",
    "import csv\n",
    "with open(SUMMARY_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=list(rows[0].keys()) if rows else [\"file\"])\n",
    "    writer.writeheader()\n",
    "    for row in rows:\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(f\"Saved {len(report)} text samples to {TXT_DIR}\")\n",
    "print(\"Report:\", REPORT)\n",
    "print(\"Summary CSV:\", SUMMARY_CSV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d9a90f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 500 text files with spaCy NER...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NER: 100%|██████████| 500/500 [03:54<00:00,  2.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== NER SUMMARY ===\n",
      "Files processed: 500\n",
      "Emails found: 1\n",
      "Phones found: 43\n",
      "Files with detected skills: 499\n",
      "Average text length: 3762 chars\n",
      "Saved ner extract -> ..\\data\\resumes_preocr\\ner_extract.json\n",
      "\n",
      "Sample entry (first file):\n",
      "{'chars': 2812,\n",
      " 'emails': [],\n",
      " 'file': 'sample_00000.txt',\n",
      " 'locations': ['Hanoi', 'Robot', 'Tkinter', 'Taiwan'],\n",
      " 'names': ['Nguyen Dang Binh', 'Luster LightTech', 'Debug'],\n",
      " 'orgs': ['AI/Computer Vision Engineer\\nAddress', 'Vision Software Senior', 'BacNinh', 'AOI', 'Luxshare-ICT',\n",
      "          'Medical Image Segmentation'],\n",
      " 'phones': [],\n",
      " 'preview': 'Nguyen Dang Binh – AI/Computer Vision Engineer\\\\nAddress: Bac Ninh City\\\\nE '\n",
      "            'mail:\\\\nMobile/Zalo:\\\\nBirthday: Jan 8th 1986\\\\nK EY STRENGTHS\\\\nMachine vision \\uf020Team '\n",
      "            'Leader\\\\n\\uf0a7\\uf020\\\\n\\uf0a7\\uf020AI engineer Automation software\\\\nP ROFESSIONAL EXPERIENCE\\\\nVision '\n",
      "            'Software Senior (Aug 2023 – Now): Luster LightTech, BacNinh, VN:\\\\n\\uf0d8 Develop the low-code software '\n",
      "            'from internal platforms for vision projects.\\\\nDebug the software at AOI machine at Luxshare-ICT, '\n",
      "            'Goerteck factory.\\\\n\\uf0d8\\\\nSoftware Engineer Senior Leader (Mar 2020 – May 2023): Electronic ACE '\n",
      "            'Antenna\\\\nCompany | Hanoi, VN:\\\\n\\uf0d8 Measurement and detection defect software project: Develop t',\n",
      " 'skills': ['c', 'c++', 'deep learning', 'machine learning', 'python', 'pytorch', 'tensorflow']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 3 (run): SpaCy NER + rule-based extraction\n",
    "import json, re, sys\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ensure spaCy model installed\n",
    "try:\n",
    "    import spacy\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except Exception as e:\n",
    "    print(\"spaCy model en_core_web_sm not found. Install it and re-run:\")\n",
    "    print(\"    python -m spacy download en_core_web_sm\")\n",
    "    raise\n",
    "\n",
    "ROOT = Path(\"../data/resumes_preocr\")\n",
    "TXT_DIR = ROOT / \"txt\"\n",
    "OUT_EXTRACT = ROOT / \"ner_extract.json\"\n",
    "\n",
    "EMAIL_RE = re.compile(r\"[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+\")\n",
    "PHONE_RE = re.compile(r\"(?:\\+?\\d{1,3}[-.\\s]?)?(?:\\(?\\d{2,4}\\)?[-.\\s]?)?\\d{6,12}\")\n",
    "\n",
    "# small initial skills vocabulary — expand with your domain\n",
    "SKILLS = {\n",
    "    \"python\",\"java\",\"c++\",\"c\",\"sql\",\"nlp\",\"natural language processing\",\"deep learning\",\n",
    "    \"machine learning\",\"tensorflow\",\"pytorch\",\"react\",\"docker\",\"kubernetes\",\"git\",\n",
    "    \"aws\",\"gcp\",\"azure\",\"linux\",\"pandas\",\"numpy\",\"scikit-learn\",\"keras\"\n",
    "}\n",
    "\n",
    "def extract_skills(text):\n",
    "    tl = text.lower()\n",
    "    found = []\n",
    "    for s in SKILLS:\n",
    "        if s in tl:\n",
    "            found.append(s)\n",
    "    return sorted(found)\n",
    "\n",
    "results = []\n",
    "files = sorted(TXT_DIR.glob(\"*.txt\"))\n",
    "print(\"Processing\", len(files), \"text files with spaCy NER...\")\n",
    "\n",
    "for p in tqdm(files, desc=\"NER\"):\n",
    "    txt = p.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    doc = nlp(txt)\n",
    "    names = []\n",
    "    orgs = []\n",
    "    gpes = []\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PERSON\":\n",
    "            names.append(ent.text)\n",
    "        elif ent.label_ in (\"ORG\",\"NORP\"):\n",
    "            orgs.append(ent.text)\n",
    "        elif ent.label_ == \"GPE\":\n",
    "            gpes.append(ent.text)\n",
    "    emails = EMAIL_RE.findall(txt)\n",
    "    phones = PHONE_RE.findall(txt)\n",
    "    skills = extract_skills(txt)\n",
    "    results.append({\n",
    "        \"file\": p.name,\n",
    "        \"names\": list(dict.fromkeys(names))[:3],\n",
    "        \"orgs\": list(dict.fromkeys(orgs))[:6],\n",
    "        \"locations\": list(dict.fromkeys(gpes))[:4],\n",
    "        \"emails\": emails,\n",
    "        \"phones\": phones,\n",
    "        \"skills\": skills,\n",
    "        \"chars\": len(txt),\n",
    "        \"preview\": txt[:600].replace(\"\\n\",\"\\\\n\")\n",
    "    })\n",
    "\n",
    "# save results\n",
    "with open(OUT_EXTRACT, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "# print quick stats\n",
    "total = len(results)\n",
    "emails_found = sum(1 for r in results if r[\"emails\"])\n",
    "phones_found = sum(1 for r in results if r[\"phones\"])\n",
    "skills_found = sum(1 for r in results if r[\"skills\"])\n",
    "avg_len = sum(r[\"chars\"] for r in results) / total if total else 0\n",
    "\n",
    "print(\"\\n=== NER SUMMARY ===\")\n",
    "print(f\"Files processed: {total}\")\n",
    "print(f\"Emails found: {emails_found}\")\n",
    "print(f\"Phones found: {phones_found}\")\n",
    "print(f\"Files with detected skills: {skills_found}\")\n",
    "print(f\"Average text length: {avg_len:.0f} chars\")\n",
    "print(\"Saved ner extract ->\", OUT_EXTRACT)\n",
    "print(\"\\nSample entry (first file):\")\n",
    "if results:\n",
    "    import pprint\n",
    "    pprint.pprint(results[0], compact=True, width=120)\n",
    "else:\n",
    "    print(\"No text files found in\", TXT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e745214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote enhanced NER -> ..\\data\\resumes_preocr\\ner_enhanced.json\n",
      "Total entries: 500\n",
      "Contact found: 152\n",
      "States: {'lang:en': 183, 'lang:vi': 315, 'has_phone': 152, 'lang:nl': 1, 'lang:zh-cn': 1, 'has_email': 1}\n",
      "\n",
      "Sample enhanced entry (first):\n",
      "{'chars': 2812,\n",
      " 'contact_status': 'missing',\n",
      " 'emails': [],\n",
      " 'file': 'sample_00000.txt',\n",
      " 'github': [],\n",
      " 'language': 'en',\n",
      " 'linkedin': [],\n",
      " 'orgs': ['BacNinh', 'AOI', 'Luxshare-ICT', 'Medical Image Segmentation'],\n",
      " 'original_preview': 'Nguyen Dang Binh – AI/Computer Vision '\n",
      "                     'Engineer\\\\nAddress: Bac Ninh City\\\\nE '\n",
      "                     'mail:\\\\nMobile/Zalo:\\\\nBirthday: Jan 8th 1986\\\\nK EY '\n",
      "                     'STRENGTHS\\\\nMachine vision \\uf020Team '\n",
      "                     'Leader\\\\n\\uf0a7\\uf020\\\\n\\uf0a7\\uf020AI engineer '\n",
      "                     'Automation software\\\\nP ROFESSIONAL EXPERIENCE\\\\nVision '\n",
      "                     'Software Senior (Aug 2023 – Now): Luster LightTech, '\n",
      "                     'BacNinh, VN:\\\\n\\uf0d8 Develop the low-code software from '\n",
      "                     'internal platforms for vision projects.\\\\nDebug the '\n",
      "                     'software at AOI machine at Luxshare-ICT, Goerteck '\n",
      "                     'factory.\\\\n\\uf0d8\\\\nSoftware Engineer Senior Leader (Mar '\n",
      "                     '2020 – May 2023): Electronic ACE Antenna\\\\nCompany | '\n",
      "                     'Hanoi, VN:\\\\n\\uf0d8 Measurement and detection defect '\n",
      "                     'software project: Develop t',\n",
      " 'phones': [],\n",
      " 'primary_name': 'Nguyen Dang Binh',\n",
      " 'skills': ['c',\n",
      "            'c#',\n",
      "            'c++',\n",
      "            'communication',\n",
      "            'computer vision',\n",
      "            'deep learning',\n",
      "            'machine learning',\n",
      "            'mlops',\n",
      "            'opencv',\n",
      "            'python',\n",
      "            'pytorch',\n",
      "            'tensorflow']}\n"
     ]
    }
   ],
   "source": [
    "# Post-process NER outputs: normalize names, improve phone/email detection, expand skills, detect language\n",
    "import json, re, os\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "\n",
    "ROOT = Path(\"../data/resumes_preocr\")\n",
    "NER_IN = ROOT / \"ner_extract.json\"\n",
    "NER_OUT = ROOT / \"ner_enhanced.json\"\n",
    "TXT_DIR = ROOT / \"txt\"\n",
    "\n",
    "# load NER results\n",
    "with open(NER_IN, \"r\", encoding=\"utf-8\") as f:\n",
    "    ner = json.load(f)\n",
    "\n",
    "# stronger regexes\n",
    "EMAIL_RE = re.compile(r\"[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+\")\n",
    "# phone: look for sequences of digits with common separators, allow country code, require 7-15 digits total\n",
    "PHONE_RE = re.compile(r\"(?:\\+?\\d{1,3}[-.\\s]?)?(?:\\(?\\d{1,4}\\)?[-.\\s]?)?(?:\\d[-.\\s]?){6,14}\\d\")\n",
    "LINKEDIN_RE = re.compile(r\"(linkedin\\.com/[A-Za-z0-9_\\-./]+|linkedin:[A-Za-z0-9_\\-/]+)\", re.I)\n",
    "GITHUB_RE = re.compile(r\"(github\\.com/[A-Za-z0-9_.\\-]+|github:[A-Za-z0-9_.\\-]+)\", re.I)\n",
    "\n",
    "# expanded skills vocabulary (extendable)\n",
    "MORE_SKILLS = [\n",
    "    \"python\",\"java\",\"c++\",\"c\",\"c#\",\"sql\",\"nlp\",\"natural language processing\",\"deep learning\",\n",
    "    \"machine learning\",\"tensorflow\",\"pytorch\",\"keras\",\"scikit-learn\",\"spark\",\"hadoop\",\n",
    "    \"pandas\",\"numpy\",\"matplotlib\",\"seaborn\",\"docker\",\"kubernetes\",\"aws\",\"gcp\",\"azure\",\n",
    "    \"react\",\"node.js\",\"express\",\"flask\",\"django\",\"rest api\",\"graphql\",\"git\",\"linux\",\n",
    "    \"bash\",\"mlops\",\"computer vision\",\"opencv\",\"pandas\",\"communication\",\"leadership\",\n",
    "    \"excel\",\"tableau\",\"power bi\",\"spark\",\"hive\",\"bigquery\",\"seo\",\"marketing\",\"sales\"\n",
    "]\n",
    "# normalize to lowercase for substring matching\n",
    "MORE_SKILLS = list(dict.fromkeys([s.lower() for s in MORE_SKILLS]))\n",
    "\n",
    "# optional fuzzy matching: use rapidfuzz if installed\n",
    "try:\n",
    "    from rapidfuzz import process, fuzz\n",
    "    FUZZY_AVAILABLE = True\n",
    "except Exception:\n",
    "    FUZZY_AVAILABLE = False\n",
    "\n",
    "# helper: load raw text for a sample file if present\n",
    "def load_text_for_file(filename):\n",
    "    txt_path = TXT_DIR / filename\n",
    "    if txt_path.exists():\n",
    "        return txt_path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    return \"\"\n",
    "\n",
    "# helper: clean orgs (remove entries that look like addresses or job titles)\n",
    "JOB_TITLE_KEYWORDS = set([\"engineer\",\"developer\",\"manager\",\"senior\",\"lead\",\"intern\",\"assistant\",\"consultant\",\n",
    "                          \"officer\",\"analyst\",\"specialist\",\"architect\",\"director\",\"president\",\"coordinator\",\n",
    "                          \"supervisor\"])\n",
    "def clean_org_list(orgs):\n",
    "    cleaned = []\n",
    "    for o in orgs:\n",
    "        s = o.strip()\n",
    "        # skip if empty or obviously a sentence fragment or contains newline markers\n",
    "        if not s or len(s) < 2:\n",
    "            continue\n",
    "        # skip if it contains 'address' or 'mobile' or 'email' (likely not org)\n",
    "        low = s.lower()\n",
    "        if any(x in low for x in (\"address\",\"mobile\",\"email\",\"phone\",\"birthday\",\"birth\",\"cv\",\"c.v\",\"objective\",\"profile\")):\n",
    "            continue\n",
    "        # skip if it's too long garbage with many punctuation characters\n",
    "        punct_ratio = sum(1 for ch in s if not ch.isalnum() and not ch.isspace()) / max(1,len(s))\n",
    "        if punct_ratio > 0.25 and len(s) < 50:\n",
    "            continue\n",
    "        # optionally skip entries that are likely job titles (we want organizations)\n",
    "        if any(jk in low for jk in JOB_TITLE_KEYWORDS):\n",
    "            # allow if it contains a comma and an apparent company name later\n",
    "            if \",\" not in s and len(s.split()) < 6:\n",
    "                # likely a job title, skip\n",
    "                continue\n",
    "        cleaned.append(s)\n",
    "    # dedupe keeping order\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for o in cleaned:\n",
    "        if o not in seen:\n",
    "            out.append(o); seen.add(o)\n",
    "    return out\n",
    "\n",
    "# helper: expand skills by substring (and fuzzy if available)\n",
    "def extract_skills_from_text(text):\n",
    "    tl = text.lower()\n",
    "    found = set()\n",
    "    for s in MORE_SKILLS:\n",
    "        if s in tl:\n",
    "            found.add(s)\n",
    "    # fuzzy: if available, match tokens\n",
    "    if FUZZY_AVAILABLE and not found:\n",
    "        # take top fuzzy matches for single-word tokens\n",
    "        tokens = set(re.findall(r\"[A-Za-z0-9+#\\.\\-]+\", tl))\n",
    "        for tok in tokens:\n",
    "            best = process.extractOne(tok, MORE_SKILLS, scorer=fuzz.partial_ratio)\n",
    "            if best and best[1] >= 90:\n",
    "                found.add(best[0])\n",
    "    return sorted(found)\n",
    "\n",
    "# try langdetect if available\n",
    "try:\n",
    "    from langdetect import detect, DetectorFactory\n",
    "    DetectorFactory.seed = 0\n",
    "    LANGDET_AVAILABLE = True\n",
    "except Exception:\n",
    "    LANGDET_AVAILABLE = False\n",
    "\n",
    "# process and enhance\n",
    "enhanced = []\n",
    "counters = Counter()\n",
    "for entry in ner:\n",
    "    fname = entry.get(\"file\")\n",
    "    raw_text = load_text_for_file(fname)\n",
    "    # combine original fields and raw text for better detection\n",
    "    emails = entry.get(\"emails\", []) or EMAIL_RE.findall(raw_text)\n",
    "    phones = entry.get(\"phones\", []) or PHONE_RE.findall(raw_text)\n",
    "    linkedin = LINKEDIN_RE.findall(raw_text) or LINKEDIN_RE.findall(\" \".join(entry.get(\"orgs\",[])))\n",
    "    github = GITHUB_RE.findall(raw_text) or GITHUB_RE.findall(\" \".join(entry.get(\"orgs\",[])))\n",
    "    # choose primary name: prefer first PERSON entity; if no person, try first line of text\n",
    "    primary_name = None\n",
    "    if entry.get(\"names\"):\n",
    "        # pick first name-like token but avoid when it's obviously organization/job (heuristic)\n",
    "        for nm in entry[\"names\"]:\n",
    "            if nm and len(nm) > 1 and not any(t.lower() in nm.lower() for t in (\"engineer\",\"developer\",\"company\",\"address\",\"mobile\")):\n",
    "                primary_name = nm.strip()\n",
    "                break\n",
    "        if primary_name is None:\n",
    "            primary_name = entry[\"names\"][0].strip()\n",
    "    if not primary_name and raw_text:\n",
    "        # take the first non-empty line up to 80 chars as a fallback\n",
    "        for ln in raw_text.splitlines():\n",
    "            ln = ln.strip()\n",
    "            if ln and len(ln) < 80:\n",
    "                primary_name = ln\n",
    "                break\n",
    "    # clean orgs\n",
    "    orgs_clean = clean_org_list(entry.get(\"orgs\",[]))\n",
    "    # expand skills\n",
    "    text_for_skills = \" \".join([raw_text, entry.get(\"preview\",\"\")])\n",
    "    skills = sorted(set(entry.get(\"skills\",[])) | set(extract_skills_from_text(text_for_skills)))\n",
    "    # detect language\n",
    "    lang = \"unknown\"\n",
    "    if LANGDET_AVAILABLE and raw_text.strip():\n",
    "        try:\n",
    "            lang = detect(raw_text[:2000])\n",
    "        except Exception:\n",
    "            lang = \"unknown\"\n",
    "    # contact status\n",
    "    if emails or phones:\n",
    "        contact_status = \"found\"\n",
    "    else:\n",
    "        # can't reliably detect redaction here (we didn't run redaction detector on this dataset)\n",
    "        contact_status = \"missing\"\n",
    "    # assemble enhanced entry\n",
    "    e = {\n",
    "        \"file\": fname,\n",
    "        \"primary_name\": primary_name or \"\",\n",
    "        \"orgs\": orgs_clean,\n",
    "        \"emails\": emails,\n",
    "        \"phones\": phones,\n",
    "        \"linkedin\": linkedin,\n",
    "        \"github\": github,\n",
    "        \"skills\": skills,\n",
    "        \"chars\": entry.get(\"chars\", 0),\n",
    "        \"language\": lang,\n",
    "        \"contact_status\": contact_status,\n",
    "        \"original_preview\": entry.get(\"preview\",\"\")\n",
    "    }\n",
    "    enhanced.append(e)\n",
    "    counters.update([contact_status])\n",
    "    if emails: counters.update([\"has_email\"])\n",
    "    if phones: counters.update([\"has_phone\"])\n",
    "    counters.update([\"lang:\"+lang])\n",
    "\n",
    "# save\n",
    "with open(NER_OUT, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(enhanced, f, indent=2)\n",
    "\n",
    "# print a concise summary\n",
    "total = len(enhanced)\n",
    "print(\"Wrote enhanced NER ->\", NER_OUT)\n",
    "print(\"Total entries:\", total)\n",
    "print(\"Contact found:\", counters[\"found\"] if \"found\" in counters else sum(1 for e in enhanced if e[\"contact_status\"]==\"found\"))\n",
    "print(\"States:\", {k:v for k,v in counters.items() if k.startswith(\"lang:\") or k in ('has_email','has_phone')})\n",
    "print(\"\\nSample enhanced entry (first):\")\n",
    "pprint(enhanced[0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
