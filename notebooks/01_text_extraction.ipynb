{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c132fee",
   "metadata": {},
   "source": [
    "### Adding path of the project folder in system variable to find modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fa69f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\\\python311.zip', 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\\\DLLs', 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\\\Lib', 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0', 'd:\\\\Work\\\\Capstone_Project\\\\resume-nlp\\\\venv', '', 'd:\\\\Work\\\\Capstone_Project\\\\resume-nlp\\\\venv\\\\Lib\\\\site-packages', 'd:\\\\Work\\\\Capstone_Project\\\\resume-nlp']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# get project root path (parent of 'notebooks' directory)\n",
    "root_path = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "if root_path not in sys.path:\n",
    "    sys.path.append(root_path)\n",
    "\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943c2e20",
   "metadata": {},
   "source": [
    "### Step 1: Extract text from pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f049d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abhishek Singh\n",
      "8010852459 | abhisheksingh.vizag@gmail.com | LinkedIn | GitHub | LeetCode\n",
      "Education\n",
      "•\n",
      "VIT Bhopal University | CGPA 9.01\n",
      "Oct 2022 – Present\n",
      "Bachelor of Technology in Computer Science and Engineering\n",
      "Bhopal, Madhya Pradesh\n",
      "•\n",
      "Higher Secondary Education | Grade: 95.4%\n",
      "July 2021\n",
      "Navy Children School, Goa\n",
      "Vasco Da Gama, Goa\n",
      "Technical Skills\n",
      "• Languages/Databases: C++, Python, SQL\n",
      "• Framew\n",
      "email found: True\n",
      "phone found: True\n",
      "education header: True\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from src.ingest import extract_text_pymupdf\n",
    "\n",
    "\n",
    "text = extract_text_pymupdf('../data/Abhishek_Singh_Resume.pdf')\n",
    "print(text[:400])\n",
    "print('email found:', bool(re.search(r\"[\\w\\.-]+@[\\w\\.-]+\", text)))\n",
    "print('phone found:', bool(re.search(r\"\\+?\\d[\\d\\s\\-()]{6,}\\d\", text)))\n",
    "print('education header:', 'Education' in text or 'EDUCATION' in text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84be2729",
   "metadata": {},
   "source": [
    "- setting up some configuration for the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e04e074e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG\n",
    "from pathlib import Path\n",
    "DATA_ROOT = Path('../data/resumes_raw_pdf')\n",
    "PDF_DIR = DATA_ROOT / 'pdfs'\n",
    "TXT_DIR = DATA_ROOT / 'txt'\n",
    "FAIL_DIR = DATA_ROOT / 'failures'\n",
    "REPORT_JSON = DATA_ROOT / 'extraction_report.json'\n",
    "REPORT_CSV = DATA_ROOT / 'extraction_report.csv'\n",
    "for d in [PDF_DIR, TXT_DIR, FAIL_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# How many resumes to download for testing (start small: 50-100)\n",
    "N_SAMPLES = 100\n",
    "\n",
    "\n",
    "# Toggle OCR fallback (requires system Tesseract and pytesseract)\n",
    "ENABLE_OCR = False\n",
    "OCR_LANGUAGE = 'eng'\n",
    "\n",
    "\n",
    "# thresholds for checks\n",
    "MIN_TEXT_CHARS = 200\n",
    "MIN_ALPHA_RATIO = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854c9dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Work\\Capstone_Project\\resume-nlp\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authenticated as: AbhishekProgrammer22\n"
     ]
    }
   ],
   "source": [
    "# Option 1: set token for this notebook session and re-run the download cell\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # take environment variables from .env file\n",
    "\n",
    "token = os.getenv(\"HUGGINGFACE_HUB_TOKEN\")   # <-- paste your token here\n",
    "os.environ[\"HUGGINGFACE_HUB_TOKEN\"] = token\n",
    "\n",
    "# optional check who you are\n",
    "from huggingface_hub import whoami\n",
    "print(\"Authenticated as:\", whoami(token=token).get(\"name\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060ed5af",
   "metadata": {},
   "source": [
    "### Step 2: Selecting a dataset and proceeding with NER Topic modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d94f7769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1940 pdf files in the repo. Will copy/process first 100.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "files:  29%|██▉       | 29/100 [00:27<01:48,  1.53s/it]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "files:  31%|███       | 31/100 [00:32<02:08,  1.87s/it]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "files:  32%|███▏      | 32/100 [00:35<02:26,  2.15s/it]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "files:  36%|███▌      | 36/100 [00:41<01:42,  1.60s/it]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "files:  40%|████      | 40/100 [00:51<02:17,  2.29s/it]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "files:  43%|████▎     | 43/100 [01:00<02:20,  2.46s/it]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "files:  64%|██████▍   | 64/100 [01:52<01:36,  2.68s/it]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "files:  67%|██████▋   | 67/100 [02:01<01:23,  2.53s/it]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "files:  68%|██████▊   | 68/100 [02:05<01:38,  3.07s/it]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "files:  73%|███████▎  | 73/100 [02:17<01:05,  2.43s/it]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "files:  82%|████████▏ | 82/100 [02:31<00:30,  1.67s/it]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "files:  89%|████████▉ | 89/100 [02:44<00:19,  1.80s/it]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "files:  91%|█████████ | 91/100 [02:50<00:21,  2.34s/it]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "files:  97%|█████████▋| 97/100 [03:06<00:08,  2.91s/it]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "files: 100%|██████████| 100/100 [03:16<00:00,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n=== SUMMARY ===\n",
      "Total files processed: 100\n",
      "Success (no detected failure reasons): 1 (1.0%)\n",
      "Files with failures: 99\n",
      "Failure reasons counts: {'missing_contact': 99, 'no_key_section': 67, 'short_text': 24, 'low_alpha_ratio': 24}\n",
      "Report saved -> ..\\data\\resumes_raw_pdf_direct\\extraction_report.json\n",
      "PDFs saved  -> ..\\data\\resumes_raw_pdf_direct\\pdfs\n",
      "Text saved  -> ..\\data\\resumes_raw_pdf_direct\\txt\n",
      "Failures    -> ..\\data\\resumes_raw_pdf_direct\\failures\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Use cached HF snapshot to copy PDFs -> run extraction and produce report\n",
    "from huggingface_hub import list_repo_files, hf_hub_download\n",
    "from pathlib import Path\n",
    "import os, shutil, json, csv, traceback\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# your extractors\n",
    "from src.ingest import extract_text_pymupdf, extract_text_pdfplumber\n",
    "\n",
    "REPO_ID = \"d4rk3r/resumes-raw-pdf\"\n",
    "OUT_ROOT = Path(\"../data/resumes_raw_pdf_direct\")\n",
    "PDF_DIR = OUT_ROOT / \"pdfs\"\n",
    "TXT_DIR = OUT_ROOT / \"txt\"\n",
    "FAIL_DIR = OUT_ROOT / \"failures\"\n",
    "REPORT_JSON = OUT_ROOT / \"extraction_report.json\"\n",
    "REPORT_CSV  = OUT_ROOT / \"extraction_report.csv\"\n",
    "\n",
    "for d in (OUT_ROOT, PDF_DIR, TXT_DIR, FAIL_DIR):\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "N_SAMPLES = 100          # change if you want fewer\n",
    "MIN_TEXT_CHARS = 200\n",
    "MIN_ALPHA_RATIO = 0.2\n",
    "\n",
    "token = os.getenv(\"HUGGINGFACE_HUB_TOKEN\")\n",
    "if not token:\n",
    "    raise RuntimeError(\"HUGGINGFACE_HUB_TOKEN missing - set it before running.\")\n",
    "\n",
    "# list files (we already saw this works)\n",
    "all_files = list_repo_files(REPO_ID, repo_type=\"dataset\", token=token)\n",
    "pdf_files = [f for f in all_files if f.lower().endswith(\".pdf\")]\n",
    "print(f\"Found {len(pdf_files)} pdf files in the repo. Will copy/process first {min(N_SAMPLES, len(pdf_files))}.\")\n",
    "\n",
    "# helper: hf_hub_download returns a local cached path when available\n",
    "def get_cached_path(fname):\n",
    "    try:\n",
    "        local = hf_hub_download(repo_id=REPO_ID, filename=fname, repo_type=\"dataset\", token=token)\n",
    "        return Path(local)\n",
    "    except Exception as e:\n",
    "        print(\"hf_hub_download failed for\", fname, \"->\", type(e).__name__, str(e)[:200])\n",
    "        return None\n",
    "\n",
    "report = []\n",
    "to_process = pdf_files[:min(N_SAMPLES, len(pdf_files))]\n",
    "\n",
    "for idx, fname in enumerate(tqdm(to_process, desc=\"files\")):\n",
    "    rec = {\"filename\": Path(fname).name, \"repo_path\": fname, \"downloaded\": False,\n",
    "           \"extraction_method\": None, \"ocr_used\": False, \"error\": None, \"checks\": None,\n",
    "           \"failure_reasons\": [], \"failure_file\": None}\n",
    "    try:\n",
    "        cached = get_cached_path(fname)\n",
    "        if cached is None:\n",
    "            rec[\"error\"] = \"cache_lookup_failed\"\n",
    "            report.append(rec)\n",
    "            continue\n",
    "\n",
    "        # copy cached file to our PDF_DIR with normalized name\n",
    "        tgt = PDF_DIR / f\"resume_{idx:05d}.pdf\"\n",
    "        shutil.copyfile(cached, tgt)\n",
    "        rec[\"downloaded\"] = True\n",
    "\n",
    "        # extract text: pymupdf primary, pdfplumber fallback\n",
    "        txt = \"\"\n",
    "        try:\n",
    "            txt = extract_text_pymupdf(str(tgt))\n",
    "            rec[\"extraction_method\"] = \"pymupdf\"\n",
    "            if len(txt) < MIN_TEXT_CHARS // 2:\n",
    "                txt2 = extract_text_pdfplumber(str(tgt))\n",
    "                if len(txt2) > len(txt):\n",
    "                    txt = txt2\n",
    "                    rec[\"extraction_method\"] += \"+pdfplumber\"\n",
    "        except Exception as e1:\n",
    "            try:\n",
    "                txt = extract_text_pdfplumber(str(tgt))\n",
    "                rec[\"extraction_method\"] = \"pdfplumber\"\n",
    "            except Exception as e2:\n",
    "                rec[\"error\"] = f\"both_extractors_failed: {e1} | {e2}\"\n",
    "                badf = FAIL_DIR / f\"downloaded_but_extractfail_{idx:05d}.txt\"\n",
    "                with open(badf, \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(f\"Cached path: {cached}\\\\nErrors:\\\\n{e1}\\\\n{e2}\")\n",
    "                rec[\"failure_file\"] = str(badf)\n",
    "                report.append(rec)\n",
    "                continue\n",
    "\n",
    "        txt = (txt or \"\").replace(\"\\r\", \"\\n\").strip()\n",
    "        # save text\n",
    "        with open(TXT_DIR / (tgt.stem + \".txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(txt)\n",
    "\n",
    "        # checks\n",
    "        import re\n",
    "        EMAIL_RE = re.compile(r\"[\\w\\.-]+@[\\w\\.-]+\\.\\w+\")\n",
    "        PHONE_RE = re.compile(r\"\\+?\\d[\\d\\s\\-\\(\\)]{6,}\\d\")\n",
    "        SECTION_KEYWORDS = ['education','experience','skills','projects','certifications','publications','summary','objective']\n",
    "\n",
    "        email = bool(EMAIL_RE.search(txt))\n",
    "        phone = bool(PHONE_RE.search(txt))\n",
    "        txt_low = txt.lower()\n",
    "        sections = {kw: (kw in txt_low) for kw in SECTION_KEYWORDS}\n",
    "        any_section = any(sections.values())\n",
    "        letters = sum(c.isalpha() for c in txt)\n",
    "        alpha_ratio = letters / max(1, len(txt))\n",
    "\n",
    "        checks = {'email': email, 'phone': phone, 'sections': sections, 'any_section': any_section,\n",
    "                  'len_chars': len(txt), 'alpha_ratio': alpha_ratio, 'preview': txt[:800].replace(\"\\n\",\"\\\\n\")}\n",
    "        rec[\"checks\"] = checks\n",
    "\n",
    "        failures = []\n",
    "        if rec[\"error\"]:\n",
    "            failures.append(\"extractor_error\")\n",
    "        if checks['len_chars'] < MIN_TEXT_CHARS:\n",
    "            failures.append(\"short_text\")\n",
    "        if checks['alpha_ratio'] < MIN_ALPHA_RATIO:\n",
    "            failures.append(\"low_alpha_ratio\")\n",
    "        if not checks['any_section']:\n",
    "            failures.append(\"no_key_section\")\n",
    "        if not (checks['email'] and checks['phone']):\n",
    "            failures.append(\"missing_contact\")\n",
    "\n",
    "        rec[\"failure_reasons\"] = failures\n",
    "        if failures:\n",
    "            fname_fail = FAIL_DIR / (tgt.stem + \"_failure.txt\")\n",
    "            with open(fname_fail, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(f\"FILENAME: {tgt.name}\\\\nREPO_PATH: {fname}\\\\nEXTRACTION_METHOD: {rec['extraction_method']}\\\\nFAILURE_REASONS: {failures}\\\\n\\\\n---PREVIEW---\\\\n\\\\n\")\n",
    "                f.write(txt)\n",
    "            rec[\"failure_file\"] = str(fname_fail)\n",
    "\n",
    "    except Exception as e:\n",
    "        rec[\"error\"] = f\"fatal:{type(e).__name__}:{e}\"\n",
    "        rec[\"failure_file\"] = None\n",
    "    report.append(rec)\n",
    "\n",
    "# save reports\n",
    "with open(REPORT_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "with open(REPORT_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as csvf:\n",
    "    writer = csv.writer(csvf)\n",
    "    writer.writerow(['filename','repo_path','downloaded','extraction_method','len_chars','alpha_ratio','email','phone','any_key_section','failure_reasons','failure_file','error'])\n",
    "    for r in report:\n",
    "        ch = r.get('checks') or {}\n",
    "        writer.writerow([r.get('filename'), r.get('repo_path'), r.get('downloaded'), r.get('extraction_method'),\n",
    "                         ch.get('len_chars'), round(ch.get('alpha_ratio',0),3), ch.get('email'), ch.get('phone'), ch.get('any_section'),\n",
    "                         ';'.join(r.get('failure_reasons',[])), r.get('failure_file',''), r.get('error','')])\n",
    "\n",
    "# pretty summary\n",
    "from collections import Counter\n",
    "total = len(report)\n",
    "failures = [r for r in report if r.get('failure_reasons')]\n",
    "success = total - len(failures)\n",
    "fail_reasons = Counter()\n",
    "for r in failures:\n",
    "    fail_reasons.update(r.get('failure_reasons',[]))\n",
    "\n",
    "print(\"\\\\n=== SUMMARY ===\")\n",
    "print(\"Total files processed:\", total)\n",
    "print(\"Success (no detected failure reasons):\", success, f\"({round(100*success/total if total else 0,2)}%)\")\n",
    "print(\"Files with failures:\", len(failures))\n",
    "print(\"Failure reasons counts:\", dict(fail_reasons))\n",
    "print(\"Report saved ->\", REPORT_JSON)\n",
    "print(\"PDFs saved  ->\", PDF_DIR)\n",
    "print(\"Text saved  ->\", TXT_DIR)\n",
    "print(\"Failures    ->\", FAIL_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9042f05a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset lhoestq/resumes-raw-pdf-for-ocr split=train (this may take a minute)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Work\\Capstone_Project\\resume-nlp\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\abhis\\.cache\\huggingface\\hub\\datasets--lhoestq--resumes-raw-pdf-for-ocr. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Generating train split: 100%|██████████| 1585/1585 [00:00<00:00, 3149.88 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded. Columns: ['label', 'images', 'text'] Num examples: 1585\n",
      "Using text field: text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "writing text:  32%|███▏      | 500/1585 [00:03<00:07, 139.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 500 text samples to ..\\data\\resumes_preocr\\txt\n",
      "Report: ..\\data\\resumes_preocr\\report_preocr.json\n",
      "Summary CSV: ..\\data\\resumes_preocr\\summary_preocr.csv\n"
     ]
    }
   ],
   "source": [
    "# Load HF dataset properly and write out text files per example\n",
    "# Paste & run in the same notebook environment\n",
    "\n",
    "from datasets import load_dataset\n",
    "from pathlib import Path\n",
    "import json, csv, shutil\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "\n",
    "OUT_ROOT = Path(\"../data/resumes_preocr\")\n",
    "TXT_DIR = OUT_ROOT / \"txt\"\n",
    "PDF_DIR = OUT_ROOT / \"pdfs\"   # may remain empty for this dataset\n",
    "REPORT = OUT_ROOT / \"report_preocr.json\"\n",
    "SUMMARY_CSV = OUT_ROOT / \"summary_preocr.csv\"\n",
    "\n",
    "OUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "TXT_DIR.mkdir(exist_ok=True)\n",
    "PDF_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "REPO_ID = \"lhoestq/resumes-raw-pdf-for-ocr\"\n",
    "SPLIT = \"train\"   # dataset split\n",
    "N_SAMPLES = 500   # adjust to how many you want to extract (max ~ full dataset size ~ 1585)\n",
    "\n",
    "print(f\"Loading dataset {REPO_ID} split={SPLIT} (this may take a minute)...\")\n",
    "ds = load_dataset(REPO_ID, split=SPLIT)\n",
    "\n",
    "print(\"Dataset loaded. Columns:\", ds.column_names, \"Num examples:\", len(ds))\n",
    "\n",
    "# Which field contains text? Common names: 'text'\n",
    "text_field = None\n",
    "for candidate in (\"text\", \"ocr\", \"page_text\", \"raw_text\"):\n",
    "    if candidate in ds.column_names:\n",
    "        text_field = candidate\n",
    "        break\n",
    "# fallback: look for any string column\n",
    "if text_field is None:\n",
    "    for c in ds.column_names:\n",
    "        # sample few rows to check if column is string-like and non-empty\n",
    "        try:\n",
    "            sample = ds[0].get(c)\n",
    "            if isinstance(sample, str):\n",
    "                text_field = c\n",
    "                break\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "if text_field is None:\n",
    "    raise RuntimeError(f\"Couldn't find a text column in dataset. Columns: {ds.column_names}\")\n",
    "\n",
    "print(\"Using text field:\", text_field)\n",
    "\n",
    "report = []\n",
    "count = 0\n",
    "for i, ex in enumerate(tqdm(ds, desc=\"writing text\")):\n",
    "    if count >= N_SAMPLES:\n",
    "        break\n",
    "    txt = ex.get(text_field) or \"\"\n",
    "    # sometimes the text may be empty (filter as desired)\n",
    "    if not txt or not txt.strip():\n",
    "        # skip empty text entries (optionally you can save them)\n",
    "        continue\n",
    "    fname = f\"sample_{count:05d}.txt\"\n",
    "    tgt = TXT_DIR / fname\n",
    "    with open(tgt, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(txt)\n",
    "    report.append({\"index\": i, \"filename\": fname, \"text_len\": len(txt), \"preview\": txt[:500].replace(\"\\n\",\"\\\\n\")})\n",
    "    count += 1\n",
    "\n",
    "# save the report json and a CSV summary with basic checks\n",
    "with open(REPORT, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "\n",
    "import re\n",
    "EMAIL_RE = re.compile(r\"[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+\")\n",
    "PHONE_RE = re.compile(r\"(?:\\+?\\d{1,3}[-.\\s]?)?(?:\\(?\\d{2,4}\\)?[-.\\s]?)?\\d{6,12}\")\n",
    "SECTIONS = ['education','experience','skills','projects','certifications','publications','summary','objective','work experience']\n",
    "\n",
    "rows = []\n",
    "for r in report:\n",
    "    txt = (TXT_DIR / r['filename']).read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    email = bool(EMAIL_RE.search(txt))\n",
    "    phone = bool(PHONE_RE.search(txt))\n",
    "    any_section = any(k in txt.lower() for k in SECTIONS)\n",
    "    rows.append({\n",
    "        \"file\": r['filename'],\n",
    "        \"chars\": len(txt),\n",
    "        \"email\": email,\n",
    "        \"phone\": phone,\n",
    "        \"any_section\": any_section,\n",
    "        \"preview\": txt[:400].replace(\"\\n\",\"\\\\n\")\n",
    "    })\n",
    "\n",
    "# write CSV\n",
    "import csv\n",
    "with open(SUMMARY_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=list(rows[0].keys()) if rows else [\"file\"])\n",
    "    writer.writeheader()\n",
    "    for row in rows:\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(f\"Saved {len(report)} text samples to {TXT_DIR}\")\n",
    "print(\"Report:\", REPORT)\n",
    "print(\"Summary CSV:\", SUMMARY_CSV)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23644b73",
   "metadata": {},
   "source": [
    "##### Multilingual NER using a different model than SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f3e5e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Work\\Capstone_Project\\resume-nlp\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "d:\\Work\\Capstone_Project\\resume-nlp\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\abhis\\.cache\\huggingface\\hub\\models--Davlan--xlm-roberta-base-ner-hrl. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Device set to use cpu\n",
      "100%|██████████| 500/500 [02:33<00:00,  3.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved fixed names to: ..\\data\\resumes_preocr\\ner_fixed_names.json\n",
      "Sample: Nguyen Dang Binh -> Nguyen Dang Binh\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# === Multilingual NER for PERSON names (fixes name extraction) ===\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "ROOT = Path(\"../data/resumes_preocr\")\n",
    "TXT_DIR = ROOT / \"txt\"\n",
    "NER_ENH = ROOT / \"ner_enhanced.json\"\n",
    "OUT = ROOT / \"ner_fixed_names.json\"\n",
    "\n",
    "# load previous enhanced NER\n",
    "with open(NER_ENH, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "MODEL = \"Davlan/xlm-roberta-base-ner-hrl\"   # multilingual NER (very good on PERSON)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "model = AutoModelForTokenClassification.from_pretrained(MODEL)\n",
    "ner_pipe = pipeline(\"token-classification\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
    "\n",
    "def load_text(fname):\n",
    "    p = TXT_DIR / fname\n",
    "    return p.read_text(encoding=\"utf-8\", errors=\"ignore\") if p.exists() else \"\"\n",
    "\n",
    "def extract_person_name(text):\n",
    "    \"\"\"Return first PERSON detected by multilingual NER.\"\"\"\n",
    "    try:\n",
    "        ents = ner_pipe(text[:800])  # only beginning of CV for speed\n",
    "    except:\n",
    "        return None\n",
    "    persons = [e[\"word\"] for e in ents if e[\"entity_group\"] == \"PER\"]\n",
    "    return persons[0] if persons else None\n",
    "\n",
    "fixed = []\n",
    "for entry in tqdm(data):\n",
    "    txt = load_text(entry[\"file\"])\n",
    "    person = extract_person_name(txt)\n",
    "    entry[\"primary_name_fixed\"] = person if person else entry[\"primary_name\"]\n",
    "    fixed.append(entry)\n",
    "\n",
    "with open(OUT, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(fixed, f, indent=2)\n",
    "\n",
    "print(\"Saved fixed names to:\", OUT)\n",
    "print(\"Sample:\", fixed[0][\"primary_name\"], \"->\", fixed[0][\"primary_name_fixed\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6e21444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cleaned NER with 'primary_name_clean' -> ..\\data\\resumes_preocr\\ner_fixed_names_clean.json  (fixed 83 names)\n",
      "sample_00000.txt -> Nguyen Dang Binh -> Nguyen Dang Binh -> Nguyen Dang Binh\n",
      "sample_00001.txt -> Phường Lái Thiêu -> Phường Lái Thiêu -> Phường Lái Thiêu\n",
      "sample_00002.txt -> Kumar Tiwari -> Akhilesh Kumar Tiwari -> Akhilesh Kumar Tiwari\n",
      "sample_00003.txt -> Thiện Chí Trần\n",
      "IOS -> Thiện Chí Trần -> Thiện Chí Trần\n",
      "sample_00004.txt -> Địa -> ĐINH PHƯƠNG HUYỀN -> ĐINH PHƯƠNG HUYỀN\n",
      "sample_00005.txt -> nghiệp chuyên -> Lê Trung Giang -> Lê Trung Giang\n"
     ]
    }
   ],
   "source": [
    "# 1) Name-cleaning heuristics\n",
    "import json, re\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path(\"../data/resumes_preocr\")\n",
    "TXT_DIR = ROOT / \"txt\"\n",
    "NER_FIXED = ROOT / \"ner_fixed_names.json\"   # or ner_fixed_names_spacy.json if you used spaCy fallback\n",
    "OUT_CLEAN = ROOT / \"ner_fixed_names_clean.json\"\n",
    "\n",
    "BAD_NAME_PATTERNS = [\n",
    "    re.compile(r\"^\\s*$\"),\n",
    "    re.compile(r\"^(?:ph|pv|hr|cv)$\", re.I),\n",
    "    re.compile(r\"^\\d{4}[\\s\\-:]\\d{4}$\"),    # year ranges\n",
    "    re.compile(r\"^(?:jun|jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec)\\b\", re.I),\n",
    "    re.compile(r\"^(?:\\d{1,2}[:/.-])\"),     # starts with numbers like dates\n",
    "    re.compile(r\"^[\\W_]+$\")                # punctuation-only\n",
    "]\n",
    "\n",
    "def looks_bad_name(s):\n",
    "    if s is None: return True\n",
    "    s = s.strip()\n",
    "    if len(s) < 2 or len(s.split()) > 6 and len(s) > 80:\n",
    "        # too short or suspiciously long\n",
    "        return True if len(s) < 2 or len(s) > 80 else False\n",
    "    low = s.lower()\n",
    "    if any(p.search(s) for p in BAD_NAME_PATTERNS):\n",
    "        return True\n",
    "    # contains words that are headings\n",
    "    if any(k in low for k in (\"experience\",\"objective\",\"summary\",\"profile\",\"address\",\"phone\",\"email\",\"marital\",\"date\",\"present\",\"current\",\"managed\")):\n",
    "        return True\n",
    "    # contains many digits -> bad\n",
    "    if sum(c.isdigit() for c in s) / max(1,len(s)) > 0.15:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def fallback_name_from_text(text):\n",
    "    # return first plausible short line near top that isn't a heading\n",
    "    for ln in text.splitlines()[:12]:\n",
    "        ln = ln.strip()\n",
    "        if not ln: continue\n",
    "        if len(ln) > 100: continue\n",
    "        low = ln.lower()\n",
    "        if any(k in low for k in (\"objective\",\"cv\",\"curriculum\",\"resume\",\"skills\",\"experience\",\"education\",\"address\",\"phone\",\"email\",\"profile\",\"summary\",\"contact\")):\n",
    "            continue\n",
    "        # filter lines that are mostly dates/locations or bullets\n",
    "        if re.match(r\"^[\\-\\u2022\\•\\*]\\s*\", ln): \n",
    "            # remove bullet char and keep going\n",
    "            ln = re.sub(r\"^[\\-\\u2022\\•\\*]\\s*\", \"\", ln).strip()\n",
    "        if len(ln) < 3 or len(ln) > 80:\n",
    "            continue\n",
    "        # good candidate\n",
    "        return ln\n",
    "    return None\n",
    "\n",
    "# load\n",
    "with open(NER_FIXED, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "cleaned = []\n",
    "fix_count = 0\n",
    "for entry in data:\n",
    "    orig = entry.get(\"primary_name_fixed\") or entry.get(\"primary_name\") or \"\"\n",
    "    if looks_bad_name(orig):\n",
    "        txt_path = TXT_DIR / entry[\"file\"]\n",
    "        txt = txt_path.read_text(encoding=\"utf-8\", errors=\"ignore\") if txt_path.exists() else \"\"\n",
    "        fallback = fallback_name_from_text(txt)\n",
    "        if fallback:\n",
    "            entry[\"primary_name_clean\"] = fallback\n",
    "            fix_count += 1\n",
    "        else:\n",
    "            entry[\"primary_name_clean\"] = None\n",
    "    else:\n",
    "        entry[\"primary_name_clean\"] = orig\n",
    "    cleaned.append(entry)\n",
    "\n",
    "with open(OUT_CLEAN, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(cleaned, f, indent=2)\n",
    "\n",
    "print(f\"Saved cleaned NER with 'primary_name_clean' -> {OUT_CLEAN}  (fixed {fix_count} names)\")\n",
    "# show a small sample mapping\n",
    "for e in cleaned[:6]:\n",
    "    print(e[\"file\"], \"->\", e.get(\"primary_name\"), \"->\", e.get(\"primary_name_fixed\"), \"->\", e.get(\"primary_name_clean\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf75898",
   "metadata": {},
   "source": [
    "### Step 3: Generating BERT sentence embedding for matching cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b964f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: all-mpnet-base-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding (clean names): 100%|██████████| 500/500 [04:36<00:00,  1.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regenerated embeddings for 500 resumes; emb_index updated at ..\\data\\resumes_preocr\\emb_index.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 2) Regenerate embeddings using primary_name_clean\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np, json\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "ROOT = Path(\"../data/resumes_preocr\")\n",
    "TXT_DIR = ROOT / \"txt\"\n",
    "CLEAN = ROOT / \"ner_fixed_names_clean.json\"\n",
    "EMB_DIR = ROOT / \"embeddings\"\n",
    "EMB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODEL_NAME = \"all-mpnet-base-v2\"   # good BERT-like sentence embedder\n",
    "\n",
    "# load cleaned metadata\n",
    "with open(CLEAN, \"r\", encoding=\"utf-8\") as f:\n",
    "    meta = json.load(f)\n",
    "\n",
    "model = SentenceTransformer(MODEL_NAME)\n",
    "print(\"Model loaded:\", MODEL_NAME)\n",
    "\n",
    "def load_text(fname, n_chars=1600):\n",
    "    p = TXT_DIR / fname\n",
    "    return p.read_text(encoding=\"utf-8\", errors=\"ignore\")[:n_chars] if p.exists() else \"\"\n",
    "\n",
    "count = 0\n",
    "for entry in tqdm(meta, desc=\"Embedding (clean names)\"):\n",
    "    fname = entry[\"file\"]\n",
    "    name = entry.get(\"primary_name_clean\") or \"\"\n",
    "    snippet = load_text(fname)\n",
    "    skills = \", \".join(entry.get(\"skills\",[])[:12])\n",
    "    orgs = \", \".join(entry.get(\"orgs\",[])[:6])\n",
    "    combined = (name + \"\\n\\n\" + snippet + \"\\n\\nSkills: \" + skills + \"\\nOrgs: \" + orgs).strip()\n",
    "    if not combined:\n",
    "        combined = snippet or \" \"\n",
    "    emb = model.encode(combined, show_progress_bar=False)\n",
    "    npy_path = EMB_DIR / (Path(fname).stem + \".npy\")\n",
    "    np.save(npy_path, emb)\n",
    "    count += 1\n",
    "\n",
    "# write emb_index.json\n",
    "emb_index = [{\"file\": e[\"file\"], \"npy\": str(EMB_DIR / (Path(e[\"file\"]).stem + \".npy\"))} for e in meta]\n",
    "with open(ROOT / \"emb_index.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(emb_index, f, indent=2)\n",
    "\n",
    "print(\"Regenerated embeddings for\", count, \"resumes; emb_index updated at\", ROOT / \"emb_index.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4495a100",
   "metadata": {},
   "source": [
    "### Step 4: Scoring resumes using custom scoring heurisitcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3650eb9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaned Refined Top 10 resumes:\n",
      "101.219 | sim=0.476 (0.738) | nameQ=1.00 | exp=25 | contact=found   | skills= 9 | LE HOANG                                 | sample_00419.txt\n",
      " 99.868 | sim=0.416 (0.708) | nameQ=1.00 | exp=17 | contact=found   | skills=10 | Hoàng Quang Hưng                         | sample_00218.txt\n",
      " 98.123 | sim=0.339 (0.669) | nameQ=1.00 | exp=40 | contact=found   | skills= 8 | Doan Minh Hoang                          | sample_00220.txt\n",
      " 97.977 | sim=0.332 (0.666) | nameQ=1.00 | exp=17 | contact=found   | skills= 9 | Nguyen Ngoc Dang                         | sample_00159.txt\n",
      " 96.126 | sim=0.517 (0.758) | nameQ=1.00 | exp=10 | contact=found   | skills= 9 | NGUYEN VAN HUONG\n",
      "Day                     | sample_00261.txt\n",
      " 92.210 | sim=0.432 (0.716) | nameQ=1.00 | exp=16 | contact=missing | skills=11 | Chung Vi Huy                             | sample_00267.txt\n",
      " 91.011 | sim=0.378 (0.689) | nameQ=1.00 | exp=15 | contact=missing | skills= 6 | DINH NGUYEN DANG KHOA                    | sample_00236.txt\n",
      " 90.233 | sim=0.433 (0.716) | nameQ=1.00 | exp=16 | contact=found   | skills= 4 | Van Luong                                | sample_00465.txt\n",
      " 89.771 | sim=0.377 (0.688) | nameQ=1.00 | exp=14 | contact=missing | skills= 6 | PHAN MINH THỌ                            | sample_00180.txt\n",
      " 86.899 | sim=0.480 (0.740) | nameQ=1.00 | exp= 3 | contact=found   | skills=11 | Trần Dưỡng                               | sample_00444.txt\n",
      "\n",
      "Saved: ..\\data\\resumes_preocr\\resume_scores_refined_clean.json and ..\\data\\resumes_preocr\\leaderboard_refined_clean.csv\n"
     ]
    }
   ],
   "source": [
    "# 3) Recompute refined scoring using cleaned names\n",
    "import json, re, csv\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "ROOT = Path(\"../data/resumes_preocr\")\n",
    "TXT_DIR = ROOT / \"txt\"\n",
    "CLEAN = ROOT / \"ner_fixed_names_clean.json\"\n",
    "EMB_INDEX = ROOT / \"emb_index.json\"\n",
    "OUT_JSON = ROOT / \"resume_scores_refined_clean.json\"\n",
    "OUT_CSV = ROOT / \"leaderboard_refined_clean.csv\"\n",
    "\n",
    "# loads\n",
    "with open(CLEAN, \"r\", encoding=\"utf-8\") as f: meta = json.load(f)\n",
    "with open(EMB_INDEX, \"r\", encoding=\"utf-8\") as f: emb_index = json.load(f)\n",
    "\n",
    "file_to_entry = {e[\"file\"]: e for e in meta}\n",
    "file_to_npy = {e[\"file\"]: e[\"npy\"] for e in emb_index}\n",
    "\n",
    "# Job description embedding (same JD or modify)\n",
    "JOB_DESC = \"\"\"\n",
    "We are hiring an NLP Engineer with strong Python skills, proven experience with PyTorch and Transformers,\n",
    "solid foundations in NLP methods (tokenization, attention, sequence models), and experience deploying models to production.\n",
    "Experience with ML pipelines, REST APIs, and cloud deployment (AWS/GCP) is a plus.\n",
    "\"\"\"\n",
    "MODEL_NAME = \"all-mpnet-base-v2\"\n",
    "model = SentenceTransformer(MODEL_NAME)\n",
    "job_emb = model.encode(JOB_DESC)\n",
    "\n",
    "# small helper to estimate experience (reuse previous helper)\n",
    "YEARS_PHRASE_RE = re.compile(r\"(\\d+)\\s+(?:years|yrs)\\b\", re.I)\n",
    "def estimate_experience(text):\n",
    "    m = YEARS_PHRASE_RE.search(text)\n",
    "    if m:\n",
    "        try: return min(int(m.group(1)), 40)\n",
    "        except: pass\n",
    "    # fallback: look for 2010-2014 style\n",
    "    m2 = re.search(r\"((19|20)\\d{2})\\s*[\\-–—]\\s*((19|20)\\d{2})\", text)\n",
    "    if m2:\n",
    "        try:\n",
    "            return min(int(m2.group(3)) - int(m2.group(1)), 40)\n",
    "        except: pass\n",
    "    return 0\n",
    "\n",
    "# scoring weights (tweak as needed)\n",
    "WEIGHT_CONTACT = 8\n",
    "WEIGHT_SKILL = 5\n",
    "WEIGHT_SIM = 45\n",
    "WEIGHT_NAME_QUALITY = 6\n",
    "WEIGHT_LANG_MATCH = 6\n",
    "WEIGHT_EXP_PER_YEAR = 1.2\n",
    "CAP_SKILLS = 6\n",
    "CAP_EXP = 15\n",
    "\n",
    "results = []\n",
    "for rec in emb_index:\n",
    "    fname = rec[\"file\"]\n",
    "    npy = rec[\"npy\"]\n",
    "    if fname not in file_to_entry: continue\n",
    "    entry = file_to_entry[fname]\n",
    "    try:\n",
    "        emb = np.load(npy)\n",
    "    except Exception:\n",
    "        continue\n",
    "    raw_sim = float(cosine_similarity([emb], [job_emb])[0,0])\n",
    "    sim_norm = (raw_sim + 1)/2.0\n",
    "    contact_bonus = WEIGHT_CONTACT if entry.get(\"contact_status\") == \"found\" else 0\n",
    "    n_skills = len(entry.get(\"skills\",[]))\n",
    "    skill_score = min(CAP_SKILLS, n_skills) * WEIGHT_SKILL\n",
    "    # name_quality heuristic: prefer names with at least 2 alphabetic words\n",
    "    name = entry.get(\"primary_name_clean\") or \"\"\n",
    "    name_quality = 1.0 if (len(name.split())>=2 and re.search(r\"[A-Za-z\\u00C0-\\u017F]\", name)) else 0.6 if len(name.split())==1 and re.search(r\"[A-Za-z\\u00C0-\\u017F]\", name) else 0\n",
    "    name_score = name_quality * WEIGHT_NAME_QUALITY\n",
    "    # language bonus\n",
    "    lang_bonus = WEIGHT_LANG_MATCH if entry.get(\"language\",\"\") == \"en\" else 0\n",
    "    # experience\n",
    "    text = (TXT_DIR / fname).read_text(encoding=\"utf-8\", errors=\"ignore\") if (TXT_DIR/ fname).exists() else \"\"\n",
    "    exp = estimate_experience(text)\n",
    "    exp_score = min(exp, CAP_EXP) * WEIGHT_EXP_PER_YEAR\n",
    "    sim_score = sim_norm * WEIGHT_SIM\n",
    "    total = contact_bonus + skill_score + sim_score + name_score + lang_bonus + exp_score\n",
    "    results.append({\n",
    "        \"file\": fname,\n",
    "        \"primary_name\": name,\n",
    "        \"contact_status\": entry.get(\"contact_status\",\"missing\"),\n",
    "        \"n_skills\": n_skills,\n",
    "        \"sim_raw\": round(raw_sim,4),\n",
    "        \"sim_norm\": round(sim_norm,4),\n",
    "        \"name_quality\": round(name_quality,3),\n",
    "        \"exp_years\": exp,\n",
    "        \"score\": round(total,3),\n",
    "        \"top_skills\": entry.get(\"skills\",[])[:8],\n",
    "        \"preview\": entry.get(\"original_preview\",\"\")[:300].replace(\"\\n\",\"\\\\n\")\n",
    "    })\n",
    "\n",
    "results_sorted = sorted(results, key=lambda x: x[\"score\"], reverse=True)\n",
    "\n",
    "# save\n",
    "with open(OUT_JSON, \"w\", encoding=\"utf-8\") as f: json.dump(results_sorted, f, indent=2)\n",
    "with open(OUT_CSV, \"w\", newline='', encoding=\"utf-8\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\"score\",\"sim_raw\",\"sim_norm\",\"name_quality\",\"exp_years\",\"contact_status\",\"n_skills\",\"primary_name\",\"file\",\"top_skills\",\"preview\"])\n",
    "    writer.writeheader()\n",
    "    for r in results_sorted: writer.writerow({k:r.get(k,\"\") for k in writer.fieldnames})\n",
    "\n",
    "# print top 10\n",
    "print(\"\\nCleaned Refined Top 10 resumes:\")\n",
    "for s in results_sorted[:10]:\n",
    "    print(f\"{s['score']:7.3f} | sim={s['sim_raw']:.3f} ({s['sim_norm']:.3f}) | nameQ={s['name_quality']:.2f} | exp={s['exp_years']:2} | contact={s['contact_status']:<7} | skills={s['n_skills']:2} | {s['primary_name'][:40]:40} | {s['file']}\")\n",
    "print(\"\\nSaved:\", OUT_JSON, \"and\", OUT_CSV)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b171658",
   "metadata": {},
   "source": [
    "##### OLD NER Technique using SpaCy -> Drawback: Only english compatible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d9a90f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 500 text files with spaCy NER...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NER: 100%|██████████| 500/500 [03:54<00:00,  2.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== NER SUMMARY ===\n",
      "Files processed: 500\n",
      "Emails found: 1\n",
      "Phones found: 43\n",
      "Files with detected skills: 499\n",
      "Average text length: 3762 chars\n",
      "Saved ner extract -> ..\\data\\resumes_preocr\\ner_extract.json\n",
      "\n",
      "Sample entry (first file):\n",
      "{'chars': 2812,\n",
      " 'emails': [],\n",
      " 'file': 'sample_00000.txt',\n",
      " 'locations': ['Hanoi', 'Robot', 'Tkinter', 'Taiwan'],\n",
      " 'names': ['Nguyen Dang Binh', 'Luster LightTech', 'Debug'],\n",
      " 'orgs': ['AI/Computer Vision Engineer\\nAddress', 'Vision Software Senior', 'BacNinh', 'AOI', 'Luxshare-ICT',\n",
      "          'Medical Image Segmentation'],\n",
      " 'phones': [],\n",
      " 'preview': 'Nguyen Dang Binh – AI/Computer Vision Engineer\\\\nAddress: Bac Ninh City\\\\nE '\n",
      "            'mail:\\\\nMobile/Zalo:\\\\nBirthday: Jan 8th 1986\\\\nK EY STRENGTHS\\\\nMachine vision \\uf020Team '\n",
      "            'Leader\\\\n\\uf0a7\\uf020\\\\n\\uf0a7\\uf020AI engineer Automation software\\\\nP ROFESSIONAL EXPERIENCE\\\\nVision '\n",
      "            'Software Senior (Aug 2023 – Now): Luster LightTech, BacNinh, VN:\\\\n\\uf0d8 Develop the low-code software '\n",
      "            'from internal platforms for vision projects.\\\\nDebug the software at AOI machine at Luxshare-ICT, '\n",
      "            'Goerteck factory.\\\\n\\uf0d8\\\\nSoftware Engineer Senior Leader (Mar 2020 – May 2023): Electronic ACE '\n",
      "            'Antenna\\\\nCompany | Hanoi, VN:\\\\n\\uf0d8 Measurement and detection defect software project: Develop t',\n",
      " 'skills': ['c', 'c++', 'deep learning', 'machine learning', 'python', 'pytorch', 'tensorflow']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 3 (run): SpaCy NER + rule-based extraction\n",
    "import json, re, sys\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ensure spaCy model installed\n",
    "try:\n",
    "    import spacy\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except Exception as e:\n",
    "    print(\"spaCy model en_core_web_sm not found. Install it and re-run:\")\n",
    "    print(\"    python -m spacy download en_core_web_sm\")\n",
    "    raise\n",
    "\n",
    "ROOT = Path(\"../data/resumes_preocr\")\n",
    "TXT_DIR = ROOT / \"txt\"\n",
    "OUT_EXTRACT = ROOT / \"ner_extract.json\"\n",
    "\n",
    "EMAIL_RE = re.compile(r\"[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+\")\n",
    "PHONE_RE = re.compile(r\"(?:\\+?\\d{1,3}[-.\\s]?)?(?:\\(?\\d{2,4}\\)?[-.\\s]?)?\\d{6,12}\")\n",
    "\n",
    "# small initial skills vocabulary — expand with your domain\n",
    "SKILLS = {\n",
    "    \"python\",\"java\",\"c++\",\"c\",\"sql\",\"nlp\",\"natural language processing\",\"deep learning\",\n",
    "    \"machine learning\",\"tensorflow\",\"pytorch\",\"react\",\"docker\",\"kubernetes\",\"git\",\n",
    "    \"aws\",\"gcp\",\"azure\",\"linux\",\"pandas\",\"numpy\",\"scikit-learn\",\"keras\"\n",
    "}\n",
    "\n",
    "def extract_skills(text):\n",
    "    tl = text.lower()\n",
    "    found = []\n",
    "    for s in SKILLS:\n",
    "        if s in tl:\n",
    "            found.append(s)\n",
    "    return sorted(found)\n",
    "\n",
    "results = []\n",
    "files = sorted(TXT_DIR.glob(\"*.txt\"))\n",
    "print(\"Processing\", len(files), \"text files with spaCy NER...\")\n",
    "\n",
    "for p in tqdm(files, desc=\"NER\"):\n",
    "    txt = p.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    doc = nlp(txt)\n",
    "    names = []\n",
    "    orgs = []\n",
    "    gpes = []\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PERSON\":\n",
    "            names.append(ent.text)\n",
    "        elif ent.label_ in (\"ORG\",\"NORP\"):\n",
    "            orgs.append(ent.text)\n",
    "        elif ent.label_ == \"GPE\":\n",
    "            gpes.append(ent.text)\n",
    "    emails = EMAIL_RE.findall(txt)\n",
    "    phones = PHONE_RE.findall(txt)\n",
    "    skills = extract_skills(txt)\n",
    "    results.append({\n",
    "        \"file\": p.name,\n",
    "        \"names\": list(dict.fromkeys(names))[:3],\n",
    "        \"orgs\": list(dict.fromkeys(orgs))[:6],\n",
    "        \"locations\": list(dict.fromkeys(gpes))[:4],\n",
    "        \"emails\": emails,\n",
    "        \"phones\": phones,\n",
    "        \"skills\": skills,\n",
    "        \"chars\": len(txt),\n",
    "        \"preview\": txt[:600].replace(\"\\n\",\"\\\\n\")\n",
    "    })\n",
    "\n",
    "# save results\n",
    "with open(OUT_EXTRACT, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "# print quick stats\n",
    "total = len(results)\n",
    "emails_found = sum(1 for r in results if r[\"emails\"])\n",
    "phones_found = sum(1 for r in results if r[\"phones\"])\n",
    "skills_found = sum(1 for r in results if r[\"skills\"])\n",
    "avg_len = sum(r[\"chars\"] for r in results) / total if total else 0\n",
    "\n",
    "print(\"\\n=== NER SUMMARY ===\")\n",
    "print(f\"Files processed: {total}\")\n",
    "print(f\"Emails found: {emails_found}\")\n",
    "print(f\"Phones found: {phones_found}\")\n",
    "print(f\"Files with detected skills: {skills_found}\")\n",
    "print(f\"Average text length: {avg_len:.0f} chars\")\n",
    "print(\"Saved ner extract ->\", OUT_EXTRACT)\n",
    "print(\"\\nSample entry (first file):\")\n",
    "if results:\n",
    "    import pprint\n",
    "    pprint.pprint(results[0], compact=True, width=120)\n",
    "else:\n",
    "    print(\"No text files found in\", TXT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e745214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote enhanced NER -> ..\\data\\resumes_preocr\\ner_enhanced.json\n",
      "Total entries: 500\n",
      "Contact found: 152\n",
      "States: {'lang:en': 183, 'lang:vi': 315, 'has_phone': 152, 'lang:nl': 1, 'lang:zh-cn': 1, 'has_email': 1}\n",
      "\n",
      "Sample enhanced entry (first):\n",
      "{'chars': 2812,\n",
      " 'contact_status': 'missing',\n",
      " 'emails': [],\n",
      " 'file': 'sample_00000.txt',\n",
      " 'github': [],\n",
      " 'language': 'en',\n",
      " 'linkedin': [],\n",
      " 'orgs': ['BacNinh', 'AOI', 'Luxshare-ICT', 'Medical Image Segmentation'],\n",
      " 'original_preview': 'Nguyen Dang Binh – AI/Computer Vision '\n",
      "                     'Engineer\\\\nAddress: Bac Ninh City\\\\nE '\n",
      "                     'mail:\\\\nMobile/Zalo:\\\\nBirthday: Jan 8th 1986\\\\nK EY '\n",
      "                     'STRENGTHS\\\\nMachine vision \\uf020Team '\n",
      "                     'Leader\\\\n\\uf0a7\\uf020\\\\n\\uf0a7\\uf020AI engineer '\n",
      "                     'Automation software\\\\nP ROFESSIONAL EXPERIENCE\\\\nVision '\n",
      "                     'Software Senior (Aug 2023 – Now): Luster LightTech, '\n",
      "                     'BacNinh, VN:\\\\n\\uf0d8 Develop the low-code software from '\n",
      "                     'internal platforms for vision projects.\\\\nDebug the '\n",
      "                     'software at AOI machine at Luxshare-ICT, Goerteck '\n",
      "                     'factory.\\\\n\\uf0d8\\\\nSoftware Engineer Senior Leader (Mar '\n",
      "                     '2020 – May 2023): Electronic ACE Antenna\\\\nCompany | '\n",
      "                     'Hanoi, VN:\\\\n\\uf0d8 Measurement and detection defect '\n",
      "                     'software project: Develop t',\n",
      " 'phones': [],\n",
      " 'primary_name': 'Nguyen Dang Binh',\n",
      " 'skills': ['c',\n",
      "            'c#',\n",
      "            'c++',\n",
      "            'communication',\n",
      "            'computer vision',\n",
      "            'deep learning',\n",
      "            'machine learning',\n",
      "            'mlops',\n",
      "            'opencv',\n",
      "            'python',\n",
      "            'pytorch',\n",
      "            'tensorflow']}\n"
     ]
    }
   ],
   "source": [
    "# Post-process NER outputs: normalize names, improve phone/email detection, expand skills, detect language\n",
    "import json, re, os\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "\n",
    "ROOT = Path(\"../data/resumes_preocr\")\n",
    "NER_IN = ROOT / \"ner_extract.json\"\n",
    "NER_OUT = ROOT / \"ner_enhanced.json\"\n",
    "TXT_DIR = ROOT / \"txt\"\n",
    "\n",
    "# load NER results\n",
    "with open(NER_IN, \"r\", encoding=\"utf-8\") as f:\n",
    "    ner = json.load(f)\n",
    "\n",
    "# stronger regexes\n",
    "EMAIL_RE = re.compile(r\"[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+\")\n",
    "# phone: look for sequences of digits with common separators, allow country code, require 7-15 digits total\n",
    "PHONE_RE = re.compile(r\"(?:\\+?\\d{1,3}[-.\\s]?)?(?:\\(?\\d{1,4}\\)?[-.\\s]?)?(?:\\d[-.\\s]?){6,14}\\d\")\n",
    "LINKEDIN_RE = re.compile(r\"(linkedin\\.com/[A-Za-z0-9_\\-./]+|linkedin:[A-Za-z0-9_\\-/]+)\", re.I)\n",
    "GITHUB_RE = re.compile(r\"(github\\.com/[A-Za-z0-9_.\\-]+|github:[A-Za-z0-9_.\\-]+)\", re.I)\n",
    "\n",
    "# expanded skills vocabulary (extendable)\n",
    "MORE_SKILLS = [\n",
    "    \"python\",\"java\",\"c++\",\"c\",\"c#\",\"sql\",\"nlp\",\"natural language processing\",\"deep learning\",\n",
    "    \"machine learning\",\"tensorflow\",\"pytorch\",\"keras\",\"scikit-learn\",\"spark\",\"hadoop\",\n",
    "    \"pandas\",\"numpy\",\"matplotlib\",\"seaborn\",\"docker\",\"kubernetes\",\"aws\",\"gcp\",\"azure\",\n",
    "    \"react\",\"node.js\",\"express\",\"flask\",\"django\",\"rest api\",\"graphql\",\"git\",\"linux\",\n",
    "    \"bash\",\"mlops\",\"computer vision\",\"opencv\",\"pandas\",\"communication\",\"leadership\",\n",
    "    \"excel\",\"tableau\",\"power bi\",\"spark\",\"hive\",\"bigquery\",\"seo\",\"marketing\",\"sales\"\n",
    "]\n",
    "# normalize to lowercase for substring matching\n",
    "MORE_SKILLS = list(dict.fromkeys([s.lower() for s in MORE_SKILLS]))\n",
    "\n",
    "# optional fuzzy matching: use rapidfuzz if installed\n",
    "try:\n",
    "    from rapidfuzz import process, fuzz\n",
    "    FUZZY_AVAILABLE = True\n",
    "except Exception:\n",
    "    FUZZY_AVAILABLE = False\n",
    "\n",
    "# helper: load raw text for a sample file if present\n",
    "def load_text_for_file(filename):\n",
    "    txt_path = TXT_DIR / filename\n",
    "    if txt_path.exists():\n",
    "        return txt_path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    return \"\"\n",
    "\n",
    "# helper: clean orgs (remove entries that look like addresses or job titles)\n",
    "JOB_TITLE_KEYWORDS = set([\"engineer\",\"developer\",\"manager\",\"senior\",\"lead\",\"intern\",\"assistant\",\"consultant\",\n",
    "                          \"officer\",\"analyst\",\"specialist\",\"architect\",\"director\",\"president\",\"coordinator\",\n",
    "                          \"supervisor\"])\n",
    "def clean_org_list(orgs):\n",
    "    cleaned = []\n",
    "    for o in orgs:\n",
    "        s = o.strip()\n",
    "        # skip if empty or obviously a sentence fragment or contains newline markers\n",
    "        if not s or len(s) < 2:\n",
    "            continue\n",
    "        # skip if it contains 'address' or 'mobile' or 'email' (likely not org)\n",
    "        low = s.lower()\n",
    "        if any(x in low for x in (\"address\",\"mobile\",\"email\",\"phone\",\"birthday\",\"birth\",\"cv\",\"c.v\",\"objective\",\"profile\")):\n",
    "            continue\n",
    "        # skip if it's too long garbage with many punctuation characters\n",
    "        punct_ratio = sum(1 for ch in s if not ch.isalnum() and not ch.isspace()) / max(1,len(s))\n",
    "        if punct_ratio > 0.25 and len(s) < 50:\n",
    "            continue\n",
    "        # optionally skip entries that are likely job titles (we want organizations)\n",
    "        if any(jk in low for jk in JOB_TITLE_KEYWORDS):\n",
    "            # allow if it contains a comma and an apparent company name later\n",
    "            if \",\" not in s and len(s.split()) < 6:\n",
    "                # likely a job title, skip\n",
    "                continue\n",
    "        cleaned.append(s)\n",
    "    # dedupe keeping order\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for o in cleaned:\n",
    "        if o not in seen:\n",
    "            out.append(o); seen.add(o)\n",
    "    return out\n",
    "\n",
    "# helper: expand skills by substring (and fuzzy if available)\n",
    "def extract_skills_from_text(text):\n",
    "    tl = text.lower()\n",
    "    found = set()\n",
    "    for s in MORE_SKILLS:\n",
    "        if s in tl:\n",
    "            found.add(s)\n",
    "    # fuzzy: if available, match tokens\n",
    "    if FUZZY_AVAILABLE and not found:\n",
    "        # take top fuzzy matches for single-word tokens\n",
    "        tokens = set(re.findall(r\"[A-Za-z0-9+#\\.\\-]+\", tl))\n",
    "        for tok in tokens:\n",
    "            best = process.extractOne(tok, MORE_SKILLS, scorer=fuzz.partial_ratio)\n",
    "            if best and best[1] >= 90:\n",
    "                found.add(best[0])\n",
    "    return sorted(found)\n",
    "\n",
    "# try langdetect if available\n",
    "try:\n",
    "    from langdetect import detect, DetectorFactory\n",
    "    DetectorFactory.seed = 0\n",
    "    LANGDET_AVAILABLE = True\n",
    "except Exception:\n",
    "    LANGDET_AVAILABLE = False\n",
    "\n",
    "# process and enhance\n",
    "enhanced = []\n",
    "counters = Counter()\n",
    "for entry in ner:\n",
    "    fname = entry.get(\"file\")\n",
    "    raw_text = load_text_for_file(fname)\n",
    "    # combine original fields and raw text for better detection\n",
    "    emails = entry.get(\"emails\", []) or EMAIL_RE.findall(raw_text)\n",
    "    phones = entry.get(\"phones\", []) or PHONE_RE.findall(raw_text)\n",
    "    linkedin = LINKEDIN_RE.findall(raw_text) or LINKEDIN_RE.findall(\" \".join(entry.get(\"orgs\",[])))\n",
    "    github = GITHUB_RE.findall(raw_text) or GITHUB_RE.findall(\" \".join(entry.get(\"orgs\",[])))\n",
    "    # choose primary name: prefer first PERSON entity; if no person, try first line of text\n",
    "    primary_name = None\n",
    "    if entry.get(\"names\"):\n",
    "        # pick first name-like token but avoid when it's obviously organization/job (heuristic)\n",
    "        for nm in entry[\"names\"]:\n",
    "            if nm and len(nm) > 1 and not any(t.lower() in nm.lower() for t in (\"engineer\",\"developer\",\"company\",\"address\",\"mobile\")):\n",
    "                primary_name = nm.strip()\n",
    "                break\n",
    "        if primary_name is None:\n",
    "            primary_name = entry[\"names\"][0].strip()\n",
    "    if not primary_name and raw_text:\n",
    "        # take the first non-empty line up to 80 chars as a fallback\n",
    "        for ln in raw_text.splitlines():\n",
    "            ln = ln.strip()\n",
    "            if ln and len(ln) < 80:\n",
    "                primary_name = ln\n",
    "                break\n",
    "    # clean orgs\n",
    "    orgs_clean = clean_org_list(entry.get(\"orgs\",[]))\n",
    "    # expand skills\n",
    "    text_for_skills = \" \".join([raw_text, entry.get(\"preview\",\"\")])\n",
    "    skills = sorted(set(entry.get(\"skills\",[])) | set(extract_skills_from_text(text_for_skills)))\n",
    "    # detect language\n",
    "    lang = \"unknown\"\n",
    "    if LANGDET_AVAILABLE and raw_text.strip():\n",
    "        try:\n",
    "            lang = detect(raw_text[:2000])\n",
    "        except Exception:\n",
    "            lang = \"unknown\"\n",
    "    # contact status\n",
    "    if emails or phones:\n",
    "        contact_status = \"found\"\n",
    "    else:\n",
    "        # can't reliably detect redaction here (we didn't run redaction detector on this dataset)\n",
    "        contact_status = \"missing\"\n",
    "    # assemble enhanced entry\n",
    "    e = {\n",
    "        \"file\": fname,\n",
    "        \"primary_name\": primary_name or \"\",\n",
    "        \"orgs\": orgs_clean,\n",
    "        \"emails\": emails,\n",
    "        \"phones\": phones,\n",
    "        \"linkedin\": linkedin,\n",
    "        \"github\": github,\n",
    "        \"skills\": skills,\n",
    "        \"chars\": entry.get(\"chars\", 0),\n",
    "        \"language\": lang,\n",
    "        \"contact_status\": contact_status,\n",
    "        \"original_preview\": entry.get(\"preview\",\"\")\n",
    "    }\n",
    "    enhanced.append(e)\n",
    "    counters.update([contact_status])\n",
    "    if emails: counters.update([\"has_email\"])\n",
    "    if phones: counters.update([\"has_phone\"])\n",
    "    counters.update([\"lang:\"+lang])\n",
    "\n",
    "# save\n",
    "with open(NER_OUT, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(enhanced, f, indent=2)\n",
    "\n",
    "# print a concise summary\n",
    "total = len(enhanced)\n",
    "print(\"Wrote enhanced NER ->\", NER_OUT)\n",
    "print(\"Total entries:\", total)\n",
    "print(\"Contact found:\", counters[\"found\"] if \"found\" in counters else sum(1 for e in enhanced if e[\"contact_status\"]==\"found\"))\n",
    "print(\"States:\", {k:v for k,v in counters.items() if k.startswith(\"lang:\") or k in ('has_email','has_phone')})\n",
    "print(\"\\nSample enhanced entry (first):\")\n",
    "pprint(enhanced[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f9e40da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Work\\Capstone_Project\\resume-nlp\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: all-mpnet-base-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Work\\Capstone_Project\\resume-nlp\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\abhis\\.cache\\huggingface\\hub\\models--sentence-transformers--all-mpnet-base-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Embedding resumes: 100%|██████████| 500/500 [05:25<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved embeddings: 500 -> ..\\data\\resumes_preocr\\embeddings\n",
      "\n",
      "Top 10 resumes by baseline score:\n",
      " 72.619 | sim=0.557 | contact=found   | skills= 9 | Python                                   | sample_00396.txt\n",
      " 70.418 | sim=0.517 | contact=found   | skills= 9 | NGUYEN VAN HUONG\n",
      "Day                     | sample_00261.txt\n",
      " 68.397 | sim=0.480 | contact=found   | skills=11 | Trần Dưỡng                               | sample_00444.txt\n",
      " 67.514 | sim=0.464 | contact=found   | skills= 7 | Yen Hoa                                  | sample_00075.txt\n",
      " 67.091 | sim=0.456 | contact=found   | skills= 9 | Duy Duc Thien                            | sample_00289.txt\n",
      " 66.588 | sim=0.447 | contact=found   | skills= 9 | Marital Status                           | sample_00419.txt\n",
      " 65.818 | sim=0.433 | contact=found   | skills= 6 | Ho Chi Minh                              | sample_00066.txt\n",
      " 65.018 | sim=0.418 | contact=found   | skills= 8 | Shipworks                                | sample_00174.txt\n",
      " 64.756 | sim=0.414 | contact=found   | skills=10 | Quang Hưng                               | sample_00218.txt\n",
      " 64.559 | sim=0.519 | contact=found   | skills= 4 | Ho Chi Minh                              | sample_00216.txt\n",
      "Saved resume_scores.json and leaderboard.csv in ..\\data\\resumes_preocr\n"
     ]
    }
   ],
   "source": [
    "# === Create BERT-based embeddings (all-mpnet-base-v2) and compute baseline scores ===\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np, json, os\n",
    "from pathlib import Path\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "ROOT = Path(\"../data/resumes_preocr\")\n",
    "TXT_DIR = ROOT / \"txt\"\n",
    "NER_ENH = ROOT / \"ner_enhanced.json\"\n",
    "EMB_DIR = ROOT / \"embeddings\"\n",
    "EMB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 1) load enhanced NER entries\n",
    "with open(NER_ENH, \"r\", encoding=\"utf-8\") as f:\n",
    "    ner = json.load(f)\n",
    "\n",
    "# helper to read resume text\n",
    "def load_text(fname):\n",
    "    p = TXT_DIR / fname\n",
    "    return p.read_text(encoding=\"utf-8\", errors=\"ignore\") if p.exists() else \"\"\n",
    "\n",
    "# 2) select BERT-style sentence-transformer model (MPNet = BERT-family)\n",
    "MODEL_NAME = \"all-mpnet-base-v2\"   # strong BERT-like sentence embedding model\n",
    "print(\"Loading model:\", MODEL_NAME)\n",
    "model = SentenceTransformer(MODEL_NAME)\n",
    "\n",
    "# 3) build & save embeddings for each resume (uses name + chunk + skills + orgs)\n",
    "emb_index = []\n",
    "for entry in tqdm(ner, desc=\"Embedding resumes\"):\n",
    "    fname = entry[\"file\"]\n",
    "    txt = load_text(fname) or \"\"\n",
    "    snippet = txt[:1600]  # representative chunk; increase if you'd like\n",
    "    name = entry.get(\"primary_name\",\"\") or \"\"\n",
    "    skills_str = \", \".join(entry.get(\"skills\",[]))\n",
    "    orgs_str = \", \".join(entry.get(\"orgs\",[]))\n",
    "    combined = (name + \"\\n\\n\" + snippet + \"\\n\\nSkills: \" + skills_str + \"\\nOrgs: \" + orgs_str).strip()\n",
    "    if not combined:\n",
    "        combined = \" \"\n",
    "    emb = model.encode(combined, show_progress_bar=False)\n",
    "    npy_path = EMB_DIR / (Path(fname).stem + \".npy\")\n",
    "    np.save(npy_path, emb)\n",
    "    emb_index.append({\"file\": fname, \"npy\": str(npy_path)})\n",
    "\n",
    "# save emb index\n",
    "with open(ROOT / \"emb_index.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(emb_index, f, indent=2)\n",
    "\n",
    "print(\"Saved embeddings:\", len(emb_index), \"->\", EMB_DIR)\n",
    "\n",
    "# 4) Job description (edit this to match the role you want)\n",
    "JOB_DESC = \"\"\"\n",
    "We are hiring an NLP Engineer with strong Python skills, proven experience with PyTorch and Transformers,\n",
    "solid foundations in NLP methods (tokenization, attention, sequence models), and experience deploying models to production.\n",
    "Experience with ML pipelines, REST APIs, and cloud deployment (AWS/GCP) is a plus.\n",
    "\"\"\"\n",
    "\n",
    "job_emb = model.encode(JOB_DESC)\n",
    "\n",
    "# 5) scoring parameters (adjustable later)\n",
    "WEIGHT_CONTACT = 12        # bonus if contact exists\n",
    "WEIGHT_SKILL = 6           # per matched skill (cap)\n",
    "WEIGHT_SIM = 55            # multiplier for similarity score\n",
    "CAP_SKILLS = 5\n",
    "\n",
    "# 6) compute similarity + baseline score\n",
    "file_to_entry = {e[\"file\"]: e for e in ner}\n",
    "scores = []\n",
    "for rec in emb_index:\n",
    "    fname = rec[\"file\"]\n",
    "    emb = np.load(rec[\"npy\"])\n",
    "    sim = float(cosine_similarity([emb], [job_emb])[0,0])\n",
    "    entry = file_to_entry.get(fname, {})\n",
    "    contact_bonus = WEIGHT_CONTACT if entry.get(\"contact_status\") == \"found\" else 0\n",
    "    skills_count = len(entry.get(\"skills\", []))\n",
    "    skill_score = min(CAP_SKILLS, skills_count) * WEIGHT_SKILL\n",
    "    sim_score = sim * WEIGHT_SIM\n",
    "    total = contact_bonus + skill_score + sim_score\n",
    "    scores.append({\n",
    "        \"file\": fname,\n",
    "        \"primary_name\": entry.get(\"primary_name\",\"\"),\n",
    "        \"contact_status\": entry.get(\"contact_status\",\"missing\"),\n",
    "        \"n_skills\": skills_count,\n",
    "        \"sim\": round(sim, 4),\n",
    "        \"score\": round(total, 4),\n",
    "        \"top_skills\": entry.get(\"skills\", [])[:6],\n",
    "        \"preview\": entry.get(\"original_preview\",\"\")[:300].replace(\"\\n\",\"\\\\n\")\n",
    "    })\n",
    "\n",
    "scores_sorted = sorted(scores, key=lambda x: x[\"score\"], reverse=True)\n",
    "\n",
    "# save results\n",
    "with open(ROOT / \"resume_scores.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(scores_sorted, f, indent=2)\n",
    "\n",
    "# print top 10\n",
    "print(\"\\nTop 10 resumes by baseline score:\")\n",
    "for s in scores_sorted[:10]:\n",
    "    print(f\"{s['score']:7.3f} | sim={s['sim']:.3f} | contact={s['contact_status']:<7} | skills={s['n_skills']:2} | {s['primary_name'][:40]:40} | {s['file']}\")\n",
    "\n",
    "# optionally: save leaderboard CSV for inspection\n",
    "import csv\n",
    "with open(ROOT / \"leaderboard.csv\", \"w\", newline='', encoding=\"utf-8\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\"score\",\"sim\",\"contact_status\",\"n_skills\",\"primary_name\",\"file\",\"top_skills\",\"preview\"])\n",
    "    writer.writeheader()\n",
    "    for r in scores_sorted:\n",
    "        writer.writerow({k: r.get(k,\"\") for k in writer.fieldnames})\n",
    "\n",
    "print(\"Saved resume_scores.json and leaderboard.csv in\", ROOT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "347492d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Work\\Capstone_Project\\resume-nlp\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: all-mpnet-base-v2\n",
      "\n",
      "Refined Top 10 resumes:\n",
      "100.559 | sim=0.447 (0.724) | nameQ=1.00 | exp=25 | contact=found   | skills= 9 | Marital Status                           | sample_00419.txt\n",
      " 99.809 | sim=0.414 (0.707) | nameQ=1.00 | exp=17 | contact=found   | skills=10 | Quang Hưng                               | sample_00218.txt\n",
      " 98.979 | sim=0.377 (0.688) | nameQ=1.00 | exp=40 | contact=found   | skills= 8 | Ly Van Sam                               | sample_00220.txt\n",
      " 97.977 | sim=0.332 (0.666) | nameQ=1.00 | exp=17 | contact=found   | skills= 9 | Nguyen Ngoc Dang                         | sample_00159.txt\n",
      " 96.126 | sim=0.517 (0.758) | nameQ=1.00 | exp=10 | contact=found   | skills= 9 | NGUYEN VAN HUONG\n",
      "Day                     | sample_00261.txt\n",
      " 92.210 | sim=0.432 (0.716) | nameQ=1.00 | exp=16 | contact=missing | skills=11 | Chung Vi Huy                             | sample_00267.txt\n",
      " 90.922 | sim=0.374 (0.687) | nameQ=1.00 | exp=15 | contact=missing | skills= 6 | Nguyen Van Tao Street                    | sample_00236.txt\n",
      " 90.233 | sim=0.433 (0.716) | nameQ=1.00 | exp=16 | contact=found   | skills= 4 | Van Luong                                | sample_00465.txt\n",
      " 88.647 | sim=0.433 (0.717) | nameQ=0.60 | exp=14 | contact=missing | skills= 6 | • Strong                                 | sample_00180.txt\n",
      " 86.899 | sim=0.480 (0.740) | nameQ=1.00 | exp= 3 | contact=found   | skills=11 | Trần Dưỡng                               | sample_00444.txt\n",
      "\n",
      "Saved: ..\\data\\resumes_preocr\\resume_scores_refined.json and ..\\data\\resumes_preocr\\leaderboard_refined.csv\n"
     ]
    }
   ],
   "source": [
    "# === Improved scoring: name_quality, language_match, rough_experience, sim normalization ===\n",
    "import json, re, csv\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "ROOT = Path(\"../data/resumes_preocr\")\n",
    "TXT_DIR = ROOT / \"txt\"\n",
    "NER_ENH = ROOT / \"ner_enhanced.json\"\n",
    "EMB_INDEX = ROOT / \"emb_index.json\"   # created by previous embedding run\n",
    "OUT_JSON = ROOT / \"resume_scores_refined.json\"\n",
    "OUT_CSV = ROOT / \"leaderboard_refined.csv\"\n",
    "\n",
    "# load data\n",
    "with open(NER_ENH, \"r\", encoding=\"utf-8\") as f:\n",
    "    ner = json.load(f)\n",
    "with open(EMB_INDEX, \"r\", encoding=\"utf-8\") as f:\n",
    "    emb_index = json.load(f)\n",
    "\n",
    "file_to_entry = {e[\"file\"]: e for e in ner}\n",
    "file_to_npy = {e[\"file\"]: e[\"npy\"] for e in emb_index}\n",
    "\n",
    "# load spaCy for name quality check\n",
    "try:\n",
    "    import spacy\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    SPACY_AVAILABLE = True\n",
    "except Exception:\n",
    "    SPACY_AVAILABLE = False\n",
    "    print(\"spaCy not available — name quality check will be skipped. Install en_core_web_sm to enable it.\")\n",
    "\n",
    "# langdetect for JD language\n",
    "try:\n",
    "    from langdetect import detect, DetectorFactory\n",
    "    DetectorFactory.seed = 0\n",
    "    LANGDET_AVAILABLE = True\n",
    "except Exception:\n",
    "    LANGDET_AVAILABLE = False\n",
    "    print(\"langdetect not available — language match will be skipped. pip install langdetect to enable it.\")\n",
    "\n",
    "# helper: load text\n",
    "def load_text(fname):\n",
    "    p = TXT_DIR / fname\n",
    "    return p.read_text(encoding=\"utf-8\", errors=\"ignore\") if p.exists() else \"\"\n",
    "\n",
    "# rough experience extractor: looks for \"X years\" or year ranges like 2016-2021, and counts span\n",
    "YEARS_RANGE_RE = re.compile(r\"(\\b(19|20)\\d{2})\\s*[\\-–—]\\s*(\\b(19|20)\\d{2})\")\n",
    "YEARS_PHRASE_RE = re.compile(r\"(\\d+)\\s+(?:years|yrs)\\b\", re.I)\n",
    "\n",
    "def estimate_experience(text):\n",
    "    # try \"X years\" phrase\n",
    "    m = YEARS_PHRASE_RE.search(text)\n",
    "    if m:\n",
    "        try:\n",
    "            val = int(m.group(1))\n",
    "            return min(val, 40)  # cap at 40\n",
    "        except:\n",
    "            pass\n",
    "    # try year ranges\n",
    "    m = YEARS_RANGE_RE.search(text)\n",
    "    if m:\n",
    "        try:\n",
    "            start = int(m.group(1))\n",
    "            end = int(m.group(3))\n",
    "            span = max(0, end - start)\n",
    "            return min(span, 40)\n",
    "        except:\n",
    "            pass\n",
    "    return 0\n",
    "\n",
    "# helper: check if primary_name is likely a PERSON using spaCy\n",
    "def name_quality(name):\n",
    "    if not name or not SPACY_AVAILABLE:\n",
    "        return 0\n",
    "    doc = nlp(name)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PERSON\":\n",
    "            return 1\n",
    "    # fallback: heuristic — at least two words and letters\n",
    "    if len(name.split()) >= 2 and re.search(r\"[A-Za-z\\u00C0-\\u017F]\", name):\n",
    "        return 0.6\n",
    "    return 0\n",
    "\n",
    "# detect JD language (use English as fallback)\n",
    "JOB_DESC = \"\"\"\n",
    "We are hiring an NLP Engineer with strong Python skills, proven experience with PyTorch and Transformers,\n",
    "solid foundations in NLP methods (tokenization, attention, sequence models), and experience deploying models to production.\n",
    "Experience with ML pipelines, REST APIs, and cloud deployment (AWS/GCP) is a plus.\n",
    "\"\"\"\n",
    "JD_LANG = \"en\"\n",
    "if LANGDET_AVAILABLE:\n",
    "    try:\n",
    "        JD_LANG = detect(JOB_DESC[:2000])\n",
    "    except Exception:\n",
    "        JD_LANG = \"en\"\n",
    "\n",
    "# load embeddings model (for job embedding)\n",
    "MODEL_NAME = \"all-mpnet-base-v2\"\n",
    "print(\"Loading embedding model:\", MODEL_NAME)\n",
    "model = SentenceTransformer(MODEL_NAME)\n",
    "job_emb = model.encode(JOB_DESC)\n",
    "\n",
    "# scoring hyperparams (tweakable)\n",
    "WEIGHT_CONTACT = 8          # smaller contact bonus\n",
    "WEIGHT_SKILL = 5            # per matched skill\n",
    "WEIGHT_SIM = 45             # semantic sim weight\n",
    "WEIGHT_NAME_QUALITY = 6     # boost if name looks like a person\n",
    "WEIGHT_LANG_MATCH = 6       # bonus if resume language == JD language\n",
    "WEIGHT_EXP_PER_YEAR = 1.2   # per estimated year of experience (capped)\n",
    "CAP_EXP = 15\n",
    "CAP_SKILLS = 6\n",
    "\n",
    "# compute all scores\n",
    "results = []\n",
    "for rec in emb_index:\n",
    "    fname = rec[\"file\"]\n",
    "    npy = rec[\"npy\"]\n",
    "    if fname not in file_to_entry:\n",
    "        continue\n",
    "    entry = file_to_entry[fname]\n",
    "    # load emb\n",
    "    try:\n",
    "        emb = np.load(npy)\n",
    "    except Exception as e:\n",
    "        continue\n",
    "    # raw cosine sim\n",
    "    raw_sim = float(cosine_similarity([emb], [job_emb])[0,0])\n",
    "    # normalize sim from [-1,1] to [0,1]\n",
    "    sim_norm = (raw_sim + 1) / 2.0\n",
    "    # contact\n",
    "    contact_bonus = WEIGHT_CONTACT if entry.get(\"contact_status\") == \"found\" else 0\n",
    "    # skills\n",
    "    n_skills = len(entry.get(\"skills\", []))\n",
    "    skill_score = min(CAP_SKILLS, n_skills) * WEIGHT_SKILL\n",
    "    # name quality\n",
    "    nq = name_quality(entry.get(\"primary_name\",\"\"))\n",
    "    name_score = nq * WEIGHT_NAME_QUALITY\n",
    "    # language match\n",
    "    lang = entry.get(\"language\",\"unknown\")\n",
    "    lang_bonus = WEIGHT_LANG_MATCH if (LANGDET_AVAILABLE and lang and JD_LANG and lang == JD_LANG) else 0\n",
    "    # rough experience\n",
    "    text = load_text(fname)\n",
    "    exp_years = estimate_experience(text)\n",
    "    exp_score = min(exp_years, CAP_EXP) * WEIGHT_EXP_PER_YEAR\n",
    "    # final score\n",
    "    sim_score = sim_norm * WEIGHT_SIM\n",
    "    total = contact_bonus + skill_score + sim_score + name_score + lang_bonus + exp_score\n",
    "    results.append({\n",
    "        \"file\": fname,\n",
    "        \"primary_name\": entry.get(\"primary_name\",\"\"),\n",
    "        \"contact_status\": entry.get(\"contact_status\",\"missing\"),\n",
    "        \"n_skills\": n_skills,\n",
    "        \"sim_raw\": round(raw_sim,4),\n",
    "        \"sim_norm\": round(sim_norm,4),\n",
    "        \"name_quality\": round(nq,3),\n",
    "        \"lang\": lang,\n",
    "        \"exp_years\": exp_years,\n",
    "        \"score\": round(total,3),\n",
    "        \"top_skills\": entry.get(\"skills\", [])[:8],\n",
    "        \"preview\": entry.get(\"original_preview\",\"\")[:300].replace(\"\\n\",\"\\\\n\")\n",
    "    })\n",
    "\n",
    "results_sorted = sorted(results, key=lambda x: x[\"score\"], reverse=True)\n",
    "\n",
    "# save results\n",
    "with open(OUT_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results_sorted, f, indent=2)\n",
    "\n",
    "# save CSV\n",
    "with open(OUT_CSV, \"w\", newline='', encoding=\"utf-8\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\"score\",\"sim_raw\",\"sim_norm\",\"name_quality\",\"exp_years\",\"lang\",\"contact_status\",\"n_skills\",\"primary_name\",\"file\",\"top_skills\",\"preview\"])\n",
    "    writer.writeheader()\n",
    "    for r in results_sorted:\n",
    "        writer.writerow({k: r.get(k,\"\") for k in writer.fieldnames})\n",
    "\n",
    "# print top 10\n",
    "print(\"\\nRefined Top 10 resumes:\")\n",
    "for s in results_sorted[:10]:\n",
    "    print(f\"{s['score']:7.3f} | sim={s['sim_raw']:.3f} ({s['sim_norm']:.3f}) | nameQ={s['name_quality']:.2f} | exp={s['exp_years']:2} | contact={s['contact_status']:<7} | skills={s['n_skills']:2} | {s['primary_name'][:40]:40} | {s['file']}\")\n",
    "print(\"\\nSaved:\", OUT_JSON, \"and\", OUT_CSV)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1207b2",
   "metadata": {},
   "source": [
    "### Step 5: Giving summary to users using the score of the resume via Gemini API (To be completed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52f8d5f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Text generation ===\n",
      "An NLP resume parser first extracts text from various document formats (PDF, DOCX). It then uses Natural Language Processing techniques like Named Entity Recognition (NER) and text classification to identify and categorize key information such as skills, experience, education, and contact details. Finally, it structures this extracted data into a machine-readable format, making it easy for Applicant Tracking Systems (ATS) to store, search, and analyze candidate profiles.\n",
      "\n",
      "=== Structured (parsed) output ===\n",
      "{\n",
      "  \"name\": null,\n",
      "  \"email\": \"john.doe@example.com\",\n",
      "  \"phone\": \"+1 (555) 123-4567\",\n",
      "  \"skills\": [\n",
      "    \"Python\",\n",
      "    \"Docker\",\n",
      "    \"SQL\"\n",
      "  ],\n",
      "  \"years_experience\": 6\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abhis\\AppData\\Local\\Temp\\ipykernel_18112\\2897953912.py:70: PydanticDeprecatedSince20: The `json` method is deprecated; use `model_dump_json` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/\n",
      "  print(parsed.json(indent=2))\n"
     ]
    }
   ],
   "source": [
    "# Jupyter cell: Google GenAI (Gemini) quick examples\n",
    "# 1) install (run once in your env / notebook). Uncomment if needed.\n",
    "# !pip install -q -U google-genai pydantic\n",
    "\n",
    "import os\n",
    "from google import genai\n",
    "from pydantic import BaseModel\n",
    "from typing import List, Optional\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # load .env if present\n",
    "\n",
    "# --- AUTH: either set environment variable GEMINI_API_KEY (recommended)\n",
    "# In a terminal / OS:\n",
    "#   export GEMINI_API_KEY=\"your_api_key_here\"      # linux / mac\n",
    "#   setx GEMINI_API_KEY \"your_api_key_here\"       # windows (restart shell)\n",
    "# Or pass the key explicitly to Client(api_key=...)\n",
    "#\n",
    "# The client will pick GEMINI_API_KEY from env automatically:\n",
    "client = genai.Client()  # uses os.environ['GEMINI_API_KEY'] if present\n",
    "\n",
    "# -----------------------\n",
    "# Example A: simple text generation\n",
    "# -----------------------\n",
    "resp = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",               # recommended model from quickstart\n",
    "    contents=\"Explain how an NLP resume parser should work in 3 sentences.\"\n",
    ")\n",
    "print(\"=== Text generation ===\")\n",
    "print(resp.text)   # concise plain text result\n",
    "\n",
    "# -----------------------\n",
    "# Example B: Structured output (JSON) using a Pydantic schema\n",
    "# Useful for extracting structured fields from a resume text.\n",
    "# NOTE: structured output requires `response_mime_type=\"application/json\"`,\n",
    "# and we pass a Pydantic model as response_schema.\n",
    "# -----------------------\n",
    "class ResumeSchema(BaseModel):\n",
    "    name: Optional[str]\n",
    "    email: Optional[str]\n",
    "    phone: Optional[str]\n",
    "    skills: List[str] = []\n",
    "    years_experience: Optional[int]\n",
    "\n",
    "resume_prompt = \"\"\"\n",
    "Extract the main contact information and skills from this resume text.\n",
    "Return only JSON that conforms to the schema: name, email, phone, skills (array of skill strings),\n",
    "years_experience (approximate integer).\n",
    "Resume text:\n",
    "---\n",
    "Software engineer with 6 years experience in backend and cloud, expert in Python, Docker, and SQL.\n",
    "Contact: john.doe@example.com, +1 (555) 123-4567\n",
    "Worked at Acme Corp and Beta Systems.\n",
    "---\n",
    "\"\"\"\n",
    "\n",
    "# call with 'config' specifying response_mime_type and response_schema (Pydantic model)\n",
    "resp_json = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=resume_prompt,\n",
    "    config={\n",
    "        \"response_mime_type\": \"application/json\",   # ask for JSON output\n",
    "        \"response_schema\": ResumeSchema,            # pydantic model to validate/parse\n",
    "    },\n",
    ")\n",
    "\n",
    "print(\"\\n=== Structured (parsed) output ===\")\n",
    "# .parsed (if available) gives the parsed Pydantic instance; else fallback to text\n",
    "try:\n",
    "    parsed = resp_json.parsed  # a ResumeSchema instance (if SDK parsed successfully)\n",
    "    print(parsed.json(indent=2))\n",
    "except Exception:\n",
    "    # fallback: print model text (raw JSON string from model)\n",
    "    print(resp_json.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ff72ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing to summarize 200 resumes with model gemini-2.5-flash...\n",
      "Wrote JSON -> ..\\data\\resumes_preocr\\summaries_gemini.json\n",
      "Wrote CSV  -> ..\\data\\resumes_preocr\\summaries_gemini.csv\n",
      "Completed with errors for 200 resumes. Check JSON 'errors' field.\n"
     ]
    }
   ],
   "source": [
    "# === Gemini structured summarization for scored resumes (ready-to-run) ===\n",
    "# Requirements:\n",
    "# pip install -U google-genai pydantic pandas\n",
    "#\n",
    "# Set env var: GEMINI_API_KEY (or use genai.Client(api_key=\"...\") below)\n",
    "#\n",
    "import os, json, re, time, csv\n",
    "from pathlib import Path\n",
    "from typing import List, Optional\n",
    "from pydantic import BaseModel\n",
    "import google.genai as genai  # updated name in SDK\n",
    "\n",
    "# ---------- Config ----------\n",
    "DATA_DIR = Path(\"../data/resumes_preocr\")\n",
    "SCORES_FILE = DATA_DIR / \"resume_scores_refined_clean.json\"\n",
    "OUT_JSON = DATA_DIR / \"summaries_gemini.json\"\n",
    "OUT_CSV = DATA_DIR / \"summaries_gemini.csv\"\n",
    "MODEL = \"gemini-2.5-flash\"   # change if needed (use model you have access to)\n",
    "BATCH_SIZE = 6               # lower to respect rate limits\n",
    "TOP_N = 200                  # how many resumes to summarize (set smaller to test)\n",
    "MAX_RETRIES = 2\n",
    "BACKOFF_BASE = 1.5\n",
    "\n",
    "# ---------- Pydantic schema for structured output ----------\n",
    "class ResumeSummary(BaseModel):\n",
    "    resume_id: str\n",
    "    name: Optional[str] = None\n",
    "    best_role: Optional[str] = None\n",
    "    years_experience: Optional[int] = None\n",
    "    top_skills: List[str] = []\n",
    "    key_achievements: List[str] = []\n",
    "    education: List[str] = []\n",
    "    contact: dict = {}  # {email:..., phone:..., linkedin:...}\n",
    "    fit_score: Optional[float] = None\n",
    "    summary: Optional[str] = None\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def mask_pii(text: str) -> str:\n",
    "    if not text:\n",
    "        return text\n",
    "    t = re.sub(r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b', '<EMAIL>', text)\n",
    "    t = re.sub(r'\\b(?:\\+?\\d[\\d\\s\\-\\(\\)]{3,}\\d)\\b', '<PHONE>', t)\n",
    "    return t\n",
    "\n",
    "def extract_json_from_text(s: str):\n",
    "    \"\"\"Naive JSON extraction; we prefer SDK's parsed output but keep fallback.\"\"\"\n",
    "    if not s:\n",
    "        return None\n",
    "    try:\n",
    "        return json.loads(s)\n",
    "    except Exception:\n",
    "        start = s.find('{')\n",
    "        if start == -1:\n",
    "            return None\n",
    "        depth = 0\n",
    "        for i in range(start, len(s)):\n",
    "            if s[i] == '{': depth += 1\n",
    "            elif s[i] == '}':\n",
    "                depth -= 1\n",
    "                if depth == 0:\n",
    "                    try:\n",
    "                        return json.loads(s[start:i+1])\n",
    "                    except Exception:\n",
    "                        return None\n",
    "    return None\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are an assistant that reads a compact resume context and returns a strict JSON summary. \"\n",
    "    \"Do not invent facts. If a field is not present, return null or an empty list. Output valid JSON matching the schema exactly.\"\n",
    ")\n",
    "\n",
    "USER_SCHEMA_AND_INSTR = \"\"\"\n",
    "Input fields:\n",
    "- resume_id, name, snippet, skills, orgs, years_experience, contact_status, resume_score\n",
    "\n",
    "Return JSON matching the ResumeSummary schema exactly:\n",
    "{\n",
    " \"resume_id\": \"<same>\",\n",
    " \"name\": \"<string or null>\",\n",
    " \"best_role\": \"<one-line role/title or null>\",\n",
    " \"years_experience\": <int or null>,\n",
    " \"top_skills\": [\"...\"],\n",
    " \"key_achievements\": [\"...\"],\n",
    " \"education\": [\"...\"],\n",
    " \"contact\": {\"email\": null, \"phone\": null, \"linkedin\": null},\n",
    " \"fit_score\": <float 0-100>,\n",
    " \"summary\": \"<2-4 sentence summary>\"\n",
    "}\n",
    "\n",
    "Rules:\n",
    "- Only use facts present in snippet, skills, or orgs. If uncertain, use null/empty.\n",
    "- Fit_score should be consistent with resume_score (you may rescale directly).\n",
    "- summary must be concise (2-4 short sentences), factual and use provided text.\n",
    "- Return JSON only.\n",
    "\"\"\"\n",
    "\n",
    "# ---------- Prepare client ----------\n",
    "API_KEY = os.environ.get(\"GEMINI_API_KEY\") or os.environ.get(\"GOOGLE_API_KEY\")\n",
    "if not API_KEY:\n",
    "    raise RuntimeError(\"Set GEMINI_API_KEY (or GOOGLE_API_KEY) environment variable with your API key.\")\n",
    "client = genai.Client(api_key=API_KEY)\n",
    "\n",
    "# ---------- Load scored resumes ----------\n",
    "with open(SCORES_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    scored = json.load(f)\n",
    "\n",
    "scored = scored[:TOP_N] if TOP_N and len(scored) > TOP_N else scored\n",
    "print(f\"Preparing to summarize {len(scored)} resumes with model {MODEL}...\")\n",
    "\n",
    "summaries = []\n",
    "errors = []\n",
    "\n",
    "# ---------- Loop and call Gemini ----------\n",
    "for idx, rec in enumerate(scored, start=1):\n",
    "    resume_id = rec.get(\"file\")\n",
    "    name = rec.get(\"primary_name\") or rec.get(\"primary_name_clean\") or None\n",
    "    snippet = rec.get(\"preview\", \"\")[:1200]  # short excerpt; adjust length if needed\n",
    "    snippet_masked = mask_pii(snippet)\n",
    "    skills = rec.get(\"top_skills\") or rec.get(\"skills\") or []\n",
    "    orgs = rec.get(\"orgs\") or []\n",
    "    years = rec.get(\"exp_years\") or rec.get(\"exp_years\", None) or None\n",
    "    contact_status = rec.get(\"contact_status\", \"missing\")\n",
    "    resume_score = float(rec.get(\"score\", 0.0))\n",
    "\n",
    "    input_json = {\n",
    "        \"resume_id\": resume_id,\n",
    "        \"name\": name,\n",
    "        \"snippet\": snippet_masked,\n",
    "        \"skills\": skills,\n",
    "        \"orgs\": orgs,\n",
    "        \"years_experience\": years,\n",
    "        \"contact_status\": contact_status,\n",
    "        \"resume_score\": resume_score\n",
    "    }\n",
    "\n",
    "    prompt_text = SYSTEM_PROMPT + \"\\n\\n\" + USER_SCHEMA_AND_INSTR + \"\\n\\n\" + \"Input JSON:\\n\" + json.dumps(input_json, ensure_ascii=False, indent=2)\n",
    "\n",
    "    parsed_obj = None\n",
    "    last_err = None\n",
    "    for attempt in range(MAX_RETRIES + 1):\n",
    "        try:\n",
    "            resp = client.models.generate_content(\n",
    "                model=MODEL,\n",
    "                contents=prompt_text,\n",
    "                config={\n",
    "                    \"response_mime_type\": \"application/json\",\n",
    "                    \"response_schema\": ResumeSummary,  # Pydantic model, SDK will validate/parse\n",
    "                    # You can add other options supported by SDK if needed\n",
    "                },\n",
    "            )\n",
    "            # SDK tries to parse into Pydantic model; access .parsed\n",
    "            parsed = getattr(resp, \"parsed\", None)\n",
    "            if parsed is not None:\n",
    "                # parsed is a Pydantic model instance or list; ensure dict form\n",
    "                if isinstance(parsed, list):\n",
    "                    parsed_obj = parsed[0]\n",
    "                else:\n",
    "                    parsed_obj = parsed\n",
    "            else:\n",
    "                # fallback: attempt to extract JSON from resp.text\n",
    "                raw_text = getattr(resp, \"text\", None) or str(resp)\n",
    "                j = extract_json_from_text(raw_text)\n",
    "                if j is None:\n",
    "                    raise ValueError(\"No JSON parsed by SDK and fallback failed.\")\n",
    "                parsed_obj = ResumeSummary(**j)\n",
    "            # ensure resume_id and fit_score\n",
    "            if not parsed_obj.resume_id:\n",
    "                parsed_obj.resume_id = resume_id\n",
    "            if parsed_obj.fit_score is None:\n",
    "                parsed_obj.fit_score = round(min(100.0, resume_score), 2)\n",
    "            summaries.append(parsed_obj.model_dump())  # convert to plain dict\n",
    "            last_err = None\n",
    "            break\n",
    "        except Exception as e:\n",
    "            last_err = str(e)\n",
    "            time.sleep(BACKOFF_BASE * (2 ** attempt))\n",
    "    if last_err:\n",
    "        errors.append({\"resume_id\": resume_id, \"error\": last_err})\n",
    "        # fallback minimal summary\n",
    "        fallback = ResumeSummary(\n",
    "            resume_id=resume_id,\n",
    "            name=name,\n",
    "            best_role=None,\n",
    "            years_experience=years,\n",
    "            top_skills=skills[:6],\n",
    "            key_achievements=[],\n",
    "            education=[],\n",
    "            contact={\"email\": None, \"phone\": None, \"linkedin\": None},\n",
    "            fit_score=round(min(100.0, resume_score),2),\n",
    "            summary=f\"Auto-fallback summary (resume_score={resume_score})\"\n",
    "        )\n",
    "        summaries.append(fallback.model_dump())\n",
    "\n",
    "    # polite pacing\n",
    "    if idx % BATCH_SIZE == 0:\n",
    "        time.sleep(1.0)\n",
    "\n",
    "# ---------- Save outputs ----------\n",
    "with open(OUT_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"summaries\": summaries, \"errors\": errors}, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# CSV quick summary\n",
    "csv_fields = [\"resume_id\",\"name\",\"best_role\",\"years_experience\",\"top_skills\",\"fit_score\",\"summary\"]\n",
    "with open(OUT_CSV, \"w\", newline='', encoding=\"utf-8\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=csv_fields)\n",
    "    writer.writeheader()\n",
    "    for s in summaries:\n",
    "        writer.writerow({\n",
    "            \"resume_id\": s.get(\"resume_id\"),\n",
    "            \"name\": s.get(\"name\"),\n",
    "            \"best_role\": s.get(\"best_role\"),\n",
    "            \"years_experience\": s.get(\"years_experience\"),\n",
    "            \"top_skills\": \",\".join(s.get(\"top_skills\") or []),\n",
    "            \"fit_score\": s.get(\"fit_score\"),\n",
    "            \"summary\": (s.get(\"summary\") or \"\")[:300]\n",
    "        })\n",
    "\n",
    "print(\"Wrote JSON ->\", OUT_JSON)\n",
    "print(\"Wrote CSV  ->\", OUT_CSV)\n",
    "if errors:\n",
    "    print(\"Completed with errors for\", len(errors), \"resumes. Check JSON 'errors' field.\")\n",
    "else:\n",
    "    print(\"All summaries produced successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d8122e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed count: 200\n",
      "Batches to process: 20 (BATCH_SIZE=10, SNIPPET_MAX=120)\n",
      "Batch 1/1 done.\n",
      "Done. Saved: ..\\data\\resumes_preocr\\summaries_gemini_batch_retry.json\n",
      "Processed one batch only (TEST_ONLY=True). Inspect batch_0 raw logs then set TEST_ONLY=False to continue.\n"
     ]
    }
   ],
   "source": [
    "# Aggressive, token-safe full cell for Gemini batch retry\n",
    "# Very small batches and tiny snippets to avoid MAX_TOKENS issues.\n",
    "\n",
    "import os, json, re, time, math\n",
    "from pathlib import Path\n",
    "from typing import List, Optional\n",
    "import google.genai as genai\n",
    "\n",
    "# ---------- Config ----------\n",
    "DATA_DIR = Path(\"../data/resumes_preocr\")\n",
    "IN_SUMMARIES = DATA_DIR / \"summaries_gemini.json\"\n",
    "SCORES_FILE = DATA_DIR / \"resume_scores_refined_clean.json\"\n",
    "OUT_RETRY = DATA_DIR / \"summaries_gemini_batch_retry.json\"\n",
    "RAW_DIR = DATA_DIR / \"gemini_batch_raw\"\n",
    "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MODEL = \"gemini-2.5-flash-lite\"\n",
    "BATCH_SIZE = 10           # tiny batch to be safe\n",
    "TEST_ONLY = True\n",
    "MAX_BATCH_RETRIES = 2\n",
    "SLEEP_BETWEEN_BATCHES = 0.8\n",
    "SNIPPET_MAX = 120        # very short snippet\n",
    "MAX_OUTPUT_TOKENS = 800\n",
    "\n",
    "API_KEY = os.environ.get(\"GEMINI_API_KEY\") or os.environ.get(\"GOOGLE_API_KEY\")\n",
    "if not API_KEY:\n",
    "    raise RuntimeError(\"Set GEMINI_API_KEY or GOOGLE_API_KEY in environment.\")\n",
    "client = genai.Client(api_key=API_KEY)\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def flatten_json_array_text(text: str) -> Optional[list]:\n",
    "    if not text:\n",
    "        return None\n",
    "    try:\n",
    "        obj = json.loads(text)\n",
    "        if isinstance(obj, list):\n",
    "            return obj\n",
    "    except Exception:\n",
    "        pass\n",
    "    a = text.find('[')\n",
    "    if a != -1:\n",
    "        depth = 0\n",
    "        for i in range(a, len(text)):\n",
    "            ch = text[i]\n",
    "            if ch == '[':\n",
    "                depth += 1\n",
    "            elif ch == ']':\n",
    "                depth -= 1\n",
    "                if depth == 0:\n",
    "                    candidate = text[a:i+1]\n",
    "                    candidate = re.sub(r',\\s*]', ']', candidate)\n",
    "                    candidate = re.sub(r',\\s*}', '}', candidate)\n",
    "                    try:\n",
    "                        parsed = json.loads(candidate)\n",
    "                        if isinstance(parsed, list):\n",
    "                            return parsed\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                    break\n",
    "    objs = []\n",
    "    start = text.find('{')\n",
    "    while start != -1:\n",
    "        depth = 0; end = None\n",
    "        for i in range(start, len(text)):\n",
    "            if text[i] == '{':\n",
    "                depth += 1\n",
    "            elif text[i] == '}':\n",
    "                depth -= 1\n",
    "                if depth == 0:\n",
    "                    end = i; break\n",
    "        if end is None: break\n",
    "        candidate = text[start:end+1]\n",
    "        cand2 = re.sub(r',\\s*}', '}', candidate).replace(\"'\", '\"')\n",
    "        try:\n",
    "            parsed = json.loads(cand2); objs.append(parsed)\n",
    "        except Exception:\n",
    "            pass\n",
    "        start = text.find('{', end+1)\n",
    "    return objs if objs else None\n",
    "\n",
    "def extract_raw_text_from_resp(resp) -> str:\n",
    "    # Try resp.text\n",
    "    raw = getattr(resp, \"text\", None)\n",
    "    if raw:\n",
    "        return raw\n",
    "    # Try candidates\n",
    "    cand_list = getattr(resp, \"candidates\", None)\n",
    "    if cand_list and len(cand_list) > 0:\n",
    "        first = cand_list[0]\n",
    "        # content may be dict-like or object\n",
    "        content = getattr(first, \"content\", None)\n",
    "        if isinstance(content, dict):\n",
    "            if \"text\" in content and content[\"text\"]:\n",
    "                return content[\"text\"]\n",
    "            if \"parts\" in content and isinstance(content[\"parts\"], list):\n",
    "                return \"\".join(content[\"parts\"])\n",
    "            return json.dumps(content)\n",
    "        txt = getattr(content, \"text\", None)\n",
    "        if txt:\n",
    "            return txt\n",
    "        parts = getattr(content, \"parts\", None)\n",
    "        if parts:\n",
    "            return \"\".join(parts)\n",
    "        t = getattr(first, \"text\", None)\n",
    "        if t:\n",
    "            return t\n",
    "        return repr(first)\n",
    "    # older choices\n",
    "    choices = getattr(resp, \"choices\", None)\n",
    "    if choices and len(choices) > 0:\n",
    "        ch = choices[0]\n",
    "        if isinstance(ch, dict) and \"text\" in ch:\n",
    "            return ch[\"text\"]\n",
    "        return getattr(ch, \"text\", None) or repr(ch)\n",
    "    return repr(resp)\n",
    "\n",
    "def clean_and_shorten_snippet(raw: str, max_len: int = SNIPPET_MAX) -> str:\n",
    "    if not raw: return \"\"\n",
    "    s = re.sub(r'[\\x00-\\x1f\\x7f-\\x9f]', ' ', raw)\n",
    "    s = re.sub(r'\\s+', ' ', s).strip()\n",
    "    s = re.sub(r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b', '<EMAIL>', s)\n",
    "    s = re.sub(r'\\b(?:\\+?\\d[\\d\\s\\-\\(\\)]{3,}\\d)\\b', '<PHONE>', s)\n",
    "    return s[:max_len]\n",
    "\n",
    "def build_batch_prompt(inputs: List[dict]) -> str:\n",
    "    # Minimal inline example (single-line) to save tokens\n",
    "    example_line = (\n",
    "        '[{\"resume_id\":\"sample_EX.txt\",\"name\":\"Jane Doe\",\"best_role\":\"Backend Engineer\",'\n",
    "        '\"years_experience\":6,\"top_skills\":[\"python\",\"sql\"],'\n",
    "        '\"key_achievements\":[\"Built ETL\"],\"education\":[\"BSc\"],'\n",
    "        '\"contact\":{\"email\":null,\"phone\":null,\"linkedin\":null},\"fit_score\":78.3,\"summary\":\"Backend engineer.\"}]'\n",
    "    )\n",
    "    # Fallback object instruction ensures a parseable array\n",
    "    prompt = (\n",
    "        \"Return JSON ARRAY ONLY. No commentary. Do NOT think out loud.\\n\\n\"\n",
    "        \"Schema for each element (must contain exactly these keys):\\n\"\n",
    "        \"resume_id,name,best_role,years_experience,top_skills,key_achievements,education,contact,fit_score,summary\\n\\n\"\n",
    "        \"Example output (single-line):\\n\" + example_line + \"\\n\\n\"\n",
    "        \"If you cannot produce a valid entry for an input, return for that item:\\n\"\n",
    "        '{\"resume_id\":\"<same id>\",\"error\":\"cannot_parse\",\"best_role\":null,\"years_experience\":null,'\n",
    "        '\"top_skills\":[],\"key_achievements\":[],\"education\":[],\"contact\":{\"email\":null,\"phone\":null,\"linkedin\":null},'\n",
    "        '\"fit_score\":0.0,\"summary\":null}\\n\\n'\n",
    "        \"Now process the inputs below and return a JSON array where i-th element corresponds to the i-th input.\\n\\n\"\n",
    "        \"Inputs:\\n\" + json.dumps(inputs, ensure_ascii=False)\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "# ---------- Load summaries and failed set ----------\n",
    "with open(IN_SUMMARIES, \"r\", encoding=\"utf-8\") as f:\n",
    "    prev = json.load(f)\n",
    "summaries = prev.get(\"summaries\", prev) if isinstance(prev, dict) else prev\n",
    "failed = [s for s in summaries if isinstance(s.get(\"summary\",\"\"), str) and s[\"summary\"].startswith(\"Auto-fallback summary\")]\n",
    "print(\"Failed count:\", len(failed))\n",
    "if not failed:\n",
    "    print(\"Nothing to retry. Exiting.\")\n",
    "else:\n",
    "    all_scored = {}\n",
    "    if SCORES_FILE.exists():\n",
    "        sf = json.load(open(SCORES_FILE, \"r\", encoding=\"utf-8\"))\n",
    "        all_scored = {r.get(\"file\"): r for r in sf}\n",
    "\n",
    "    inputs = []\n",
    "    for fentry in failed:\n",
    "        rid = fentry[\"resume_id\"]\n",
    "        scored_rec = all_scored.get(rid, {})\n",
    "        raw_snip = scored_rec.get(\"preview\") or fentry.get(\"summary\") or \"\"\n",
    "        snippet = clean_and_shorten_snippet(raw_snip, max_len=SNIPPET_MAX)\n",
    "        inp = {\n",
    "            \"resume_id\": rid,\n",
    "            \"name\": (scored_rec.get(\"primary_name\") or fentry.get(\"name\") or \"\")[:80],\n",
    "            \"snippet\": snippet,\n",
    "            \"skills\": scored_rec.get(\"top_skills\") or fentry.get(\"top_skills\") or [],\n",
    "            \"years_experience\": scored_rec.get(\"exp_years\") or fentry.get(\"years_experience\") or None,\n",
    "            \"resume_score\": float(scored_rec.get(\"score\") or fentry.get(\"fit_score\") or 0.0)\n",
    "        }\n",
    "        inputs.append(inp)\n",
    "\n",
    "    num_batches = math.ceil(len(inputs) / BATCH_SIZE)\n",
    "    print(f\"Batches to process: {num_batches} (BATCH_SIZE={BATCH_SIZE}, SNIPPET_MAX={SNIPPET_MAX})\")\n",
    "    outputs_by_id = {}\n",
    "    raw_logs = {}\n",
    "\n",
    "    batches_to_run = 1 if TEST_ONLY else num_batches\n",
    "    for b in range(batches_to_run):\n",
    "        batch_inputs = inputs[b*BATCH_SIZE:(b+1)*BATCH_SIZE]\n",
    "        if not batch_inputs:\n",
    "            continue\n",
    "        prompt = build_batch_prompt(batch_inputs)\n",
    "\n",
    "        parsed_list = None\n",
    "        last_err = None\n",
    "        for attempt in range(MAX_BATCH_RETRIES + 1):\n",
    "            try:\n",
    "                resp = client.models.generate_content(\n",
    "                    model=MODEL,\n",
    "                    contents=prompt,\n",
    "                    config={\"max_output_tokens\": MAX_OUTPUT_TOKENS, \"temperature\": 0.0}\n",
    "                )\n",
    "                raw_text = extract_raw_text_from_resp(resp)\n",
    "                raw_logs[f\"batch_{b}\"] = raw_text\n",
    "                # Save a small repr of resp for debugging token reasons\n",
    "                raw_logs[f\"batch_{b}_repr\"] = repr(resp)[:1000]\n",
    "                parsed_list = flatten_json_array_text(raw_text)\n",
    "                if parsed_list is None:\n",
    "                    raise ValueError(\"No JSON array or objects parsed from model output.\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                last_err = str(e)\n",
    "                wait = 1.2 * (2 ** attempt)\n",
    "                print(f\" Batch {b} attempt {attempt} failed: {last_err} — sleeping {wait}s\")\n",
    "                time.sleep(wait)\n",
    "        if parsed_list is None:\n",
    "            print(f\"Batch {b} failed after retries: {last_err}. Saving raw output for debug.\")\n",
    "            raw_logs[f\"batch_{b}_error\"] = last_err\n",
    "            for inp in batch_inputs:\n",
    "                outputs_by_id[inp[\"resume_id\"]] = None\n",
    "        else:\n",
    "            if len(parsed_list) != len(batch_inputs):\n",
    "                for obj in parsed_list:\n",
    "                    rid = obj.get(\"resume_id\")\n",
    "                    if rid:\n",
    "                        outputs_by_id[rid] = obj\n",
    "                for i, inp in enumerate(batch_inputs):\n",
    "                    if inp[\"resume_id\"] not in outputs_by_id:\n",
    "                        if i < len(parsed_list) and isinstance(parsed_list[i], dict):\n",
    "                            outputs_by_id[inp[\"resume_id\"]] = parsed_list[i]\n",
    "                        else:\n",
    "                            outputs_by_id[inp[\"resume_id\"]] = None\n",
    "            else:\n",
    "                for inp, out in zip(batch_inputs, parsed_list):\n",
    "                    outputs_by_id[inp[\"resume_id\"]] = out\n",
    "\n",
    "        print(f\"Batch {b+1}/{batches_to_run} done.\")\n",
    "        time.sleep(SLEEP_BETWEEN_BATCHES)\n",
    "\n",
    "    # Merge results\n",
    "    merged = []\n",
    "    for s in summaries:\n",
    "        rid = s.get(\"resume_id\")\n",
    "        if rid in outputs_by_id and outputs_by_id[rid]:\n",
    "            out = outputs_by_id[rid]\n",
    "            if out.get(\"fit_score\") is None:\n",
    "                out[\"fit_score\"] = round(min(100.0, out.get(\"resume_score\") or s.get(\"fit_score\") or 0.0), 2)\n",
    "            merged.append(out)\n",
    "        else:\n",
    "            merged.append(s)\n",
    "\n",
    "    with open(OUT_RETRY, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\"summaries\": merged}, f, ensure_ascii=False, indent=2)\n",
    "    with open(RAW_DIR / \"batch_raw_logs.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(raw_logs, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(\"Done. Saved:\", OUT_RETRY)\n",
    "    if TEST_ONLY:\n",
    "        print(\"Processed one batch only (TEST_ONLY=True). Inspect batch_0 raw logs then set TEST_ONLY=False to continue.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d452ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sdk_http_response=HttpResponse(\n",
      "  headers=<dict len=11>\n",
      ") candidates=[Candidate(\n",
      "  content=Content(\n",
      "    parts=[\n",
      "      Part(\n",
      "        text='OK'\n",
      "      ),\n",
      "    ],\n",
      "    role='model'\n",
      "  ),\n",
      "  finish_reason=<FinishReason.STOP: 'STOP'>,\n",
      "  index=0\n",
      ")] create_time=None model_version='gemini-2.5-flash-lite' prompt_feedback=None response_id='XecZaeXjJNCBqfkPgJqF6Qs' usage_metadata=GenerateContentResponseUsageMetadata(\n",
      "  candidates_token_count=1,\n",
      "  prompt_token_count=6,\n",
      "  prompt_tokens_details=[\n",
      "    ModalityTokenCount(\n",
      "      modality=<MediaModality.TEXT: 'TEXT'>,\n",
      "      token_count=6\n",
      "    ),\n",
      "  ],\n",
      "  total_token_count=7\n",
      ") automatic_function_calling_history=[] parsed=None\n"
     ]
    }
   ],
   "source": [
    "from google.genai import Client\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # load .env if present\n",
    "\n",
    "client = Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "resp = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash-lite\",\n",
    "    contents=\"Say the word OK.\",\n",
    "    config={\"max_output_tokens\": 10}\n",
    ")\n",
    "print(resp)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
